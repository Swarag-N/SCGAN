{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "ImageGPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4xspi6ngSsT",
        "colab_type": "text"
      },
      "source": [
        "Downloading The Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--a9PGye1tz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1I0qw-65KBA6np8vIZzO6oeiOvcDBttAY \n",
        "!unrar x \"/content/ISTD_Dataset.rar\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex52reOkiKQ5",
        "colab_type": "text"
      },
      "source": [
        "Making Dataset      (Image Loader)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghmVjEqI28Cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as DATA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXUgCScR3AeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_dataset():\n",
        "    dataset = []\n",
        "    original_img_rpath = '/content/ISTD_Dataset/train/train_A'\n",
        "    shadow_mask_rpath = '/content/ISTD_Dataset/train/train_B'\n",
        "    shadow_free_img_rpath = '/content/ISTD_Dataset/train/train_C'\n",
        "    for img_path in glob.glob(os.path.join(original_img_rpath, '*.png')):\n",
        "        basename = os.path.basename(img_path)\n",
        "        original_img_path = os.path.join(original_img_rpath, basename)\n",
        "        shadow_mask_path = os.path.join(shadow_mask_rpath, basename)\n",
        "        shadow_free_img_path = os.path.join(shadow_free_img_rpath, basename)\n",
        "        #print(original_img_path, shadow_mask_path, shadow_free_img_path)\n",
        "        dataset.append([original_img_path, shadow_mask_path, shadow_free_img_path])\n",
        "    #print(dataset)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "class shadow_triplets_loader(DATA.Dataset):\n",
        "    def __init__(self):\n",
        "        super(shadow_triplets_loader, self).__init__()\n",
        "        self.train_set_path = make_dataset()\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        original_img_path, shadow_mask_path, shadow_free_img_path = self.train_set_path[item]\n",
        "        transform = transforms.ToTensor()\n",
        "        #print(original_img_path, shadow_mask_path, shadow_free_img_path)\n",
        "        original_img = Image.open(original_img_path)\n",
        "        shadow_mask = Image.open(shadow_mask_path)\n",
        "        shadow_free_img = Image.open(shadow_free_img_path)\n",
        "\n",
        "        original_img = transform(original_img.resize((256, 256)))\n",
        "        shadow_mask = transform(shadow_mask.resize((256, 256)))\n",
        "        shadow_free_img = transform(shadow_free_img.resize((256, 256)))\n",
        "\n",
        "        return original_img, shadow_mask, shadow_free_img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_set_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW7Li7L0iTE2",
        "colab_type": "text"
      },
      "source": [
        "Stcgan_net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2iIqRnk3MQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjFB1F2z3Ssw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator_first(nn.Module):\n",
        "    def  __init__(self):\n",
        "        super(Generator_first, self).__init__()\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt6 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt7 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1024, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt8 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1024, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt9 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt10 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt11 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 1, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        conv0 = self.conv0(input)\n",
        "        conv1 = self.conv1(conv0)\n",
        "        conv2 = self.conv2(conv1)\n",
        "        conv3 = self.conv3(conv2)\n",
        "        conv4 = self.conv4(conv3)\n",
        "        conv4 = self.conv4(conv4)\n",
        "        conv4 = self.conv4(conv4)\n",
        "        conv5 = self.conv5(conv4)\n",
        "        convt6 = self.convt6(conv5)\n",
        "        conv6 = torch.cat((conv4, convt6), 1)\n",
        "        convt7 = self.convt7(conv6)\n",
        "        conv6 = torch.cat((conv4, convt7), 1)\n",
        "        convt7 = self.convt7(conv6)\n",
        "        conv6 = torch.cat((conv4, convt7), 1)\n",
        "        convt7 = self.convt7(conv6)\n",
        "        conv7 = torch.cat((conv3, convt7), 1)\n",
        "        convt8 = self.convt8(conv7)\n",
        "        conv8 = torch.cat((conv2, convt8), 1)\n",
        "        convt9 = self.convt9(conv8)\n",
        "        conv9 = torch.cat((conv1, convt9), 1)\n",
        "        convt10 = self.convt10(conv9)\n",
        "        conv10 = torch.cat((conv0, convt10), 1)\n",
        "        convt11 = self.convt11(conv10)\n",
        "\n",
        "        return convt11\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.normal_(m.weight, mean=0, std=0.02)\n",
        "                torch.nn.init.constant_(m.bias, 0.1)\n",
        "\n",
        "\n",
        "class Generator_second(nn.Module):\n",
        "    def  __init__(self):\n",
        "        super(Generator_second, self).__init__()\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(4, 64, 3, 1, 1),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt6 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt7 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1024, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt8 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1024, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt9 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt10 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.convt11 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 3, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, input):\n",
        "        conv0 = self.conv0(input)\n",
        "        conv1 = self.conv1(conv0)\n",
        "        conv2 = self.conv2(conv1)\n",
        "        conv3 = self.conv3(conv2)\n",
        "        conv4 = self.conv4(conv3)\n",
        "        conv4 = self.conv4(conv4)\n",
        "        conv4 = self.conv4(conv4)\n",
        "        conv5 = self.conv5(conv4)\n",
        "        convt6 = self.convt6(conv5)\n",
        "        conv6 = torch.cat((conv4, convt6), 1)\n",
        "        convt7 = self.convt7(conv6)\n",
        "        conv6 = torch.cat((conv4, convt7), 1)\n",
        "        convt7 = self.convt7(conv6)\n",
        "        conv6 = torch.cat((conv4, convt7), 1)\n",
        "        convt7 = self.convt7(conv6)\n",
        "        conv7 = torch.cat((conv3, convt7), 1)\n",
        "        convt8 = self.convt8(conv7)\n",
        "        conv8 = torch.cat((conv2, convt8), 1)\n",
        "        convt9 = self.convt9(conv8)\n",
        "        conv9 = torch.cat((conv1, convt9), 1)\n",
        "        convt10 = self.convt10(conv9)\n",
        "        conv10 = torch.cat((conv0, convt10), 1)\n",
        "        convt11 = self.convt11(conv10)\n",
        "\n",
        "        return convt11\n",
        "\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.normal_(m.weight, mean=0, std=0.02)\n",
        "                torch.nn.init.constant_(m.bias, 0.1)\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator_first(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator_first, self).__init__()\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv2d(4, 64, 3, 1, 1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(512, 1, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.feature(input)\n",
        "        return output\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.normal_(m.weight, mean=0, std=0.02)\n",
        "                torch.nn.init.constant_(m.bias, 0.1)\n",
        "\n",
        "\n",
        "class Discriminator_second(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator_second, self).__init__()\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv2d(7, 64, 3, 1, 1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(512, 1, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.feature(input)\n",
        "        return output\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.normal_(m.weight, mean=0, std=0.02)\n",
        "                torch.nn.init.constant_(m.bias, 0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh5Q4_7UieK_",
        "colab_type": "text"
      },
      "source": [
        "Main "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNVBWyDd1pDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data as Data\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8ATYKIUiliD",
        "colab_type": "text"
      },
      "source": [
        "Check whether GPU is enabled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOk6rzXa1pDh",
        "colab_type": "code",
        "outputId": "56f901fb-c318-4ccb-a154-223c18644af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrekWyTI1pDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE=1\n",
        "lambda1 = 5\n",
        "lambda2 = 0.1\n",
        "lambda3 = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USkg-GoBoGd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3aa220a5-55ac-4ed1-a9b7-a461e86df358"
      },
      "source": [
        "THIS_FOLDER = os.getcwd()\n",
        "print(THIS_FOLDER)\n",
        "!mkdir \"/content/models\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYJ9n5Yc1pDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def single_gpu_train():\n",
        "    dataset = shadow_triplets_loader()\n",
        "    data_loader = Data.DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    G1 = Generator_first().to(device)\n",
        "    G2 = Generator_second().to(device)\n",
        "    D1 = Discriminator_first().to(device)\n",
        "    D2 = Discriminator_second().to(device)\n",
        "\n",
        "\n",
        "    criterion1 = torch.nn.BCELoss(size_average=False)\n",
        "    criterion2 = torch.nn.L1Loss()\n",
        "    optimizerd = torch.optim.Adam([\n",
        "        {'params': D1.parameters()},\n",
        "        {'params': D2.parameters()}], lr=0.001)\n",
        "    optimizerg = torch.optim.Adam([\n",
        "        {'params': G1.parameters()},\n",
        "        {'params': G2.parameters()}], lr=0.001)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        for i, data in enumerate(data_loader):\n",
        "            original_image, shadow_mask, shadow_free_image = data\n",
        "            original_image = original_image.to(device)\n",
        "            shadow_mask = shadow_mask.to(device)\n",
        "            shadow_free_image = shadow_free_image.to(device)\n",
        "\n",
        "            g1_output = G1(original_image)\n",
        "            g1 = torch.cat((original_image, g1_output), 1)\n",
        "            gt1 = torch.cat((original_image, shadow_mask), 1)\n",
        "\n",
        "            prob_gt1 = D1(gt1).detach()\n",
        "            prob_g1 = D1(g1)\n",
        "\n",
        "            #D1_loss = -torch.mean(torch.log(prob_gt1) +  torch.log(1 - prob_g1))\n",
        "            #G1_loss = torch.mean(torch.log(shadow_mask - g1_output))\n",
        "            D1_loss = criterion1(prob_g1, prob_gt1)\n",
        "            G1_loss = criterion2(g1_output, shadow_mask)\n",
        "\n",
        "            g2_input = torch.cat((original_image, shadow_mask), 1)\n",
        "            g2_output = G2(g2_input)\n",
        "\n",
        "            gt2 = torch.cat((original_image, shadow_mask, shadow_free_image), 1)\n",
        "            g2 = torch.cat((original_image, g1_output, g2_output), 1)\n",
        "\n",
        "            prob_gt2 = D2(gt2).detach()\n",
        "            prob_g2 = D2(g2)\n",
        "\n",
        "            #D2_loss = -torch.mean(torch.log(prob_gt2) + torch.log(1 - prob_g2))\n",
        "            #G2_loss = torch.mean(torch.log(shadow_free_image, g2_output))\n",
        "            D2_loss = criterion1(prob_g2, prob_gt2)\n",
        "            G2_loss = criterion2(g2_output, shadow_free_image)\n",
        "\n",
        "            loss = G1_loss + lambda1 * G2_loss + lambda2 * D1_loss + lambda3 * D2_loss\n",
        "            print('Epoch: %d | iter: %d | train loss: %.10f' % (epoch, i, float(loss)))\n",
        "            if epoch % 2000 < 1000:\n",
        "                optimizerd.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizerd.step()\n",
        "            else:\n",
        "                optimizerg.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizerg.step()\n",
        "            \n",
        "\n",
        "        generator1_model = os.path.join(THIS_FOLDER,\"model/generator1_%d.pkl\" % epoch)\n",
        "        generator2_model = os.path.join(THIS_FOLDER,\"model/generator2_%d.pkl\" % epoch)\n",
        "        discriminator1_model = os.path.join(THIS_FOLDER,\"model/discriminator1_%d.pkl\" % epoch)\n",
        "        discriminator2_model = os.path.join(THIS_FOLDER,\"model/discriminator2_%d.pkl\" % epoch)\n",
        "        torch.save(G1.state_dict(), generator1_model)\n",
        "        torch.save(G2.state_dict(), generator2_model)\n",
        "        torch.save(D1.state_dict(), discriminator1_model)\n",
        "        torch.save(D2.state_dict(), discriminator2_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svxW5rlo1pDo",
        "colab_type": "code",
        "outputId": "5f4804bd-1aa4-456a-bea2-aae133ae92bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "single_gpu_train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 0 | iter: 93 | train loss: 3637.9711914062\n",
            "Epoch: 0 | iter: 94 | train loss: 4025.8056640625\n",
            "Epoch: 0 | iter: 95 | train loss: 3295.4125976562\n",
            "Epoch: 0 | iter: 96 | train loss: 2807.0349121094\n",
            "Epoch: 0 | iter: 97 | train loss: 5198.6147460938\n",
            "Epoch: 0 | iter: 98 | train loss: 2983.0083007812\n",
            "Epoch: 0 | iter: 99 | train loss: 5476.3896484375\n",
            "Epoch: 0 | iter: 100 | train loss: 2945.2133789062\n",
            "Epoch: 0 | iter: 101 | train loss: 3491.6235351562\n",
            "Epoch: 0 | iter: 102 | train loss: 3640.7768554688\n",
            "Epoch: 0 | iter: 103 | train loss: 3211.3901367188\n",
            "Epoch: 0 | iter: 104 | train loss: 2818.6359863281\n",
            "Epoch: 0 | iter: 105 | train loss: 3562.7590332031\n",
            "Epoch: 0 | iter: 106 | train loss: 4460.6811523438\n",
            "Epoch: 0 | iter: 107 | train loss: 3865.8618164062\n",
            "Epoch: 0 | iter: 108 | train loss: 4514.1591796875\n",
            "Epoch: 0 | iter: 109 | train loss: 2980.7270507812\n",
            "Epoch: 0 | iter: 110 | train loss: 3947.5766601562\n",
            "Epoch: 0 | iter: 111 | train loss: 3401.9960937500\n",
            "Epoch: 0 | iter: 112 | train loss: 4671.5712890625\n",
            "Epoch: 0 | iter: 113 | train loss: 5315.0546875000\n",
            "Epoch: 0 | iter: 114 | train loss: 3359.6635742188\n",
            "Epoch: 0 | iter: 115 | train loss: 4034.8579101562\n",
            "Epoch: 0 | iter: 116 | train loss: 3638.8330078125\n",
            "Epoch: 0 | iter: 117 | train loss: 3422.6477050781\n",
            "Epoch: 0 | iter: 118 | train loss: 3752.3403320312\n",
            "Epoch: 0 | iter: 119 | train loss: 4413.2060546875\n",
            "Epoch: 0 | iter: 120 | train loss: 3873.7875976562\n",
            "Epoch: 0 | iter: 121 | train loss: 4769.6899414062\n",
            "Epoch: 0 | iter: 122 | train loss: 5340.8828125000\n",
            "Epoch: 0 | iter: 123 | train loss: 5346.9501953125\n",
            "Epoch: 0 | iter: 124 | train loss: 3231.0126953125\n",
            "Epoch: 0 | iter: 125 | train loss: 4966.1928710938\n",
            "Epoch: 0 | iter: 126 | train loss: 3537.9272460938\n",
            "Epoch: 0 | iter: 127 | train loss: 4391.2163085938\n",
            "Epoch: 0 | iter: 128 | train loss: 4094.3320312500\n",
            "Epoch: 0 | iter: 129 | train loss: 3906.1367187500\n",
            "Epoch: 0 | iter: 130 | train loss: 4053.9790039062\n",
            "Epoch: 0 | iter: 131 | train loss: 3147.9584960938\n",
            "Epoch: 0 | iter: 132 | train loss: 3482.7343750000\n",
            "Epoch: 0 | iter: 133 | train loss: 5114.3583984375\n",
            "Epoch: 0 | iter: 134 | train loss: 4548.1640625000\n",
            "Epoch: 0 | iter: 135 | train loss: 4375.4897460938\n",
            "Epoch: 0 | iter: 136 | train loss: 4866.9428710938\n",
            "Epoch: 0 | iter: 137 | train loss: 3519.6704101562\n",
            "Epoch: 0 | iter: 138 | train loss: 3929.9101562500\n",
            "Epoch: 0 | iter: 139 | train loss: 3992.0332031250\n",
            "Epoch: 0 | iter: 140 | train loss: 3783.3652343750\n",
            "Epoch: 0 | iter: 141 | train loss: 4226.6152343750\n",
            "Epoch: 0 | iter: 142 | train loss: 4221.8833007812\n",
            "Epoch: 0 | iter: 143 | train loss: 4834.4877929688\n",
            "Epoch: 0 | iter: 144 | train loss: 4122.5781250000\n",
            "Epoch: 0 | iter: 145 | train loss: 4515.9814453125\n",
            "Epoch: 0 | iter: 146 | train loss: 3507.3950195312\n",
            "Epoch: 0 | iter: 147 | train loss: 4203.9799804688\n",
            "Epoch: 0 | iter: 148 | train loss: 4224.3876953125\n",
            "Epoch: 0 | iter: 149 | train loss: 4573.8203125000\n",
            "Epoch: 0 | iter: 150 | train loss: 5607.0107421875\n",
            "Epoch: 0 | iter: 151 | train loss: 4763.4755859375\n",
            "Epoch: 0 | iter: 152 | train loss: 3738.4960937500\n",
            "Epoch: 0 | iter: 153 | train loss: 4792.6240234375\n",
            "Epoch: 0 | iter: 154 | train loss: 5105.6269531250\n",
            "Epoch: 0 | iter: 155 | train loss: 4319.0625000000\n",
            "Epoch: 0 | iter: 156 | train loss: 4006.6093750000\n",
            "Epoch: 0 | iter: 157 | train loss: 4254.3837890625\n",
            "Epoch: 0 | iter: 158 | train loss: 4349.9565429688\n",
            "Epoch: 0 | iter: 159 | train loss: 3758.3188476562\n",
            "Epoch: 0 | iter: 160 | train loss: 4469.6699218750\n",
            "Epoch: 0 | iter: 161 | train loss: 4645.1049804688\n",
            "Epoch: 0 | iter: 162 | train loss: 4619.6250000000\n",
            "Epoch: 0 | iter: 163 | train loss: 5894.4130859375\n",
            "Epoch: 0 | iter: 164 | train loss: 3797.6479492188\n",
            "Epoch: 0 | iter: 165 | train loss: 5022.8447265625\n",
            "Epoch: 0 | iter: 166 | train loss: 5148.9379882812\n",
            "Epoch: 0 | iter: 167 | train loss: 4210.2441406250\n",
            "Epoch: 0 | iter: 168 | train loss: 4740.6020507812\n",
            "Epoch: 0 | iter: 169 | train loss: 4661.4184570312\n",
            "Epoch: 0 | iter: 170 | train loss: 5301.7744140625\n",
            "Epoch: 0 | iter: 171 | train loss: 4658.7558593750\n",
            "Epoch: 0 | iter: 172 | train loss: 4053.5615234375\n",
            "Epoch: 0 | iter: 173 | train loss: 5092.4282226562\n",
            "Epoch: 0 | iter: 174 | train loss: 4662.2773437500\n",
            "Epoch: 0 | iter: 175 | train loss: 3907.4414062500\n",
            "Epoch: 0 | iter: 176 | train loss: 4419.0830078125\n",
            "Epoch: 0 | iter: 177 | train loss: 4363.0605468750\n",
            "Epoch: 0 | iter: 178 | train loss: 4778.4946289062\n",
            "Epoch: 0 | iter: 179 | train loss: 4939.7729492188\n",
            "Epoch: 0 | iter: 180 | train loss: 5145.4790039062\n",
            "Epoch: 0 | iter: 181 | train loss: 4791.4277343750\n",
            "Epoch: 0 | iter: 182 | train loss: 4724.7583007812\n",
            "Epoch: 0 | iter: 183 | train loss: 4219.1152343750\n",
            "Epoch: 0 | iter: 184 | train loss: 4211.6572265625\n",
            "Epoch: 0 | iter: 185 | train loss: 4247.3652343750\n",
            "Epoch: 0 | iter: 186 | train loss: 5492.0000000000\n",
            "Epoch: 0 | iter: 187 | train loss: 4201.0942382812\n",
            "Epoch: 0 | iter: 188 | train loss: 4224.3383789062\n",
            "Epoch: 0 | iter: 189 | train loss: 6028.8652343750\n",
            "Epoch: 0 | iter: 190 | train loss: 4315.7729492188\n",
            "Epoch: 0 | iter: 191 | train loss: 4410.3837890625\n",
            "Epoch: 0 | iter: 192 | train loss: 4875.2734375000\n",
            "Epoch: 0 | iter: 193 | train loss: 3885.6274414062\n",
            "Epoch: 0 | iter: 194 | train loss: 4237.3427734375\n",
            "Epoch: 0 | iter: 195 | train loss: 4613.7944335938\n",
            "Epoch: 0 | iter: 196 | train loss: 5045.7807617188\n",
            "Epoch: 0 | iter: 197 | train loss: 4115.7099609375\n",
            "Epoch: 0 | iter: 198 | train loss: 3527.3762207031\n",
            "Epoch: 0 | iter: 199 | train loss: 4461.3002929688\n",
            "Epoch: 0 | iter: 200 | train loss: 4545.3291015625\n",
            "Epoch: 0 | iter: 201 | train loss: 4601.1899414062\n",
            "Epoch: 0 | iter: 202 | train loss: 6207.9462890625\n",
            "Epoch: 0 | iter: 203 | train loss: 4045.6650390625\n",
            "Epoch: 0 | iter: 204 | train loss: 5203.9589843750\n",
            "Epoch: 0 | iter: 205 | train loss: 4881.9394531250\n",
            "Epoch: 0 | iter: 206 | train loss: 3941.1342773438\n",
            "Epoch: 0 | iter: 207 | train loss: 3726.9316406250\n",
            "Epoch: 0 | iter: 208 | train loss: 4913.4643554688\n",
            "Epoch: 0 | iter: 209 | train loss: 4644.3740234375\n",
            "Epoch: 0 | iter: 210 | train loss: 5363.5795898438\n",
            "Epoch: 0 | iter: 211 | train loss: 3823.1166992188\n",
            "Epoch: 0 | iter: 212 | train loss: 3912.7119140625\n",
            "Epoch: 0 | iter: 213 | train loss: 4898.0922851562\n",
            "Epoch: 0 | iter: 214 | train loss: 4445.6660156250\n",
            "Epoch: 0 | iter: 215 | train loss: 4537.2358398438\n",
            "Epoch: 0 | iter: 216 | train loss: 4645.5893554688\n",
            "Epoch: 0 | iter: 217 | train loss: 4460.0942382812\n",
            "Epoch: 0 | iter: 218 | train loss: 4917.2958984375\n",
            "Epoch: 0 | iter: 219 | train loss: 3846.1591796875\n",
            "Epoch: 0 | iter: 220 | train loss: 4448.8085937500\n",
            "Epoch: 0 | iter: 221 | train loss: 3998.7558593750\n",
            "Epoch: 0 | iter: 222 | train loss: 4266.6425781250\n",
            "Epoch: 0 | iter: 223 | train loss: 4486.1835937500\n",
            "Epoch: 0 | iter: 224 | train loss: 3804.2104492188\n",
            "Epoch: 0 | iter: 225 | train loss: 4219.8461914062\n",
            "Epoch: 0 | iter: 226 | train loss: 2871.4328613281\n",
            "Epoch: 0 | iter: 227 | train loss: 4649.8686523438\n",
            "Epoch: 0 | iter: 228 | train loss: 3582.9760742188\n",
            "Epoch: 0 | iter: 229 | train loss: 5269.4453125000\n",
            "Epoch: 0 | iter: 230 | train loss: 3823.1088867188\n",
            "Epoch: 0 | iter: 231 | train loss: 3291.5180664062\n",
            "Epoch: 0 | iter: 232 | train loss: 4755.9741210938\n",
            "Epoch: 0 | iter: 233 | train loss: 3322.7231445312\n",
            "Epoch: 0 | iter: 234 | train loss: 5088.9453125000\n",
            "Epoch: 0 | iter: 235 | train loss: 5429.2070312500\n",
            "Epoch: 0 | iter: 236 | train loss: 4250.8979492188\n",
            "Epoch: 0 | iter: 237 | train loss: 3616.6508789062\n",
            "Epoch: 0 | iter: 238 | train loss: 3643.3632812500\n",
            "Epoch: 0 | iter: 239 | train loss: 2759.4721679688\n",
            "Epoch: 0 | iter: 240 | train loss: 3787.3710937500\n",
            "Epoch: 0 | iter: 241 | train loss: 3661.2846679688\n",
            "Epoch: 0 | iter: 242 | train loss: 4504.5976562500\n",
            "Epoch: 0 | iter: 243 | train loss: 4828.4584960938\n",
            "Epoch: 0 | iter: 244 | train loss: 4041.5771484375\n",
            "Epoch: 0 | iter: 245 | train loss: 4150.0312500000\n",
            "Epoch: 0 | iter: 246 | train loss: 4367.1850585938\n",
            "Epoch: 0 | iter: 247 | train loss: 3690.3945312500\n",
            "Epoch: 0 | iter: 248 | train loss: 3924.1804199219\n",
            "Epoch: 0 | iter: 249 | train loss: 4043.5151367188\n",
            "Epoch: 0 | iter: 250 | train loss: 4503.7753906250\n",
            "Epoch: 0 | iter: 251 | train loss: 4141.7285156250\n",
            "Epoch: 0 | iter: 252 | train loss: 3833.7387695312\n",
            "Epoch: 0 | iter: 253 | train loss: 2962.1325683594\n",
            "Epoch: 0 | iter: 254 | train loss: 3096.7363281250\n",
            "Epoch: 0 | iter: 255 | train loss: 3023.6452636719\n",
            "Epoch: 0 | iter: 256 | train loss: 3076.5148925781\n",
            "Epoch: 0 | iter: 257 | train loss: 2691.0944824219\n",
            "Epoch: 0 | iter: 258 | train loss: 4651.4755859375\n",
            "Epoch: 0 | iter: 259 | train loss: 3128.2900390625\n",
            "Epoch: 0 | iter: 260 | train loss: 4086.6406250000\n",
            "Epoch: 0 | iter: 261 | train loss: 3299.8857421875\n",
            "Epoch: 0 | iter: 262 | train loss: 3763.0759277344\n",
            "Epoch: 0 | iter: 263 | train loss: 2628.6647949219\n",
            "Epoch: 0 | iter: 264 | train loss: 3360.9594726562\n",
            "Epoch: 0 | iter: 265 | train loss: 3693.4609375000\n",
            "Epoch: 0 | iter: 266 | train loss: 3472.7065429688\n",
            "Epoch: 0 | iter: 267 | train loss: 3428.7490234375\n",
            "Epoch: 0 | iter: 268 | train loss: 2747.4228515625\n",
            "Epoch: 0 | iter: 269 | train loss: 3965.4750976562\n",
            "Epoch: 0 | iter: 270 | train loss: 2842.8891601562\n",
            "Epoch: 0 | iter: 271 | train loss: 4007.7419433594\n",
            "Epoch: 0 | iter: 272 | train loss: 2664.9885253906\n",
            "Epoch: 0 | iter: 273 | train loss: 3726.1892089844\n",
            "Epoch: 0 | iter: 274 | train loss: 3597.0637207031\n",
            "Epoch: 0 | iter: 275 | train loss: 3580.8237304688\n",
            "Epoch: 0 | iter: 276 | train loss: 3582.8903808594\n",
            "Epoch: 0 | iter: 277 | train loss: 3972.6596679688\n",
            "Epoch: 0 | iter: 278 | train loss: 3443.6379394531\n",
            "Epoch: 0 | iter: 279 | train loss: 3532.0693359375\n",
            "Epoch: 0 | iter: 280 | train loss: 3143.4338378906\n",
            "Epoch: 0 | iter: 281 | train loss: 3401.9345703125\n",
            "Epoch: 0 | iter: 282 | train loss: 3684.4794921875\n",
            "Epoch: 0 | iter: 283 | train loss: 3263.7456054688\n",
            "Epoch: 0 | iter: 284 | train loss: 3022.5712890625\n",
            "Epoch: 0 | iter: 285 | train loss: 3138.3242187500\n",
            "Epoch: 0 | iter: 286 | train loss: 3391.1347656250\n",
            "Epoch: 0 | iter: 287 | train loss: 3428.1987304688\n",
            "Epoch: 0 | iter: 288 | train loss: 3408.9482421875\n",
            "Epoch: 0 | iter: 289 | train loss: 2482.4060058594\n",
            "Epoch: 0 | iter: 290 | train loss: 2762.6022949219\n",
            "Epoch: 0 | iter: 291 | train loss: 2501.6127929688\n",
            "Epoch: 0 | iter: 292 | train loss: 2780.9467773438\n",
            "Epoch: 0 | iter: 293 | train loss: 2933.7509765625\n",
            "Epoch: 0 | iter: 294 | train loss: 2989.1806640625\n",
            "Epoch: 0 | iter: 295 | train loss: 2700.5927734375\n",
            "Epoch: 0 | iter: 296 | train loss: 3346.6567382812\n",
            "Epoch: 0 | iter: 297 | train loss: 2759.5219726562\n",
            "Epoch: 0 | iter: 298 | train loss: 2668.0361328125\n",
            "Epoch: 0 | iter: 299 | train loss: 2278.1677246094\n",
            "Epoch: 0 | iter: 300 | train loss: 3543.8310546875\n",
            "Epoch: 0 | iter: 301 | train loss: 3090.1035156250\n",
            "Epoch: 0 | iter: 302 | train loss: 3290.8227539062\n",
            "Epoch: 0 | iter: 303 | train loss: 3354.5791015625\n",
            "Epoch: 0 | iter: 304 | train loss: 3539.7387695312\n",
            "Epoch: 0 | iter: 305 | train loss: 2824.4338378906\n",
            "Epoch: 0 | iter: 306 | train loss: 3413.7675781250\n",
            "Epoch: 0 | iter: 307 | train loss: 5044.7744140625\n",
            "Epoch: 0 | iter: 308 | train loss: 2155.2802734375\n",
            "Epoch: 0 | iter: 309 | train loss: 2598.9243164062\n",
            "Epoch: 0 | iter: 310 | train loss: 2954.6899414062\n",
            "Epoch: 0 | iter: 311 | train loss: 2626.3305664062\n",
            "Epoch: 0 | iter: 312 | train loss: 2430.0605468750\n",
            "Epoch: 0 | iter: 313 | train loss: 2823.9096679688\n",
            "Epoch: 0 | iter: 314 | train loss: 2307.5817871094\n",
            "Epoch: 0 | iter: 315 | train loss: 2796.2241210938\n",
            "Epoch: 0 | iter: 316 | train loss: 2562.9677734375\n",
            "Epoch: 0 | iter: 317 | train loss: 3461.9741210938\n",
            "Epoch: 0 | iter: 318 | train loss: 3161.7670898438\n",
            "Epoch: 0 | iter: 319 | train loss: 2179.0083007812\n",
            "Epoch: 0 | iter: 320 | train loss: 2952.9497070312\n",
            "Epoch: 0 | iter: 321 | train loss: 3353.0068359375\n",
            "Epoch: 0 | iter: 322 | train loss: 2699.4638671875\n",
            "Epoch: 0 | iter: 323 | train loss: 3528.1440429688\n",
            "Epoch: 0 | iter: 324 | train loss: 2755.7036132812\n",
            "Epoch: 0 | iter: 325 | train loss: 2487.7763671875\n",
            "Epoch: 0 | iter: 326 | train loss: 3276.5048828125\n",
            "Epoch: 0 | iter: 327 | train loss: 4048.6459960938\n",
            "Epoch: 0 | iter: 328 | train loss: 2631.9746093750\n",
            "Epoch: 0 | iter: 329 | train loss: 3711.1286621094\n",
            "Epoch: 0 | iter: 330 | train loss: 2508.1655273438\n",
            "Epoch: 0 | iter: 331 | train loss: 2666.6386718750\n",
            "Epoch: 0 | iter: 332 | train loss: 3610.7133789062\n",
            "Epoch: 0 | iter: 333 | train loss: 2707.7294921875\n",
            "Epoch: 0 | iter: 334 | train loss: 3114.0732421875\n",
            "Epoch: 0 | iter: 335 | train loss: 1917.6644287109\n",
            "Epoch: 0 | iter: 336 | train loss: 2578.0874023438\n",
            "Epoch: 0 | iter: 337 | train loss: 2442.7795410156\n",
            "Epoch: 0 | iter: 338 | train loss: 3024.8603515625\n",
            "Epoch: 0 | iter: 339 | train loss: 3635.1040039062\n",
            "Epoch: 0 | iter: 340 | train loss: 2498.8286132812\n",
            "Epoch: 0 | iter: 341 | train loss: 2731.7448730469\n",
            "Epoch: 0 | iter: 342 | train loss: 4006.4541015625\n",
            "Epoch: 0 | iter: 343 | train loss: 2833.0305175781\n",
            "Epoch: 0 | iter: 344 | train loss: 3595.4711914062\n",
            "Epoch: 0 | iter: 345 | train loss: 2428.1601562500\n",
            "Epoch: 0 | iter: 346 | train loss: 2818.4829101562\n",
            "Epoch: 0 | iter: 347 | train loss: 2335.7336425781\n",
            "Epoch: 0 | iter: 348 | train loss: 3227.3332519531\n",
            "Epoch: 0 | iter: 349 | train loss: 3415.3305664062\n",
            "Epoch: 0 | iter: 350 | train loss: 2535.1843261719\n",
            "Epoch: 0 | iter: 351 | train loss: 3127.7810058594\n",
            "Epoch: 0 | iter: 352 | train loss: 3535.3583984375\n",
            "Epoch: 0 | iter: 353 | train loss: 2927.6674804688\n",
            "Epoch: 0 | iter: 354 | train loss: 3361.7021484375\n",
            "Epoch: 0 | iter: 355 | train loss: 3627.1386718750\n",
            "Epoch: 0 | iter: 356 | train loss: 3222.3886718750\n",
            "Epoch: 0 | iter: 357 | train loss: 2668.6352539062\n",
            "Epoch: 0 | iter: 358 | train loss: 2709.2944335938\n",
            "Epoch: 0 | iter: 359 | train loss: 3392.6535644531\n",
            "Epoch: 0 | iter: 360 | train loss: 3048.1293945312\n",
            "Epoch: 0 | iter: 361 | train loss: 2829.5556640625\n",
            "Epoch: 0 | iter: 362 | train loss: 2570.4194335938\n",
            "Epoch: 0 | iter: 363 | train loss: 3265.6923828125\n",
            "Epoch: 0 | iter: 364 | train loss: 3009.7609863281\n",
            "Epoch: 0 | iter: 365 | train loss: 1849.8356933594\n",
            "Epoch: 0 | iter: 366 | train loss: 2228.3112792969\n",
            "Epoch: 0 | iter: 367 | train loss: 2947.3906250000\n",
            "Epoch: 0 | iter: 368 | train loss: 1492.5341796875\n",
            "Epoch: 0 | iter: 369 | train loss: 3264.3735351562\n",
            "Epoch: 0 | iter: 370 | train loss: 3239.4311523438\n",
            "Epoch: 0 | iter: 371 | train loss: 2413.7392578125\n",
            "Epoch: 0 | iter: 372 | train loss: 2956.3464355469\n",
            "Epoch: 0 | iter: 373 | train loss: 2618.4123535156\n",
            "Epoch: 0 | iter: 374 | train loss: 2641.9982910156\n",
            "Epoch: 0 | iter: 375 | train loss: 2895.0949707031\n",
            "Epoch: 0 | iter: 376 | train loss: 1959.9392089844\n",
            "Epoch: 0 | iter: 377 | train loss: 2102.5432128906\n",
            "Epoch: 0 | iter: 378 | train loss: 3356.8159179688\n",
            "Epoch: 0 | iter: 379 | train loss: 2693.8105468750\n",
            "Epoch: 0 | iter: 380 | train loss: 4903.0351562500\n",
            "Epoch: 0 | iter: 381 | train loss: 2065.3679199219\n",
            "Epoch: 0 | iter: 382 | train loss: 2013.2338867188\n",
            "Epoch: 0 | iter: 383 | train loss: 3347.1713867188\n",
            "Epoch: 0 | iter: 384 | train loss: 2891.8217773438\n",
            "Epoch: 0 | iter: 385 | train loss: 2665.2617187500\n",
            "Epoch: 0 | iter: 386 | train loss: 3461.4150390625\n",
            "Epoch: 0 | iter: 387 | train loss: 2585.3764648438\n",
            "Epoch: 0 | iter: 388 | train loss: 2923.9033203125\n",
            "Epoch: 0 | iter: 389 | train loss: 3175.5048828125\n",
            "Epoch: 0 | iter: 390 | train loss: 2518.2812500000\n",
            "Epoch: 0 | iter: 391 | train loss: 2291.4458007812\n",
            "Epoch: 0 | iter: 392 | train loss: 1868.3037109375\n",
            "Epoch: 0 | iter: 393 | train loss: 4210.6606445312\n",
            "Epoch: 0 | iter: 394 | train loss: 2492.3330078125\n",
            "Epoch: 0 | iter: 395 | train loss: 2800.0700683594\n",
            "Epoch: 0 | iter: 396 | train loss: 2682.8012695312\n",
            "Epoch: 0 | iter: 397 | train loss: 3728.5708007812\n",
            "Epoch: 0 | iter: 398 | train loss: 3067.5729980469\n",
            "Epoch: 0 | iter: 399 | train loss: 2701.3647460938\n",
            "Epoch: 0 | iter: 400 | train loss: 3880.6191406250\n",
            "Epoch: 0 | iter: 401 | train loss: 3070.1442871094\n",
            "Epoch: 0 | iter: 402 | train loss: 2896.9316406250\n",
            "Epoch: 0 | iter: 403 | train loss: 2849.2648925781\n",
            "Epoch: 0 | iter: 404 | train loss: 2627.1582031250\n",
            "Epoch: 0 | iter: 405 | train loss: 3284.8237304688\n",
            "Epoch: 0 | iter: 406 | train loss: 3160.0891113281\n",
            "Epoch: 0 | iter: 407 | train loss: 2815.2336425781\n",
            "Epoch: 0 | iter: 408 | train loss: 3593.3720703125\n",
            "Epoch: 0 | iter: 409 | train loss: 3298.2900390625\n",
            "Epoch: 0 | iter: 410 | train loss: 3218.0407714844\n",
            "Epoch: 0 | iter: 411 | train loss: 2843.7363281250\n",
            "Epoch: 0 | iter: 412 | train loss: 2894.8041992188\n",
            "Epoch: 0 | iter: 413 | train loss: 2724.6782226562\n",
            "Epoch: 0 | iter: 414 | train loss: 2726.7592773438\n",
            "Epoch: 0 | iter: 415 | train loss: 3821.2231445312\n",
            "Epoch: 0 | iter: 416 | train loss: 2437.0688476562\n",
            "Epoch: 0 | iter: 417 | train loss: 3367.1289062500\n",
            "Epoch: 0 | iter: 418 | train loss: 4030.4294433594\n",
            "Epoch: 0 | iter: 419 | train loss: 2664.9511718750\n",
            "Epoch: 0 | iter: 420 | train loss: 3775.6958007812\n",
            "Epoch: 0 | iter: 421 | train loss: 2668.4360351562\n",
            "Epoch: 0 | iter: 422 | train loss: 3256.7573242188\n",
            "Epoch: 0 | iter: 423 | train loss: 3043.6254882812\n",
            "Epoch: 0 | iter: 424 | train loss: 2484.7998046875\n",
            "Epoch: 0 | iter: 425 | train loss: 2691.6735839844\n",
            "Epoch: 0 | iter: 426 | train loss: 3343.3618164062\n",
            "Epoch: 0 | iter: 427 | train loss: 3075.2690429688\n",
            "Epoch: 0 | iter: 428 | train loss: 3367.5883789062\n",
            "Epoch: 0 | iter: 429 | train loss: 2264.7041015625\n",
            "Epoch: 0 | iter: 430 | train loss: 2913.9648437500\n",
            "Epoch: 0 | iter: 431 | train loss: 2643.9951171875\n",
            "Epoch: 0 | iter: 432 | train loss: 3406.7016601562\n",
            "Epoch: 0 | iter: 433 | train loss: 2596.0798339844\n",
            "Epoch: 0 | iter: 434 | train loss: 2829.6315917969\n",
            "Epoch: 0 | iter: 435 | train loss: 3422.5532226562\n",
            "Epoch: 0 | iter: 436 | train loss: 4045.0087890625\n",
            "Epoch: 0 | iter: 437 | train loss: 2436.9528808594\n",
            "Epoch: 0 | iter: 438 | train loss: 3105.8093261719\n",
            "Epoch: 0 | iter: 439 | train loss: 2441.8359375000\n",
            "Epoch: 0 | iter: 440 | train loss: 3337.4174804688\n",
            "Epoch: 0 | iter: 441 | train loss: 2947.6083984375\n",
            "Epoch: 0 | iter: 442 | train loss: 3466.0234375000\n",
            "Epoch: 0 | iter: 443 | train loss: 3305.0419921875\n",
            "Epoch: 0 | iter: 444 | train loss: 3278.2929687500\n",
            "Epoch: 0 | iter: 445 | train loss: 3271.0373535156\n",
            "Epoch: 0 | iter: 446 | train loss: 3468.7504882812\n",
            "Epoch: 0 | iter: 447 | train loss: 3183.9172363281\n",
            "Epoch: 0 | iter: 448 | train loss: 3514.1479492188\n",
            "Epoch: 0 | iter: 449 | train loss: 3167.3791503906\n",
            "Epoch: 0 | iter: 450 | train loss: 2929.0234375000\n",
            "Epoch: 0 | iter: 451 | train loss: 3058.3435058594\n",
            "Epoch: 0 | iter: 452 | train loss: 3230.1723632812\n",
            "Epoch: 0 | iter: 453 | train loss: 3693.1850585938\n",
            "Epoch: 0 | iter: 454 | train loss: 4342.0019531250\n",
            "Epoch: 0 | iter: 455 | train loss: 2742.9936523438\n",
            "Epoch: 0 | iter: 456 | train loss: 4820.7202148438\n",
            "Epoch: 0 | iter: 457 | train loss: 4808.0571289062\n",
            "Epoch: 0 | iter: 458 | train loss: 4651.4838867188\n",
            "Epoch: 0 | iter: 459 | train loss: 4313.9892578125\n",
            "Epoch: 0 | iter: 460 | train loss: 4760.4804687500\n",
            "Epoch: 0 | iter: 461 | train loss: 3346.0476074219\n",
            "Epoch: 0 | iter: 462 | train loss: 3141.9655761719\n",
            "Epoch: 0 | iter: 463 | train loss: 3995.0771484375\n",
            "Epoch: 0 | iter: 464 | train loss: 4037.1242675781\n",
            "Epoch: 0 | iter: 465 | train loss: 1989.0776367188\n",
            "Epoch: 0 | iter: 466 | train loss: 2473.6296386719\n",
            "Epoch: 0 | iter: 467 | train loss: 3616.1982421875\n",
            "Epoch: 0 | iter: 468 | train loss: 2259.8869628906\n",
            "Epoch: 0 | iter: 469 | train loss: 1315.8496093750\n",
            "Epoch: 0 | iter: 470 | train loss: 2826.6542968750\n",
            "Epoch: 0 | iter: 471 | train loss: 3406.4135742188\n",
            "Epoch: 0 | iter: 472 | train loss: 2482.9008789062\n",
            "Epoch: 0 | iter: 473 | train loss: 2234.1379394531\n",
            "Epoch: 0 | iter: 474 | train loss: 3803.1191406250\n",
            "Epoch: 0 | iter: 475 | train loss: 2410.2253417969\n",
            "Epoch: 0 | iter: 476 | train loss: 2533.1406250000\n",
            "Epoch: 0 | iter: 477 | train loss: 1631.0524902344\n",
            "Epoch: 0 | iter: 478 | train loss: 1658.3308105469\n",
            "Epoch: 0 | iter: 479 | train loss: 2583.2961425781\n",
            "Epoch: 0 | iter: 480 | train loss: 2703.4846191406\n",
            "Epoch: 0 | iter: 481 | train loss: 3568.5312500000\n",
            "Epoch: 0 | iter: 482 | train loss: 2885.9438476562\n",
            "Epoch: 0 | iter: 483 | train loss: 1619.2055664062\n",
            "Epoch: 0 | iter: 484 | train loss: 1672.8566894531\n",
            "Epoch: 0 | iter: 485 | train loss: 2008.9865722656\n",
            "Epoch: 0 | iter: 486 | train loss: 1943.5664062500\n",
            "Epoch: 0 | iter: 487 | train loss: 2973.0102539062\n",
            "Epoch: 0 | iter: 488 | train loss: 2827.3842773438\n",
            "Epoch: 0 | iter: 489 | train loss: 2307.4372558594\n",
            "Epoch: 0 | iter: 490 | train loss: 2837.7592773438\n",
            "Epoch: 0 | iter: 491 | train loss: 1952.6694335938\n",
            "Epoch: 0 | iter: 492 | train loss: 2173.1279296875\n",
            "Epoch: 0 | iter: 493 | train loss: 3021.7768554688\n",
            "Epoch: 0 | iter: 494 | train loss: 1853.2401123047\n",
            "Epoch: 0 | iter: 495 | train loss: 3876.7514648438\n",
            "Epoch: 0 | iter: 496 | train loss: 2934.4091796875\n",
            "Epoch: 0 | iter: 497 | train loss: 2855.0075683594\n",
            "Epoch: 0 | iter: 498 | train loss: 3086.9562988281\n",
            "Epoch: 0 | iter: 499 | train loss: 2200.0256347656\n",
            "Epoch: 0 | iter: 500 | train loss: 2572.4072265625\n",
            "Epoch: 0 | iter: 501 | train loss: 2813.8330078125\n",
            "Epoch: 0 | iter: 502 | train loss: 2631.0004882812\n",
            "Epoch: 0 | iter: 503 | train loss: 2788.2780761719\n",
            "Epoch: 0 | iter: 504 | train loss: 3900.5444335938\n",
            "Epoch: 0 | iter: 505 | train loss: 2558.8229980469\n",
            "Epoch: 0 | iter: 506 | train loss: 3432.1079101562\n",
            "Epoch: 0 | iter: 507 | train loss: 2182.1118164062\n",
            "Epoch: 0 | iter: 508 | train loss: 3758.8378906250\n",
            "Epoch: 0 | iter: 509 | train loss: 3076.4479980469\n",
            "Epoch: 0 | iter: 510 | train loss: 2728.2089843750\n",
            "Epoch: 0 | iter: 511 | train loss: 3250.0417480469\n",
            "Epoch: 0 | iter: 512 | train loss: 2363.4321289062\n",
            "Epoch: 0 | iter: 513 | train loss: 2164.5026855469\n",
            "Epoch: 0 | iter: 514 | train loss: 3507.6398925781\n",
            "Epoch: 0 | iter: 515 | train loss: 3478.8974609375\n",
            "Epoch: 0 | iter: 516 | train loss: 2606.3144531250\n",
            "Epoch: 0 | iter: 517 | train loss: 2708.5874023438\n",
            "Epoch: 0 | iter: 518 | train loss: 2493.2790527344\n",
            "Epoch: 0 | iter: 519 | train loss: 3002.3203125000\n",
            "Epoch: 0 | iter: 520 | train loss: 1809.8354492188\n",
            "Epoch: 0 | iter: 521 | train loss: 2467.7468261719\n",
            "Epoch: 0 | iter: 522 | train loss: 2323.8393554688\n",
            "Epoch: 0 | iter: 523 | train loss: 2477.0322265625\n",
            "Epoch: 0 | iter: 524 | train loss: 3750.9511718750\n",
            "Epoch: 0 | iter: 525 | train loss: 2973.5913085938\n",
            "Epoch: 0 | iter: 526 | train loss: 2468.6386718750\n",
            "Epoch: 0 | iter: 527 | train loss: 1756.9268798828\n",
            "Epoch: 0 | iter: 528 | train loss: 3890.0112304688\n",
            "Epoch: 0 | iter: 529 | train loss: 1224.6287841797\n",
            "Epoch: 0 | iter: 530 | train loss: 3489.1445312500\n",
            "Epoch: 0 | iter: 531 | train loss: 3786.4326171875\n",
            "Epoch: 0 | iter: 532 | train loss: 2674.0063476562\n",
            "Epoch: 0 | iter: 533 | train loss: 3440.3408203125\n",
            "Epoch: 0 | iter: 534 | train loss: 3120.4914550781\n",
            "Epoch: 0 | iter: 535 | train loss: 3613.2807617188\n",
            "Epoch: 0 | iter: 536 | train loss: 2788.0270996094\n",
            "Epoch: 0 | iter: 537 | train loss: 2264.8166503906\n",
            "Epoch: 0 | iter: 538 | train loss: 2306.7163085938\n",
            "Epoch: 0 | iter: 539 | train loss: 2235.8862304688\n",
            "Epoch: 0 | iter: 540 | train loss: 2575.6235351562\n",
            "Epoch: 0 | iter: 541 | train loss: 1681.2049560547\n",
            "Epoch: 0 | iter: 542 | train loss: 3157.5168457031\n",
            "Epoch: 0 | iter: 543 | train loss: 2397.9409179688\n",
            "Epoch: 0 | iter: 544 | train loss: 2985.0332031250\n",
            "Epoch: 0 | iter: 545 | train loss: 4250.4453125000\n",
            "Epoch: 0 | iter: 546 | train loss: 2044.6535644531\n",
            "Epoch: 0 | iter: 547 | train loss: 2838.2597656250\n",
            "Epoch: 0 | iter: 548 | train loss: 2254.0966796875\n",
            "Epoch: 0 | iter: 549 | train loss: 2299.7636718750\n",
            "Epoch: 0 | iter: 550 | train loss: 2876.9277343750\n",
            "Epoch: 0 | iter: 551 | train loss: 2847.7568359375\n",
            "Epoch: 0 | iter: 552 | train loss: 3408.7255859375\n",
            "Epoch: 0 | iter: 553 | train loss: 3013.0307617188\n",
            "Epoch: 0 | iter: 554 | train loss: 2753.9472656250\n",
            "Epoch: 0 | iter: 555 | train loss: 1502.1876220703\n",
            "Epoch: 0 | iter: 556 | train loss: 3644.3349609375\n",
            "Epoch: 0 | iter: 557 | train loss: 3531.8671875000\n",
            "Epoch: 0 | iter: 558 | train loss: 2399.7817382812\n",
            "Epoch: 0 | iter: 559 | train loss: 4040.6406250000\n",
            "Epoch: 0 | iter: 560 | train loss: 2352.1464843750\n",
            "Epoch: 0 | iter: 561 | train loss: 2599.7094726562\n",
            "Epoch: 0 | iter: 562 | train loss: 2398.1757812500\n",
            "Epoch: 0 | iter: 563 | train loss: 4679.6655273438\n",
            "Epoch: 0 | iter: 564 | train loss: 2532.1953125000\n",
            "Epoch: 0 | iter: 565 | train loss: 2122.7438964844\n",
            "Epoch: 0 | iter: 566 | train loss: 2765.3569335938\n",
            "Epoch: 0 | iter: 567 | train loss: 2241.7089843750\n",
            "Epoch: 0 | iter: 568 | train loss: 3371.1152343750\n",
            "Epoch: 0 | iter: 569 | train loss: 1736.3872070312\n",
            "Epoch: 0 | iter: 570 | train loss: 2839.3735351562\n",
            "Epoch: 0 | iter: 571 | train loss: 2937.8408203125\n",
            "Epoch: 0 | iter: 572 | train loss: 4071.7031250000\n",
            "Epoch: 0 | iter: 573 | train loss: 4334.9028320312\n",
            "Epoch: 0 | iter: 574 | train loss: 2915.4370117188\n",
            "Epoch: 0 | iter: 575 | train loss: 2746.2353515625\n",
            "Epoch: 0 | iter: 576 | train loss: 3007.7358398438\n",
            "Epoch: 0 | iter: 577 | train loss: 3025.0424804688\n",
            "Epoch: 0 | iter: 578 | train loss: 2759.1291503906\n",
            "Epoch: 0 | iter: 579 | train loss: 2253.1872558594\n",
            "Epoch: 0 | iter: 580 | train loss: 2569.5939941406\n",
            "Epoch: 0 | iter: 581 | train loss: 1689.4501953125\n",
            "Epoch: 0 | iter: 582 | train loss: 3489.2822265625\n",
            "Epoch: 0 | iter: 583 | train loss: 1960.5014648438\n",
            "Epoch: 0 | iter: 584 | train loss: 2880.1118164062\n",
            "Epoch: 0 | iter: 585 | train loss: 1657.8063964844\n",
            "Epoch: 0 | iter: 586 | train loss: 2541.8928222656\n",
            "Epoch: 0 | iter: 587 | train loss: 1819.1622314453\n",
            "Epoch: 0 | iter: 588 | train loss: 2662.0935058594\n",
            "Epoch: 0 | iter: 589 | train loss: 1199.6979980469\n",
            "Epoch: 0 | iter: 590 | train loss: 2639.4855957031\n",
            "Epoch: 0 | iter: 591 | train loss: 2347.8266601562\n",
            "Epoch: 0 | iter: 592 | train loss: 3320.0732421875\n",
            "Epoch: 0 | iter: 593 | train loss: 2162.1669921875\n",
            "Epoch: 0 | iter: 594 | train loss: 2890.9057617188\n",
            "Epoch: 0 | iter: 595 | train loss: 2009.0773925781\n",
            "Epoch: 0 | iter: 596 | train loss: 2579.7416992188\n",
            "Epoch: 0 | iter: 597 | train loss: 3014.9008789062\n",
            "Epoch: 0 | iter: 598 | train loss: 2393.8911132812\n",
            "Epoch: 0 | iter: 599 | train loss: 4351.2470703125\n",
            "Epoch: 0 | iter: 600 | train loss: 2788.1533203125\n",
            "Epoch: 0 | iter: 601 | train loss: 2370.6540527344\n",
            "Epoch: 0 | iter: 602 | train loss: 1383.9460449219\n",
            "Epoch: 0 | iter: 603 | train loss: 1202.6280517578\n",
            "Epoch: 0 | iter: 604 | train loss: 2530.9221191406\n",
            "Epoch: 0 | iter: 605 | train loss: 2374.4812011719\n",
            "Epoch: 0 | iter: 606 | train loss: 1555.2534179688\n",
            "Epoch: 0 | iter: 607 | train loss: 1556.3840332031\n",
            "Epoch: 0 | iter: 608 | train loss: 2273.3095703125\n",
            "Epoch: 0 | iter: 609 | train loss: 2854.1464843750\n",
            "Epoch: 0 | iter: 610 | train loss: 3759.2636718750\n",
            "Epoch: 0 | iter: 611 | train loss: 2009.3186035156\n",
            "Epoch: 0 | iter: 612 | train loss: 1614.0908203125\n",
            "Epoch: 0 | iter: 613 | train loss: 3417.4094238281\n",
            "Epoch: 0 | iter: 614 | train loss: 1638.6113281250\n",
            "Epoch: 0 | iter: 615 | train loss: 2773.5849609375\n",
            "Epoch: 0 | iter: 616 | train loss: 1679.5549316406\n",
            "Epoch: 0 | iter: 617 | train loss: 1522.3386230469\n",
            "Epoch: 0 | iter: 618 | train loss: 1105.1645507812\n",
            "Epoch: 0 | iter: 619 | train loss: 1795.1861572266\n",
            "Epoch: 0 | iter: 620 | train loss: 1118.7441406250\n",
            "Epoch: 0 | iter: 621 | train loss: 1396.7022705078\n",
            "Epoch: 0 | iter: 622 | train loss: 1598.4925537109\n",
            "Epoch: 0 | iter: 623 | train loss: 763.5673828125\n",
            "Epoch: 0 | iter: 624 | train loss: 1868.3859863281\n",
            "Epoch: 0 | iter: 625 | train loss: 1180.5555419922\n",
            "Epoch: 0 | iter: 626 | train loss: 2350.4726562500\n",
            "Epoch: 0 | iter: 627 | train loss: 1238.7839355469\n",
            "Epoch: 0 | iter: 628 | train loss: 2079.9746093750\n",
            "Epoch: 0 | iter: 629 | train loss: 2532.1782226562\n",
            "Epoch: 0 | iter: 630 | train loss: 1694.3916015625\n",
            "Epoch: 0 | iter: 631 | train loss: 2464.3081054688\n",
            "Epoch: 0 | iter: 632 | train loss: 1796.0637207031\n",
            "Epoch: 0 | iter: 633 | train loss: 1649.0178222656\n",
            "Epoch: 0 | iter: 634 | train loss: 1577.4376220703\n",
            "Epoch: 0 | iter: 635 | train loss: 1471.4393310547\n",
            "Epoch: 0 | iter: 636 | train loss: 1172.6824951172\n",
            "Epoch: 0 | iter: 637 | train loss: 1619.1433105469\n",
            "Epoch: 0 | iter: 638 | train loss: 2913.9545898438\n",
            "Epoch: 0 | iter: 639 | train loss: 1551.4218750000\n",
            "Epoch: 0 | iter: 640 | train loss: 2045.9125976562\n",
            "Epoch: 0 | iter: 641 | train loss: 1731.0537109375\n",
            "Epoch: 0 | iter: 642 | train loss: 1686.2048339844\n",
            "Epoch: 0 | iter: 643 | train loss: 2068.1047363281\n",
            "Epoch: 0 | iter: 644 | train loss: 1558.2230224609\n",
            "Epoch: 0 | iter: 645 | train loss: 1524.5793457031\n",
            "Epoch: 0 | iter: 646 | train loss: 3051.4379882812\n",
            "Epoch: 0 | iter: 647 | train loss: 1307.3579101562\n",
            "Epoch: 0 | iter: 648 | train loss: 2202.5444335938\n",
            "Epoch: 0 | iter: 649 | train loss: 1654.8754882812\n",
            "Epoch: 0 | iter: 650 | train loss: 2078.4072265625\n",
            "Epoch: 0 | iter: 651 | train loss: 1664.8731689453\n",
            "Epoch: 0 | iter: 652 | train loss: 2321.0515136719\n",
            "Epoch: 0 | iter: 653 | train loss: 2251.0908203125\n",
            "Epoch: 0 | iter: 654 | train loss: 1908.0803222656\n",
            "Epoch: 0 | iter: 655 | train loss: 1601.8873291016\n",
            "Epoch: 0 | iter: 656 | train loss: 1978.0733642578\n",
            "Epoch: 0 | iter: 657 | train loss: 1722.6040039062\n",
            "Epoch: 0 | iter: 658 | train loss: 2727.4318847656\n",
            "Epoch: 0 | iter: 659 | train loss: 1910.8597412109\n",
            "Epoch: 0 | iter: 660 | train loss: 1226.0583496094\n",
            "Epoch: 0 | iter: 661 | train loss: 2125.7260742188\n",
            "Epoch: 0 | iter: 662 | train loss: 1723.5581054688\n",
            "Epoch: 0 | iter: 663 | train loss: 877.6771240234\n",
            "Epoch: 0 | iter: 664 | train loss: 1633.1101074219\n",
            "Epoch: 0 | iter: 665 | train loss: 1426.2705078125\n",
            "Epoch: 0 | iter: 666 | train loss: 1405.3931884766\n",
            "Epoch: 0 | iter: 667 | train loss: 1185.1204833984\n",
            "Epoch: 0 | iter: 668 | train loss: 1356.6364746094\n",
            "Epoch: 0 | iter: 669 | train loss: 1576.2783203125\n",
            "Epoch: 0 | iter: 670 | train loss: 1646.8417968750\n",
            "Epoch: 0 | iter: 671 | train loss: 1439.1281738281\n",
            "Epoch: 0 | iter: 672 | train loss: 1512.2056884766\n",
            "Epoch: 0 | iter: 673 | train loss: 1580.7640380859\n",
            "Epoch: 0 | iter: 674 | train loss: 1183.4117431641\n",
            "Epoch: 0 | iter: 675 | train loss: 1503.9720458984\n",
            "Epoch: 0 | iter: 676 | train loss: 1082.3005371094\n",
            "Epoch: 0 | iter: 677 | train loss: 2494.4020996094\n",
            "Epoch: 0 | iter: 678 | train loss: 921.8073730469\n",
            "Epoch: 0 | iter: 679 | train loss: 1718.0317382812\n",
            "Epoch: 0 | iter: 680 | train loss: 1099.2397460938\n",
            "Epoch: 0 | iter: 681 | train loss: 1351.4571533203\n",
            "Epoch: 0 | iter: 682 | train loss: 1724.6374511719\n",
            "Epoch: 0 | iter: 683 | train loss: 1086.3649902344\n",
            "Epoch: 0 | iter: 684 | train loss: 1836.4796142578\n",
            "Epoch: 0 | iter: 685 | train loss: 1213.4786376953\n",
            "Epoch: 0 | iter: 686 | train loss: 1335.9855957031\n",
            "Epoch: 0 | iter: 687 | train loss: 1327.4895019531\n",
            "Epoch: 0 | iter: 688 | train loss: 1697.5832519531\n",
            "Epoch: 0 | iter: 689 | train loss: 1287.0301513672\n",
            "Epoch: 0 | iter: 690 | train loss: 1010.6518554688\n",
            "Epoch: 0 | iter: 691 | train loss: 1247.9093017578\n",
            "Epoch: 0 | iter: 692 | train loss: 2102.9375000000\n",
            "Epoch: 0 | iter: 693 | train loss: 962.6398925781\n",
            "Epoch: 0 | iter: 694 | train loss: 1390.8259277344\n",
            "Epoch: 0 | iter: 695 | train loss: 1260.4550781250\n",
            "Epoch: 0 | iter: 696 | train loss: 1544.9157714844\n",
            "Epoch: 0 | iter: 697 | train loss: 1381.1436767578\n",
            "Epoch: 0 | iter: 698 | train loss: 995.7794189453\n",
            "Epoch: 0 | iter: 699 | train loss: 1095.4819335938\n",
            "Epoch: 0 | iter: 700 | train loss: 1633.5292968750\n",
            "Epoch: 0 | iter: 701 | train loss: 1590.3675537109\n",
            "Epoch: 0 | iter: 702 | train loss: 1551.0469970703\n",
            "Epoch: 0 | iter: 703 | train loss: 1412.8031005859\n",
            "Epoch: 0 | iter: 704 | train loss: 617.9458618164\n",
            "Epoch: 0 | iter: 705 | train loss: 1240.0909423828\n",
            "Epoch: 0 | iter: 706 | train loss: 1134.3833007812\n",
            "Epoch: 0 | iter: 707 | train loss: 874.2638549805\n",
            "Epoch: 0 | iter: 708 | train loss: 1410.2584228516\n",
            "Epoch: 0 | iter: 709 | train loss: 1863.1130371094\n",
            "Epoch: 0 | iter: 710 | train loss: 1740.5122070312\n",
            "Epoch: 0 | iter: 711 | train loss: 735.8145141602\n",
            "Epoch: 0 | iter: 712 | train loss: 1314.2761230469\n",
            "Epoch: 0 | iter: 713 | train loss: 1170.8398437500\n",
            "Epoch: 0 | iter: 714 | train loss: 716.2331542969\n",
            "Epoch: 0 | iter: 715 | train loss: 1097.0244140625\n",
            "Epoch: 0 | iter: 716 | train loss: 1405.4968261719\n",
            "Epoch: 0 | iter: 717 | train loss: 551.9166259766\n",
            "Epoch: 0 | iter: 718 | train loss: 1071.3271484375\n",
            "Epoch: 0 | iter: 719 | train loss: 1000.1919555664\n",
            "Epoch: 0 | iter: 720 | train loss: 932.4864501953\n",
            "Epoch: 0 | iter: 721 | train loss: 1572.1525878906\n",
            "Epoch: 0 | iter: 722 | train loss: 1791.1801757812\n",
            "Epoch: 0 | iter: 723 | train loss: 1810.5917968750\n",
            "Epoch: 0 | iter: 724 | train loss: 898.7617187500\n",
            "Epoch: 0 | iter: 725 | train loss: 1388.8271484375\n",
            "Epoch: 0 | iter: 726 | train loss: 1229.7836914062\n",
            "Epoch: 0 | iter: 727 | train loss: 1142.0466308594\n",
            "Epoch: 0 | iter: 728 | train loss: 1950.4570312500\n",
            "Epoch: 0 | iter: 729 | train loss: 1457.1641845703\n",
            "Epoch: 0 | iter: 730 | train loss: 871.9090576172\n",
            "Epoch: 0 | iter: 731 | train loss: 1105.5678710938\n",
            "Epoch: 0 | iter: 732 | train loss: 1299.7733154297\n",
            "Epoch: 0 | iter: 733 | train loss: 1278.0891113281\n",
            "Epoch: 0 | iter: 734 | train loss: 709.6212158203\n",
            "Epoch: 0 | iter: 735 | train loss: 1764.6622314453\n",
            "Epoch: 0 | iter: 736 | train loss: 2026.3432617188\n",
            "Epoch: 0 | iter: 737 | train loss: 992.9714355469\n",
            "Epoch: 0 | iter: 738 | train loss: 569.0144042969\n",
            "Epoch: 0 | iter: 739 | train loss: 1314.0499267578\n",
            "Epoch: 0 | iter: 740 | train loss: 1377.0933837891\n",
            "Epoch: 0 | iter: 741 | train loss: 1566.6182861328\n",
            "Epoch: 0 | iter: 742 | train loss: 1202.6862792969\n",
            "Epoch: 0 | iter: 743 | train loss: 1094.5405273438\n",
            "Epoch: 0 | iter: 744 | train loss: 1008.9723510742\n",
            "Epoch: 0 | iter: 745 | train loss: 685.0153198242\n",
            "Epoch: 0 | iter: 746 | train loss: 1140.7392578125\n",
            "Epoch: 0 | iter: 747 | train loss: 1129.3607177734\n",
            "Epoch: 0 | iter: 748 | train loss: 1240.2634277344\n",
            "Epoch: 0 | iter: 749 | train loss: 1611.0332031250\n",
            "Epoch: 0 | iter: 750 | train loss: 1498.8894042969\n",
            "Epoch: 0 | iter: 751 | train loss: 1076.6076660156\n",
            "Epoch: 0 | iter: 752 | train loss: 1078.5444335938\n",
            "Epoch: 0 | iter: 753 | train loss: 1102.7186279297\n",
            "Epoch: 0 | iter: 754 | train loss: 2082.2316894531\n",
            "Epoch: 0 | iter: 755 | train loss: 1169.9465332031\n",
            "Epoch: 0 | iter: 756 | train loss: 1168.2579345703\n",
            "Epoch: 0 | iter: 757 | train loss: 2033.4393310547\n",
            "Epoch: 0 | iter: 758 | train loss: 1765.1125488281\n",
            "Epoch: 0 | iter: 759 | train loss: 727.6919555664\n",
            "Epoch: 0 | iter: 760 | train loss: 834.3070678711\n",
            "Epoch: 0 | iter: 761 | train loss: 812.2227172852\n",
            "Epoch: 0 | iter: 762 | train loss: 736.0621948242\n",
            "Epoch: 0 | iter: 763 | train loss: 1240.1458740234\n",
            "Epoch: 0 | iter: 764 | train loss: 1068.4780273438\n",
            "Epoch: 0 | iter: 765 | train loss: 459.0812988281\n",
            "Epoch: 0 | iter: 766 | train loss: 1549.6436767578\n",
            "Epoch: 0 | iter: 767 | train loss: 678.2964477539\n",
            "Epoch: 0 | iter: 768 | train loss: 645.4664306641\n",
            "Epoch: 0 | iter: 769 | train loss: 1117.9853515625\n",
            "Epoch: 0 | iter: 770 | train loss: 1008.8175048828\n",
            "Epoch: 0 | iter: 771 | train loss: 1444.4570312500\n",
            "Epoch: 0 | iter: 772 | train loss: 1074.5924072266\n",
            "Epoch: 0 | iter: 773 | train loss: 1107.5485839844\n",
            "Epoch: 0 | iter: 774 | train loss: 823.7646484375\n",
            "Epoch: 0 | iter: 775 | train loss: 1216.9448242188\n",
            "Epoch: 0 | iter: 776 | train loss: 1453.6650390625\n",
            "Epoch: 0 | iter: 777 | train loss: 1303.4506835938\n",
            "Epoch: 0 | iter: 778 | train loss: 1033.3735351562\n",
            "Epoch: 0 | iter: 779 | train loss: 1080.6175537109\n",
            "Epoch: 0 | iter: 780 | train loss: 1532.8967285156\n",
            "Epoch: 0 | iter: 781 | train loss: 1132.0728759766\n",
            "Epoch: 0 | iter: 782 | train loss: 805.8375244141\n",
            "Epoch: 0 | iter: 783 | train loss: 1358.3883056641\n",
            "Epoch: 0 | iter: 784 | train loss: 1595.2353515625\n",
            "Epoch: 0 | iter: 785 | train loss: 836.0933837891\n",
            "Epoch: 0 | iter: 786 | train loss: 1901.6690673828\n",
            "Epoch: 0 | iter: 787 | train loss: 1866.0710449219\n",
            "Epoch: 0 | iter: 788 | train loss: 705.9689941406\n",
            "Epoch: 0 | iter: 789 | train loss: 1513.5236816406\n",
            "Epoch: 0 | iter: 790 | train loss: 876.2825927734\n",
            "Epoch: 0 | iter: 791 | train loss: 1443.0523681641\n",
            "Epoch: 0 | iter: 792 | train loss: 819.5499267578\n",
            "Epoch: 0 | iter: 793 | train loss: 896.0640258789\n",
            "Epoch: 0 | iter: 794 | train loss: 933.4263916016\n",
            "Epoch: 0 | iter: 795 | train loss: 1101.4654541016\n",
            "Epoch: 0 | iter: 796 | train loss: 1351.1204833984\n",
            "Epoch: 0 | iter: 797 | train loss: 1030.4958496094\n",
            "Epoch: 0 | iter: 798 | train loss: 1220.3527832031\n",
            "Epoch: 0 | iter: 799 | train loss: 789.2574462891\n",
            "Epoch: 0 | iter: 800 | train loss: 869.0968017578\n",
            "Epoch: 0 | iter: 801 | train loss: 754.0586547852\n",
            "Epoch: 0 | iter: 802 | train loss: 872.5581665039\n",
            "Epoch: 0 | iter: 803 | train loss: 1495.4512939453\n",
            "Epoch: 0 | iter: 804 | train loss: 1478.3842773438\n",
            "Epoch: 0 | iter: 805 | train loss: 1847.7697753906\n",
            "Epoch: 0 | iter: 806 | train loss: 1150.2346191406\n",
            "Epoch: 0 | iter: 807 | train loss: 1507.0655517578\n",
            "Epoch: 0 | iter: 808 | train loss: 1450.1791992188\n",
            "Epoch: 0 | iter: 809 | train loss: 1070.4050292969\n",
            "Epoch: 0 | iter: 810 | train loss: 796.3068847656\n",
            "Epoch: 0 | iter: 811 | train loss: 1250.3093261719\n",
            "Epoch: 0 | iter: 812 | train loss: 1141.2430419922\n",
            "Epoch: 0 | iter: 813 | train loss: 1099.5734863281\n",
            "Epoch: 0 | iter: 814 | train loss: 922.7722167969\n",
            "Epoch: 0 | iter: 815 | train loss: 862.0444335938\n",
            "Epoch: 0 | iter: 816 | train loss: 1383.6979980469\n",
            "Epoch: 0 | iter: 817 | train loss: 449.8597412109\n",
            "Epoch: 0 | iter: 818 | train loss: 1377.5751953125\n",
            "Epoch: 0 | iter: 819 | train loss: 1014.8491821289\n",
            "Epoch: 0 | iter: 820 | train loss: 1207.4484863281\n",
            "Epoch: 0 | iter: 821 | train loss: 1218.0936279297\n",
            "Epoch: 0 | iter: 822 | train loss: 979.5184326172\n",
            "Epoch: 0 | iter: 823 | train loss: 849.3784179688\n",
            "Epoch: 0 | iter: 824 | train loss: 827.5161132812\n",
            "Epoch: 0 | iter: 825 | train loss: 758.1961669922\n",
            "Epoch: 0 | iter: 826 | train loss: 1094.9184570312\n",
            "Epoch: 0 | iter: 827 | train loss: 829.5256958008\n",
            "Epoch: 0 | iter: 828 | train loss: 1457.3236083984\n",
            "Epoch: 0 | iter: 829 | train loss: 593.4760131836\n",
            "Epoch: 0 | iter: 830 | train loss: 542.0275878906\n",
            "Epoch: 0 | iter: 831 | train loss: 946.8149414062\n",
            "Epoch: 0 | iter: 832 | train loss: 724.0017089844\n",
            "Epoch: 0 | iter: 833 | train loss: 731.2102661133\n",
            "Epoch: 0 | iter: 834 | train loss: 845.3975830078\n",
            "Epoch: 0 | iter: 835 | train loss: 1148.1978759766\n",
            "Epoch: 0 | iter: 836 | train loss: 822.4769287109\n",
            "Epoch: 0 | iter: 837 | train loss: 2203.8498535156\n",
            "Epoch: 0 | iter: 838 | train loss: 1201.9139404297\n",
            "Epoch: 0 | iter: 839 | train loss: 1033.3372802734\n",
            "Epoch: 0 | iter: 840 | train loss: 1706.2023925781\n",
            "Epoch: 0 | iter: 841 | train loss: 1473.5153808594\n",
            "Epoch: 0 | iter: 842 | train loss: 1589.2431640625\n",
            "Epoch: 0 | iter: 843 | train loss: 1668.0065917969\n",
            "Epoch: 0 | iter: 844 | train loss: 1328.3609619141\n",
            "Epoch: 0 | iter: 845 | train loss: 1147.7546386719\n",
            "Epoch: 0 | iter: 846 | train loss: 2398.5031738281\n",
            "Epoch: 0 | iter: 847 | train loss: 1173.3892822266\n",
            "Epoch: 0 | iter: 848 | train loss: 1097.7226562500\n",
            "Epoch: 0 | iter: 849 | train loss: 1192.0952148438\n",
            "Epoch: 0 | iter: 850 | train loss: 645.8137207031\n",
            "Epoch: 0 | iter: 851 | train loss: 1444.2053222656\n",
            "Epoch: 0 | iter: 852 | train loss: 951.1212768555\n",
            "Epoch: 0 | iter: 853 | train loss: 1199.6130371094\n",
            "Epoch: 0 | iter: 854 | train loss: 1310.0966796875\n",
            "Epoch: 0 | iter: 855 | train loss: 1344.3531494141\n",
            "Epoch: 0 | iter: 856 | train loss: 853.1695556641\n",
            "Epoch: 0 | iter: 857 | train loss: 917.6204833984\n",
            "Epoch: 0 | iter: 858 | train loss: 1046.6481933594\n",
            "Epoch: 0 | iter: 859 | train loss: 1628.1535644531\n",
            "Epoch: 0 | iter: 860 | train loss: 1281.4508056641\n",
            "Epoch: 0 | iter: 861 | train loss: 1244.8923339844\n",
            "Epoch: 0 | iter: 862 | train loss: 848.1450805664\n",
            "Epoch: 0 | iter: 863 | train loss: 775.1009521484\n",
            "Epoch: 0 | iter: 864 | train loss: 1941.4112548828\n",
            "Epoch: 0 | iter: 865 | train loss: 1043.7634277344\n",
            "Epoch: 0 | iter: 866 | train loss: 1333.1484375000\n",
            "Epoch: 0 | iter: 867 | train loss: 1303.1009521484\n",
            "Epoch: 0 | iter: 868 | train loss: 2143.4211425781\n",
            "Epoch: 0 | iter: 869 | train loss: 1242.2503662109\n",
            "Epoch: 0 | iter: 870 | train loss: 1173.4349365234\n",
            "Epoch: 0 | iter: 871 | train loss: 633.1688842773\n",
            "Epoch: 0 | iter: 872 | train loss: 984.4637451172\n",
            "Epoch: 0 | iter: 873 | train loss: 1046.3033447266\n",
            "Epoch: 0 | iter: 874 | train loss: 1059.1982421875\n",
            "Epoch: 0 | iter: 875 | train loss: 1278.2453613281\n",
            "Epoch: 0 | iter: 876 | train loss: 1331.6514892578\n",
            "Epoch: 0 | iter: 877 | train loss: 1301.7158203125\n",
            "Epoch: 0 | iter: 878 | train loss: 688.7122802734\n",
            "Epoch: 0 | iter: 879 | train loss: 986.5462036133\n",
            "Epoch: 0 | iter: 880 | train loss: 1521.4422607422\n",
            "Epoch: 0 | iter: 881 | train loss: 1262.1989746094\n",
            "Epoch: 0 | iter: 882 | train loss: 973.6342163086\n",
            "Epoch: 0 | iter: 883 | train loss: 1286.5190429688\n",
            "Epoch: 0 | iter: 884 | train loss: 1133.8455810547\n",
            "Epoch: 0 | iter: 885 | train loss: 801.7478637695\n",
            "Epoch: 0 | iter: 886 | train loss: 982.8853149414\n",
            "Epoch: 0 | iter: 887 | train loss: 1210.3775634766\n",
            "Epoch: 0 | iter: 888 | train loss: 1040.2692871094\n",
            "Epoch: 0 | iter: 889 | train loss: 992.7222900391\n",
            "Epoch: 0 | iter: 890 | train loss: 1555.8529052734\n",
            "Epoch: 0 | iter: 891 | train loss: 1574.3608398438\n",
            "Epoch: 0 | iter: 892 | train loss: 1367.0462646484\n",
            "Epoch: 0 | iter: 893 | train loss: 1179.1971435547\n",
            "Epoch: 0 | iter: 894 | train loss: 1130.1325683594\n",
            "Epoch: 0 | iter: 895 | train loss: 1450.6726074219\n",
            "Epoch: 0 | iter: 896 | train loss: 787.0634765625\n",
            "Epoch: 0 | iter: 897 | train loss: 961.8151855469\n",
            "Epoch: 0 | iter: 898 | train loss: 1594.1677246094\n",
            "Epoch: 0 | iter: 899 | train loss: 1472.3137207031\n",
            "Epoch: 0 | iter: 900 | train loss: 1339.5561523438\n",
            "Epoch: 0 | iter: 901 | train loss: 1442.9243164062\n",
            "Epoch: 0 | iter: 902 | train loss: 1412.6634521484\n",
            "Epoch: 0 | iter: 903 | train loss: 1286.4613037109\n",
            "Epoch: 0 | iter: 904 | train loss: 1164.8050537109\n",
            "Epoch: 0 | iter: 905 | train loss: 1960.5697021484\n",
            "Epoch: 0 | iter: 906 | train loss: 1591.1557617188\n",
            "Epoch: 0 | iter: 907 | train loss: 1344.2072753906\n",
            "Epoch: 0 | iter: 908 | train loss: 1309.2319335938\n",
            "Epoch: 0 | iter: 909 | train loss: 1705.2485351562\n",
            "Epoch: 0 | iter: 910 | train loss: 1580.6458740234\n",
            "Epoch: 0 | iter: 911 | train loss: 1964.9008789062\n",
            "Epoch: 0 | iter: 912 | train loss: 1442.5079345703\n",
            "Epoch: 0 | iter: 913 | train loss: 1475.9904785156\n",
            "Epoch: 0 | iter: 914 | train loss: 2335.9968261719\n",
            "Epoch: 0 | iter: 915 | train loss: 1719.1346435547\n",
            "Epoch: 0 | iter: 916 | train loss: 1237.1677246094\n",
            "Epoch: 0 | iter: 917 | train loss: 2239.1066894531\n",
            "Epoch: 0 | iter: 918 | train loss: 1374.3562011719\n",
            "Epoch: 0 | iter: 919 | train loss: 1503.4132080078\n",
            "Epoch: 0 | iter: 920 | train loss: 1754.7106933594\n",
            "Epoch: 0 | iter: 921 | train loss: 2407.8991699219\n",
            "Epoch: 0 | iter: 922 | train loss: 1291.1707763672\n",
            "Epoch: 0 | iter: 923 | train loss: 1138.1286621094\n",
            "Epoch: 0 | iter: 924 | train loss: 2083.6298828125\n",
            "Epoch: 0 | iter: 925 | train loss: 1375.1933593750\n",
            "Epoch: 0 | iter: 926 | train loss: 2554.9248046875\n",
            "Epoch: 0 | iter: 927 | train loss: 2153.7048339844\n",
            "Epoch: 0 | iter: 928 | train loss: 2893.0371093750\n",
            "Epoch: 0 | iter: 929 | train loss: 1500.7370605469\n",
            "Epoch: 0 | iter: 930 | train loss: 1286.7257080078\n",
            "Epoch: 0 | iter: 931 | train loss: 2396.1926269531\n",
            "Epoch: 0 | iter: 932 | train loss: 2064.8278808594\n",
            "Epoch: 0 | iter: 933 | train loss: 1645.2674560547\n",
            "Epoch: 0 | iter: 934 | train loss: 2654.7695312500\n",
            "Epoch: 0 | iter: 935 | train loss: 2742.1850585938\n",
            "Epoch: 0 | iter: 936 | train loss: 1322.4395751953\n",
            "Epoch: 0 | iter: 937 | train loss: 1522.8227539062\n",
            "Epoch: 0 | iter: 938 | train loss: 1623.5385742188\n",
            "Epoch: 0 | iter: 939 | train loss: 2005.8353271484\n",
            "Epoch: 0 | iter: 940 | train loss: 2121.7316894531\n",
            "Epoch: 0 | iter: 941 | train loss: 1279.5080566406\n",
            "Epoch: 0 | iter: 942 | train loss: 1840.6469726562\n",
            "Epoch: 0 | iter: 943 | train loss: 2965.3630371094\n",
            "Epoch: 0 | iter: 944 | train loss: 1377.2214355469\n",
            "Epoch: 0 | iter: 945 | train loss: 1829.9675292969\n",
            "Epoch: 0 | iter: 946 | train loss: 3325.7456054688\n",
            "Epoch: 0 | iter: 947 | train loss: 1519.3984375000\n",
            "Epoch: 0 | iter: 948 | train loss: 2156.5175781250\n",
            "Epoch: 0 | iter: 949 | train loss: 1909.7419433594\n",
            "Epoch: 0 | iter: 950 | train loss: 3207.7661132812\n",
            "Epoch: 0 | iter: 951 | train loss: 1726.3438720703\n",
            "Epoch: 0 | iter: 952 | train loss: 2590.9794921875\n",
            "Epoch: 0 | iter: 953 | train loss: 2408.3449707031\n",
            "Epoch: 0 | iter: 954 | train loss: 2152.6325683594\n",
            "Epoch: 0 | iter: 955 | train loss: 2467.6472167969\n",
            "Epoch: 0 | iter: 956 | train loss: 1876.4205322266\n",
            "Epoch: 0 | iter: 957 | train loss: 1845.7038574219\n",
            "Epoch: 0 | iter: 958 | train loss: 2707.2607421875\n",
            "Epoch: 0 | iter: 959 | train loss: 1996.9636230469\n",
            "Epoch: 0 | iter: 960 | train loss: 1599.2592773438\n",
            "Epoch: 0 | iter: 961 | train loss: 2399.5751953125\n",
            "Epoch: 0 | iter: 962 | train loss: 2016.4188232422\n",
            "Epoch: 0 | iter: 963 | train loss: 2422.8552246094\n",
            "Epoch: 0 | iter: 964 | train loss: 2117.0693359375\n",
            "Epoch: 0 | iter: 965 | train loss: 3180.4916992188\n",
            "Epoch: 0 | iter: 966 | train loss: 1912.4554443359\n",
            "Epoch: 0 | iter: 967 | train loss: 2189.9887695312\n",
            "Epoch: 0 | iter: 968 | train loss: 2277.4567871094\n",
            "Epoch: 0 | iter: 969 | train loss: 1629.3966064453\n",
            "Epoch: 0 | iter: 970 | train loss: 1753.0205078125\n",
            "Epoch: 0 | iter: 971 | train loss: 1032.5578613281\n",
            "Epoch: 0 | iter: 972 | train loss: 3252.4602050781\n",
            "Epoch: 0 | iter: 973 | train loss: 2053.1059570312\n",
            "Epoch: 0 | iter: 974 | train loss: 2164.7817382812\n",
            "Epoch: 0 | iter: 975 | train loss: 1417.6968994141\n",
            "Epoch: 0 | iter: 976 | train loss: 1931.5675048828\n",
            "Epoch: 0 | iter: 977 | train loss: 1696.1256103516\n",
            "Epoch: 0 | iter: 978 | train loss: 2528.0083007812\n",
            "Epoch: 0 | iter: 979 | train loss: 2383.2783203125\n",
            "Epoch: 0 | iter: 980 | train loss: 1662.2790527344\n",
            "Epoch: 0 | iter: 981 | train loss: 2736.8540039062\n",
            "Epoch: 0 | iter: 982 | train loss: 3008.7377929688\n",
            "Epoch: 0 | iter: 983 | train loss: 1543.0587158203\n",
            "Epoch: 0 | iter: 984 | train loss: 2579.1914062500\n",
            "Epoch: 0 | iter: 985 | train loss: 2967.0458984375\n",
            "Epoch: 0 | iter: 986 | train loss: 1229.3737792969\n",
            "Epoch: 0 | iter: 987 | train loss: 2853.6186523438\n",
            "Epoch: 0 | iter: 988 | train loss: 2495.6689453125\n",
            "Epoch: 0 | iter: 989 | train loss: 2294.9428710938\n",
            "Epoch: 0 | iter: 990 | train loss: 2650.1623535156\n",
            "Epoch: 0 | iter: 991 | train loss: 1598.8674316406\n",
            "Epoch: 0 | iter: 992 | train loss: 3448.5136718750\n",
            "Epoch: 0 | iter: 993 | train loss: 1946.5364990234\n",
            "Epoch: 0 | iter: 994 | train loss: 1715.8547363281\n",
            "Epoch: 0 | iter: 995 | train loss: 2390.2924804688\n",
            "Epoch: 0 | iter: 996 | train loss: 1269.4716796875\n",
            "Epoch: 0 | iter: 997 | train loss: 2691.5754394531\n",
            "Epoch: 0 | iter: 998 | train loss: 1910.5959472656\n",
            "Epoch: 0 | iter: 999 | train loss: 1778.9013671875\n",
            "Epoch: 0 | iter: 1000 | train loss: 1368.9731445312\n",
            "Epoch: 0 | iter: 1001 | train loss: 3750.3098144531\n",
            "Epoch: 0 | iter: 1002 | train loss: 2601.5349121094\n",
            "Epoch: 0 | iter: 1003 | train loss: 2337.6293945312\n",
            "Epoch: 0 | iter: 1004 | train loss: 1474.6473388672\n",
            "Epoch: 0 | iter: 1005 | train loss: 1621.5257568359\n",
            "Epoch: 0 | iter: 1006 | train loss: 1960.7169189453\n",
            "Epoch: 0 | iter: 1007 | train loss: 2024.7993164062\n",
            "Epoch: 0 | iter: 1008 | train loss: 3743.6267089844\n",
            "Epoch: 0 | iter: 1009 | train loss: 1991.1621093750\n",
            "Epoch: 0 | iter: 1010 | train loss: 4219.6401367188\n",
            "Epoch: 0 | iter: 1011 | train loss: 3286.5280761719\n",
            "Epoch: 0 | iter: 1012 | train loss: 2174.9924316406\n",
            "Epoch: 0 | iter: 1013 | train loss: 2243.2519531250\n",
            "Epoch: 0 | iter: 1014 | train loss: 1869.2916259766\n",
            "Epoch: 0 | iter: 1015 | train loss: 2167.5791015625\n",
            "Epoch: 0 | iter: 1016 | train loss: 2056.3913574219\n",
            "Epoch: 0 | iter: 1017 | train loss: 4042.6918945312\n",
            "Epoch: 0 | iter: 1018 | train loss: 3086.8542480469\n",
            "Epoch: 0 | iter: 1019 | train loss: 2171.3300781250\n",
            "Epoch: 0 | iter: 1020 | train loss: 2679.1496582031\n",
            "Epoch: 0 | iter: 1021 | train loss: 2541.9836425781\n",
            "Epoch: 0 | iter: 1022 | train loss: 3623.8139648438\n",
            "Epoch: 0 | iter: 1023 | train loss: 2344.3149414062\n",
            "Epoch: 0 | iter: 1024 | train loss: 3092.9643554688\n",
            "Epoch: 0 | iter: 1025 | train loss: 2648.1782226562\n",
            "Epoch: 0 | iter: 1026 | train loss: 3991.7836914062\n",
            "Epoch: 0 | iter: 1027 | train loss: 2851.9997558594\n",
            "Epoch: 0 | iter: 1028 | train loss: 3855.1875000000\n",
            "Epoch: 0 | iter: 1029 | train loss: 3379.9946289062\n",
            "Epoch: 0 | iter: 1030 | train loss: 4068.2041015625\n",
            "Epoch: 0 | iter: 1031 | train loss: 5221.7622070312\n",
            "Epoch: 0 | iter: 1032 | train loss: 3600.4228515625\n",
            "Epoch: 0 | iter: 1033 | train loss: 4004.3029785156\n",
            "Epoch: 0 | iter: 1034 | train loss: 3470.2844238281\n",
            "Epoch: 0 | iter: 1035 | train loss: 5111.7304687500\n",
            "Epoch: 0 | iter: 1036 | train loss: 4330.8144531250\n",
            "Epoch: 0 | iter: 1037 | train loss: 3038.8913574219\n",
            "Epoch: 0 | iter: 1038 | train loss: 3938.9782714844\n",
            "Epoch: 0 | iter: 1039 | train loss: 3173.2143554688\n",
            "Epoch: 0 | iter: 1040 | train loss: 3314.5390625000\n",
            "Epoch: 0 | iter: 1041 | train loss: 3136.7690429688\n",
            "Epoch: 0 | iter: 1042 | train loss: 2896.9414062500\n",
            "Epoch: 0 | iter: 1043 | train loss: 3902.2229003906\n",
            "Epoch: 0 | iter: 1044 | train loss: 3022.8601074219\n",
            "Epoch: 0 | iter: 1045 | train loss: 2962.6931152344\n",
            "Epoch: 0 | iter: 1046 | train loss: 2842.2844238281\n",
            "Epoch: 0 | iter: 1047 | train loss: 2820.5053710938\n",
            "Epoch: 0 | iter: 1048 | train loss: 3242.8950195312\n",
            "Epoch: 0 | iter: 1049 | train loss: 2812.8383789062\n",
            "Epoch: 0 | iter: 1050 | train loss: 3232.3518066406\n",
            "Epoch: 0 | iter: 1051 | train loss: 3598.8227539062\n",
            "Epoch: 0 | iter: 1052 | train loss: 3166.5065917969\n",
            "Epoch: 0 | iter: 1053 | train loss: 2875.0566406250\n",
            "Epoch: 0 | iter: 1054 | train loss: 2701.4819335938\n",
            "Epoch: 0 | iter: 1055 | train loss: 3324.3930664062\n",
            "Epoch: 0 | iter: 1056 | train loss: 2772.4921875000\n",
            "Epoch: 0 | iter: 1057 | train loss: 2877.9453125000\n",
            "Epoch: 0 | iter: 1058 | train loss: 3328.5778808594\n",
            "Epoch: 0 | iter: 1059 | train loss: 2569.0900878906\n",
            "Epoch: 0 | iter: 1060 | train loss: 2797.0180664062\n",
            "Epoch: 0 | iter: 1061 | train loss: 3686.6042480469\n",
            "Epoch: 0 | iter: 1062 | train loss: 2988.7272949219\n",
            "Epoch: 0 | iter: 1063 | train loss: 2633.2041015625\n",
            "Epoch: 0 | iter: 1064 | train loss: 2809.8491210938\n",
            "Epoch: 0 | iter: 1065 | train loss: 2576.3771972656\n",
            "Epoch: 0 | iter: 1066 | train loss: 2892.6184082031\n",
            "Epoch: 0 | iter: 1067 | train loss: 3722.6533203125\n",
            "Epoch: 0 | iter: 1068 | train loss: 3400.8183593750\n",
            "Epoch: 0 | iter: 1069 | train loss: 2947.2082519531\n",
            "Epoch: 0 | iter: 1070 | train loss: 2586.4462890625\n",
            "Epoch: 0 | iter: 1071 | train loss: 3076.3076171875\n",
            "Epoch: 0 | iter: 1072 | train loss: 2746.4443359375\n",
            "Epoch: 0 | iter: 1073 | train loss: 2817.5932617188\n",
            "Epoch: 0 | iter: 1074 | train loss: 2485.4853515625\n",
            "Epoch: 0 | iter: 1075 | train loss: 3037.7221679688\n",
            "Epoch: 0 | iter: 1076 | train loss: 2754.4130859375\n",
            "Epoch: 0 | iter: 1077 | train loss: 3266.3911132812\n",
            "Epoch: 0 | iter: 1078 | train loss: 3230.0014648438\n",
            "Epoch: 0 | iter: 1079 | train loss: 3397.8510742188\n",
            "Epoch: 0 | iter: 1080 | train loss: 2885.9821777344\n",
            "Epoch: 0 | iter: 1081 | train loss: 3854.5488281250\n",
            "Epoch: 0 | iter: 1082 | train loss: 3180.4997558594\n",
            "Epoch: 0 | iter: 1083 | train loss: 2946.8498535156\n",
            "Epoch: 0 | iter: 1084 | train loss: 2486.3164062500\n",
            "Epoch: 0 | iter: 1085 | train loss: 4261.2895507812\n",
            "Epoch: 0 | iter: 1086 | train loss: 2994.2065429688\n",
            "Epoch: 0 | iter: 1087 | train loss: 4301.4384765625\n",
            "Epoch: 0 | iter: 1088 | train loss: 2791.5844726562\n",
            "Epoch: 0 | iter: 1089 | train loss: 2765.9741210938\n",
            "Epoch: 0 | iter: 1090 | train loss: 3170.2338867188\n",
            "Epoch: 0 | iter: 1091 | train loss: 2622.5246582031\n",
            "Epoch: 0 | iter: 1092 | train loss: 3546.3925781250\n",
            "Epoch: 0 | iter: 1093 | train loss: 2609.2966308594\n",
            "Epoch: 0 | iter: 1094 | train loss: 3961.8786621094\n",
            "Epoch: 0 | iter: 1095 | train loss: 3461.8261718750\n",
            "Epoch: 0 | iter: 1096 | train loss: 2541.6918945312\n",
            "Epoch: 0 | iter: 1097 | train loss: 2789.6174316406\n",
            "Epoch: 0 | iter: 1098 | train loss: 3241.7624511719\n",
            "Epoch: 0 | iter: 1099 | train loss: 3503.8283691406\n",
            "Epoch: 0 | iter: 1100 | train loss: 3467.3366699219\n",
            "Epoch: 0 | iter: 1101 | train loss: 2640.1953125000\n",
            "Epoch: 0 | iter: 1102 | train loss: 2830.1022949219\n",
            "Epoch: 0 | iter: 1103 | train loss: 3091.7502441406\n",
            "Epoch: 0 | iter: 1104 | train loss: 2890.1257324219\n",
            "Epoch: 0 | iter: 1105 | train loss: 4596.2207031250\n",
            "Epoch: 0 | iter: 1106 | train loss: 3221.8422851562\n",
            "Epoch: 0 | iter: 1107 | train loss: 2911.5075683594\n",
            "Epoch: 0 | iter: 1108 | train loss: 3315.5700683594\n",
            "Epoch: 0 | iter: 1109 | train loss: 2954.1166992188\n",
            "Epoch: 0 | iter: 1110 | train loss: 2721.8161621094\n",
            "Epoch: 0 | iter: 1111 | train loss: 3322.9343261719\n",
            "Epoch: 0 | iter: 1112 | train loss: 2675.7248535156\n",
            "Epoch: 0 | iter: 1113 | train loss: 3080.5942382812\n",
            "Epoch: 0 | iter: 1114 | train loss: 4588.3583984375\n",
            "Epoch: 0 | iter: 1115 | train loss: 2580.2775878906\n",
            "Epoch: 0 | iter: 1116 | train loss: 3003.9370117188\n",
            "Epoch: 0 | iter: 1117 | train loss: 3492.2062988281\n",
            "Epoch: 0 | iter: 1118 | train loss: 2727.3908691406\n",
            "Epoch: 0 | iter: 1119 | train loss: 3849.1582031250\n",
            "Epoch: 0 | iter: 1120 | train loss: 3238.7944335938\n",
            "Epoch: 0 | iter: 1121 | train loss: 3171.3833007812\n",
            "Epoch: 0 | iter: 1122 | train loss: 3164.0783691406\n",
            "Epoch: 0 | iter: 1123 | train loss: 2986.1201171875\n",
            "Epoch: 0 | iter: 1124 | train loss: 2918.3845214844\n",
            "Epoch: 0 | iter: 1125 | train loss: 3756.0764160156\n",
            "Epoch: 0 | iter: 1126 | train loss: 2992.2707519531\n",
            "Epoch: 0 | iter: 1127 | train loss: 3590.1296386719\n",
            "Epoch: 0 | iter: 1128 | train loss: 2795.0031738281\n",
            "Epoch: 0 | iter: 1129 | train loss: 3760.8750000000\n",
            "Epoch: 0 | iter: 1130 | train loss: 2877.0595703125\n",
            "Epoch: 0 | iter: 1131 | train loss: 3168.7661132812\n",
            "Epoch: 0 | iter: 1132 | train loss: 4262.4599609375\n",
            "Epoch: 0 | iter: 1133 | train loss: 2777.7834472656\n",
            "Epoch: 0 | iter: 1134 | train loss: 2686.3476562500\n",
            "Epoch: 0 | iter: 1135 | train loss: 3071.5686035156\n",
            "Epoch: 0 | iter: 1136 | train loss: 2768.3254394531\n",
            "Epoch: 0 | iter: 1137 | train loss: 3276.6040039062\n",
            "Epoch: 0 | iter: 1138 | train loss: 2451.1525878906\n",
            "Epoch: 0 | iter: 1139 | train loss: 3101.0300292969\n",
            "Epoch: 0 | iter: 1140 | train loss: 2302.6860351562\n",
            "Epoch: 0 | iter: 1141 | train loss: 2732.5146484375\n",
            "Epoch: 0 | iter: 1142 | train loss: 2474.4062500000\n",
            "Epoch: 0 | iter: 1143 | train loss: 2484.7136230469\n",
            "Epoch: 0 | iter: 1144 | train loss: 2720.7182617188\n",
            "Epoch: 0 | iter: 1145 | train loss: 3613.7885742188\n",
            "Epoch: 0 | iter: 1146 | train loss: 3284.2136230469\n",
            "Epoch: 0 | iter: 1147 | train loss: 2820.2988281250\n",
            "Epoch: 0 | iter: 1148 | train loss: 2303.0812988281\n",
            "Epoch: 0 | iter: 1149 | train loss: 2445.3344726562\n",
            "Epoch: 0 | iter: 1150 | train loss: 2486.3276367188\n",
            "Epoch: 0 | iter: 1151 | train loss: 2925.3186035156\n",
            "Epoch: 0 | iter: 1152 | train loss: 2822.0166015625\n",
            "Epoch: 0 | iter: 1153 | train loss: 3197.0629882812\n",
            "Epoch: 0 | iter: 1154 | train loss: 2869.1774902344\n",
            "Epoch: 0 | iter: 1155 | train loss: 3731.7404785156\n",
            "Epoch: 0 | iter: 1156 | train loss: 3524.7009277344\n",
            "Epoch: 0 | iter: 1157 | train loss: 2446.0649414062\n",
            "Epoch: 0 | iter: 1158 | train loss: 2490.0256347656\n",
            "Epoch: 0 | iter: 1159 | train loss: 2912.8171386719\n",
            "Epoch: 0 | iter: 1160 | train loss: 3161.2089843750\n",
            "Epoch: 0 | iter: 1161 | train loss: 3563.2702636719\n",
            "Epoch: 0 | iter: 1162 | train loss: 2786.2314453125\n",
            "Epoch: 0 | iter: 1163 | train loss: 2319.2424316406\n",
            "Epoch: 0 | iter: 1164 | train loss: 2625.5344238281\n",
            "Epoch: 0 | iter: 1165 | train loss: 3970.2607421875\n",
            "Epoch: 0 | iter: 1166 | train loss: 3127.2016601562\n",
            "Epoch: 0 | iter: 1167 | train loss: 2325.4648437500\n",
            "Epoch: 0 | iter: 1168 | train loss: 3474.1567382812\n",
            "Epoch: 0 | iter: 1169 | train loss: 2942.0944824219\n",
            "Epoch: 0 | iter: 1170 | train loss: 3188.5537109375\n",
            "Epoch: 0 | iter: 1171 | train loss: 3189.0063476562\n",
            "Epoch: 0 | iter: 1172 | train loss: 3870.6015625000\n",
            "Epoch: 0 | iter: 1173 | train loss: 3184.0671386719\n",
            "Epoch: 0 | iter: 1174 | train loss: 3355.5820312500\n",
            "Epoch: 0 | iter: 1175 | train loss: 2306.9323730469\n",
            "Epoch: 0 | iter: 1176 | train loss: 3609.8884277344\n",
            "Epoch: 0 | iter: 1177 | train loss: 2557.3647460938\n",
            "Epoch: 0 | iter: 1178 | train loss: 2441.7319335938\n",
            "Epoch: 0 | iter: 1179 | train loss: 2518.4758300781\n",
            "Epoch: 0 | iter: 1180 | train loss: 2930.5683593750\n",
            "Epoch: 0 | iter: 1181 | train loss: 2536.8439941406\n",
            "Epoch: 0 | iter: 1182 | train loss: 2633.8051757812\n",
            "Epoch: 0 | iter: 1183 | train loss: 2599.9020996094\n",
            "Epoch: 0 | iter: 1184 | train loss: 2015.7438964844\n",
            "Epoch: 0 | iter: 1185 | train loss: 3097.9458007812\n",
            "Epoch: 0 | iter: 1186 | train loss: 3352.9943847656\n",
            "Epoch: 0 | iter: 1187 | train loss: 2954.6936035156\n",
            "Epoch: 0 | iter: 1188 | train loss: 2189.7062988281\n",
            "Epoch: 0 | iter: 1189 | train loss: 1913.8431396484\n",
            "Epoch: 0 | iter: 1190 | train loss: 2975.3781738281\n",
            "Epoch: 0 | iter: 1191 | train loss: 3019.4492187500\n",
            "Epoch: 0 | iter: 1192 | train loss: 2980.1713867188\n",
            "Epoch: 0 | iter: 1193 | train loss: 3058.6711425781\n",
            "Epoch: 0 | iter: 1194 | train loss: 2902.6381835938\n",
            "Epoch: 0 | iter: 1195 | train loss: 3582.3784179688\n",
            "Epoch: 0 | iter: 1196 | train loss: 2713.4506835938\n",
            "Epoch: 0 | iter: 1197 | train loss: 2636.6596679688\n",
            "Epoch: 0 | iter: 1198 | train loss: 2221.4902343750\n",
            "Epoch: 0 | iter: 1199 | train loss: 2201.5839843750\n",
            "Epoch: 0 | iter: 1200 | train loss: 2389.3146972656\n",
            "Epoch: 0 | iter: 1201 | train loss: 3778.2561035156\n",
            "Epoch: 0 | iter: 1202 | train loss: 2290.1240234375\n",
            "Epoch: 0 | iter: 1203 | train loss: 3280.9863281250\n",
            "Epoch: 0 | iter: 1204 | train loss: 2927.7526855469\n",
            "Epoch: 0 | iter: 1205 | train loss: 3141.5710449219\n",
            "Epoch: 0 | iter: 1206 | train loss: 2734.5932617188\n",
            "Epoch: 0 | iter: 1207 | train loss: 2120.2192382812\n",
            "Epoch: 0 | iter: 1208 | train loss: 2381.8076171875\n",
            "Epoch: 0 | iter: 1209 | train loss: 2437.7438964844\n",
            "Epoch: 0 | iter: 1210 | train loss: 2043.7998046875\n",
            "Epoch: 0 | iter: 1211 | train loss: 2485.6130371094\n",
            "Epoch: 0 | iter: 1212 | train loss: 2800.7944335938\n",
            "Epoch: 0 | iter: 1213 | train loss: 2439.8361816406\n",
            "Epoch: 0 | iter: 1214 | train loss: 2604.8076171875\n",
            "Epoch: 0 | iter: 1215 | train loss: 2063.8537597656\n",
            "Epoch: 0 | iter: 1216 | train loss: 2107.4487304688\n",
            "Epoch: 0 | iter: 1217 | train loss: 2813.3361816406\n",
            "Epoch: 0 | iter: 1218 | train loss: 3284.8520507812\n",
            "Epoch: 0 | iter: 1219 | train loss: 2633.7573242188\n",
            "Epoch: 0 | iter: 1220 | train loss: 2322.1254882812\n",
            "Epoch: 0 | iter: 1221 | train loss: 2288.0734863281\n",
            "Epoch: 0 | iter: 1222 | train loss: 2516.9401855469\n",
            "Epoch: 0 | iter: 1223 | train loss: 2021.8026123047\n",
            "Epoch: 0 | iter: 1224 | train loss: 2445.0483398438\n",
            "Epoch: 0 | iter: 1225 | train loss: 2640.4995117188\n",
            "Epoch: 0 | iter: 1226 | train loss: 2985.4575195312\n",
            "Epoch: 0 | iter: 1227 | train loss: 2103.2326660156\n",
            "Epoch: 0 | iter: 1228 | train loss: 2320.4897460938\n",
            "Epoch: 0 | iter: 1229 | train loss: 3206.6552734375\n",
            "Epoch: 0 | iter: 1230 | train loss: 2146.0932617188\n",
            "Epoch: 0 | iter: 1231 | train loss: 1858.0745849609\n",
            "Epoch: 0 | iter: 1232 | train loss: 2434.0102539062\n",
            "Epoch: 0 | iter: 1233 | train loss: 2121.1271972656\n",
            "Epoch: 0 | iter: 1234 | train loss: 2067.4194335938\n",
            "Epoch: 0 | iter: 1235 | train loss: 2870.0822753906\n",
            "Epoch: 0 | iter: 1236 | train loss: 1806.1395263672\n",
            "Epoch: 0 | iter: 1237 | train loss: 1508.4367675781\n",
            "Epoch: 0 | iter: 1238 | train loss: 1965.4671630859\n",
            "Epoch: 0 | iter: 1239 | train loss: 1811.0568847656\n",
            "Epoch: 0 | iter: 1240 | train loss: 1876.9372558594\n",
            "Epoch: 0 | iter: 1241 | train loss: 1665.1931152344\n",
            "Epoch: 0 | iter: 1242 | train loss: 3108.0449218750\n",
            "Epoch: 0 | iter: 1243 | train loss: 2825.1484375000\n",
            "Epoch: 0 | iter: 1244 | train loss: 1415.5373535156\n",
            "Epoch: 0 | iter: 1245 | train loss: 1780.7215576172\n",
            "Epoch: 0 | iter: 1246 | train loss: 1078.1362304688\n",
            "Epoch: 0 | iter: 1247 | train loss: 2647.0083007812\n",
            "Epoch: 0 | iter: 1248 | train loss: 2166.3395996094\n",
            "Epoch: 0 | iter: 1249 | train loss: 1763.3580322266\n",
            "Epoch: 0 | iter: 1250 | train loss: 1501.2897949219\n",
            "Epoch: 0 | iter: 1251 | train loss: 2401.0363769531\n",
            "Epoch: 0 | iter: 1252 | train loss: 1772.4509277344\n",
            "Epoch: 0 | iter: 1253 | train loss: 2158.7609863281\n",
            "Epoch: 0 | iter: 1254 | train loss: 2473.9272460938\n",
            "Epoch: 0 | iter: 1255 | train loss: 2136.6201171875\n",
            "Epoch: 0 | iter: 1256 | train loss: 1880.1018066406\n",
            "Epoch: 0 | iter: 1257 | train loss: 2360.7941894531\n",
            "Epoch: 0 | iter: 1258 | train loss: 1779.2974853516\n",
            "Epoch: 0 | iter: 1259 | train loss: 2547.1796875000\n",
            "Epoch: 0 | iter: 1260 | train loss: 2433.9941406250\n",
            "Epoch: 0 | iter: 1261 | train loss: 1304.2790527344\n",
            "Epoch: 0 | iter: 1262 | train loss: 3149.2714843750\n",
            "Epoch: 0 | iter: 1263 | train loss: 1679.1806640625\n",
            "Epoch: 0 | iter: 1264 | train loss: 1357.8719482422\n",
            "Epoch: 0 | iter: 1265 | train loss: 1716.6904296875\n",
            "Epoch: 0 | iter: 1266 | train loss: 1862.5135498047\n",
            "Epoch: 0 | iter: 1267 | train loss: 1751.4562988281\n",
            "Epoch: 0 | iter: 1268 | train loss: 2001.2048339844\n",
            "Epoch: 0 | iter: 1269 | train loss: 2998.1147460938\n",
            "Epoch: 0 | iter: 1270 | train loss: 1429.0682373047\n",
            "Epoch: 0 | iter: 1271 | train loss: 1770.6668701172\n",
            "Epoch: 0 | iter: 1272 | train loss: 1964.9423828125\n",
            "Epoch: 0 | iter: 1273 | train loss: 2410.6562500000\n",
            "Epoch: 0 | iter: 1274 | train loss: 1545.5899658203\n",
            "Epoch: 0 | iter: 1275 | train loss: 1968.8582763672\n",
            "Epoch: 0 | iter: 1276 | train loss: 2513.5769042969\n",
            "Epoch: 0 | iter: 1277 | train loss: 2644.1228027344\n",
            "Epoch: 0 | iter: 1278 | train loss: 2395.5983886719\n",
            "Epoch: 0 | iter: 1279 | train loss: 2933.5007324219\n",
            "Epoch: 0 | iter: 1280 | train loss: 2134.5275878906\n",
            "Epoch: 0 | iter: 1281 | train loss: 2187.7192382812\n",
            "Epoch: 0 | iter: 1282 | train loss: 1701.8242187500\n",
            "Epoch: 0 | iter: 1283 | train loss: 2114.8854980469\n",
            "Epoch: 0 | iter: 1284 | train loss: 1512.7658691406\n",
            "Epoch: 0 | iter: 1285 | train loss: 1447.3280029297\n",
            "Epoch: 0 | iter: 1286 | train loss: 2060.1191406250\n",
            "Epoch: 0 | iter: 1287 | train loss: 2215.2814941406\n",
            "Epoch: 0 | iter: 1288 | train loss: 1515.4943847656\n",
            "Epoch: 0 | iter: 1289 | train loss: 1720.4405517578\n",
            "Epoch: 0 | iter: 1290 | train loss: 1730.5462646484\n",
            "Epoch: 0 | iter: 1291 | train loss: 1643.5717773438\n",
            "Epoch: 0 | iter: 1292 | train loss: 1874.4847412109\n",
            "Epoch: 0 | iter: 1293 | train loss: 3927.1459960938\n",
            "Epoch: 0 | iter: 1294 | train loss: 2773.4743652344\n",
            "Epoch: 0 | iter: 1295 | train loss: 2900.7067871094\n",
            "Epoch: 0 | iter: 1296 | train loss: 2749.8408203125\n",
            "Epoch: 0 | iter: 1297 | train loss: 2791.0273437500\n",
            "Epoch: 0 | iter: 1298 | train loss: 2445.7712402344\n",
            "Epoch: 0 | iter: 1299 | train loss: 2018.7673339844\n",
            "Epoch: 0 | iter: 1300 | train loss: 3221.4228515625\n",
            "Epoch: 0 | iter: 1301 | train loss: 1507.8477783203\n",
            "Epoch: 0 | iter: 1302 | train loss: 1274.9809570312\n",
            "Epoch: 0 | iter: 1303 | train loss: 1126.6180419922\n",
            "Epoch: 0 | iter: 1304 | train loss: 2202.5024414062\n",
            "Epoch: 0 | iter: 1305 | train loss: 1956.5063476562\n",
            "Epoch: 0 | iter: 1306 | train loss: 1907.3122558594\n",
            "Epoch: 0 | iter: 1307 | train loss: 2413.0729980469\n",
            "Epoch: 0 | iter: 1308 | train loss: 1091.0716552734\n",
            "Epoch: 0 | iter: 1309 | train loss: 1922.2946777344\n",
            "Epoch: 0 | iter: 1310 | train loss: 2697.1320800781\n",
            "Epoch: 0 | iter: 1311 | train loss: 2098.3872070312\n",
            "Epoch: 0 | iter: 1312 | train loss: 1788.8204345703\n",
            "Epoch: 0 | iter: 1313 | train loss: 2685.9790039062\n",
            "Epoch: 0 | iter: 1314 | train loss: 1201.2840576172\n",
            "Epoch: 0 | iter: 1315 | train loss: 2662.1220703125\n",
            "Epoch: 0 | iter: 1316 | train loss: 2155.5124511719\n",
            "Epoch: 0 | iter: 1317 | train loss: 1281.5988769531\n",
            "Epoch: 0 | iter: 1318 | train loss: 2421.3227539062\n",
            "Epoch: 0 | iter: 1319 | train loss: 2571.4624023438\n",
            "Epoch: 0 | iter: 1320 | train loss: 2683.3518066406\n",
            "Epoch: 0 | iter: 1321 | train loss: 2107.6196289062\n",
            "Epoch: 0 | iter: 1322 | train loss: 1727.5311279297\n",
            "Epoch: 0 | iter: 1323 | train loss: 1458.0399169922\n",
            "Epoch: 0 | iter: 1324 | train loss: 1912.9687500000\n",
            "Epoch: 0 | iter: 1325 | train loss: 2294.3220214844\n",
            "Epoch: 0 | iter: 1326 | train loss: 2457.8835449219\n",
            "Epoch: 0 | iter: 1327 | train loss: 2078.9614257812\n",
            "Epoch: 0 | iter: 1328 | train loss: 2341.8259277344\n",
            "Epoch: 0 | iter: 1329 | train loss: 2155.4748535156\n",
            "Epoch: 1 | iter: 0 | train loss: 2225.6928710938\n",
            "Epoch: 1 | iter: 1 | train loss: 1942.5661621094\n",
            "Epoch: 1 | iter: 2 | train loss: 2187.5612792969\n",
            "Epoch: 1 | iter: 3 | train loss: 1632.5006103516\n",
            "Epoch: 1 | iter: 4 | train loss: 2344.6291503906\n",
            "Epoch: 1 | iter: 5 | train loss: 1621.8488769531\n",
            "Epoch: 1 | iter: 6 | train loss: 1507.5122070312\n",
            "Epoch: 1 | iter: 7 | train loss: 1725.2519531250\n",
            "Epoch: 1 | iter: 8 | train loss: 1520.9732666016\n",
            "Epoch: 1 | iter: 9 | train loss: 2190.9384765625\n",
            "Epoch: 1 | iter: 10 | train loss: 2034.9052734375\n",
            "Epoch: 1 | iter: 11 | train loss: 1483.9891357422\n",
            "Epoch: 1 | iter: 12 | train loss: 2354.0424804688\n",
            "Epoch: 1 | iter: 13 | train loss: 2156.1286621094\n",
            "Epoch: 1 | iter: 14 | train loss: 1418.7325439453\n",
            "Epoch: 1 | iter: 15 | train loss: 1930.4096679688\n",
            "Epoch: 1 | iter: 16 | train loss: 1485.5278320312\n",
            "Epoch: 1 | iter: 17 | train loss: 958.2155151367\n",
            "Epoch: 1 | iter: 18 | train loss: 2212.3164062500\n",
            "Epoch: 1 | iter: 19 | train loss: 1265.7192382812\n",
            "Epoch: 1 | iter: 20 | train loss: 2781.3547363281\n",
            "Epoch: 1 | iter: 21 | train loss: 1613.3433837891\n",
            "Epoch: 1 | iter: 22 | train loss: 1746.6102294922\n",
            "Epoch: 1 | iter: 23 | train loss: 2301.4499511719\n",
            "Epoch: 1 | iter: 24 | train loss: 2072.4091796875\n",
            "Epoch: 1 | iter: 25 | train loss: 3046.9348144531\n",
            "Epoch: 1 | iter: 26 | train loss: 1471.6090087891\n",
            "Epoch: 1 | iter: 27 | train loss: 1256.8189697266\n",
            "Epoch: 1 | iter: 28 | train loss: 2442.8164062500\n",
            "Epoch: 1 | iter: 29 | train loss: 1823.8391113281\n",
            "Epoch: 1 | iter: 30 | train loss: 3116.9802246094\n",
            "Epoch: 1 | iter: 31 | train loss: 1741.2349853516\n",
            "Epoch: 1 | iter: 32 | train loss: 1503.9124755859\n",
            "Epoch: 1 | iter: 33 | train loss: 2179.7922363281\n",
            "Epoch: 1 | iter: 34 | train loss: 1845.0102539062\n",
            "Epoch: 1 | iter: 35 | train loss: 1536.0211181641\n",
            "Epoch: 1 | iter: 36 | train loss: 1267.9954833984\n",
            "Epoch: 1 | iter: 37 | train loss: 1529.7177734375\n",
            "Epoch: 1 | iter: 38 | train loss: 2287.3564453125\n",
            "Epoch: 1 | iter: 39 | train loss: 2436.3813476562\n",
            "Epoch: 1 | iter: 40 | train loss: 1368.2127685547\n",
            "Epoch: 1 | iter: 41 | train loss: 1858.5810546875\n",
            "Epoch: 1 | iter: 42 | train loss: 1855.9285888672\n",
            "Epoch: 1 | iter: 43 | train loss: 2621.2436523438\n",
            "Epoch: 1 | iter: 44 | train loss: 1484.7694091797\n",
            "Epoch: 1 | iter: 45 | train loss: 1834.1363525391\n",
            "Epoch: 1 | iter: 46 | train loss: 1421.3594970703\n",
            "Epoch: 1 | iter: 47 | train loss: 1322.4614257812\n",
            "Epoch: 1 | iter: 48 | train loss: 2543.4174804688\n",
            "Epoch: 1 | iter: 49 | train loss: 2032.7341308594\n",
            "Epoch: 1 | iter: 50 | train loss: 1433.4671630859\n",
            "Epoch: 1 | iter: 51 | train loss: 3180.8444824219\n",
            "Epoch: 1 | iter: 52 | train loss: 2041.7462158203\n",
            "Epoch: 1 | iter: 53 | train loss: 2205.8208007812\n",
            "Epoch: 1 | iter: 54 | train loss: 2295.7458496094\n",
            "Epoch: 1 | iter: 55 | train loss: 1758.4813232422\n",
            "Epoch: 1 | iter: 56 | train loss: 1605.4230957031\n",
            "Epoch: 1 | iter: 57 | train loss: 1948.1494140625\n",
            "Epoch: 1 | iter: 58 | train loss: 1701.6340332031\n",
            "Epoch: 1 | iter: 59 | train loss: 2411.3862304688\n",
            "Epoch: 1 | iter: 60 | train loss: 1366.2097167969\n",
            "Epoch: 1 | iter: 61 | train loss: 1969.1235351562\n",
            "Epoch: 1 | iter: 62 | train loss: 2263.2678222656\n",
            "Epoch: 1 | iter: 63 | train loss: 2573.9692382812\n",
            "Epoch: 1 | iter: 64 | train loss: 2576.1093750000\n",
            "Epoch: 1 | iter: 65 | train loss: 2512.0068359375\n",
            "Epoch: 1 | iter: 66 | train loss: 2712.6655273438\n",
            "Epoch: 1 | iter: 67 | train loss: 2267.2238769531\n",
            "Epoch: 1 | iter: 68 | train loss: 2059.1293945312\n",
            "Epoch: 1 | iter: 69 | train loss: 1967.9761962891\n",
            "Epoch: 1 | iter: 70 | train loss: 1129.0631103516\n",
            "Epoch: 1 | iter: 71 | train loss: 2439.9262695312\n",
            "Epoch: 1 | iter: 72 | train loss: 2593.5456542969\n",
            "Epoch: 1 | iter: 73 | train loss: 1845.9685058594\n",
            "Epoch: 1 | iter: 74 | train loss: 1342.6307373047\n",
            "Epoch: 1 | iter: 75 | train loss: 1571.1895751953\n",
            "Epoch: 1 | iter: 76 | train loss: 2269.1823730469\n",
            "Epoch: 1 | iter: 77 | train loss: 1031.6608886719\n",
            "Epoch: 1 | iter: 78 | train loss: 2247.3215332031\n",
            "Epoch: 1 | iter: 79 | train loss: 1318.0820312500\n",
            "Epoch: 1 | iter: 80 | train loss: 3289.9455566406\n",
            "Epoch: 1 | iter: 81 | train loss: 1333.5150146484\n",
            "Epoch: 1 | iter: 82 | train loss: 1822.9101562500\n",
            "Epoch: 1 | iter: 83 | train loss: 1270.7651367188\n",
            "Epoch: 1 | iter: 84 | train loss: 1745.5782470703\n",
            "Epoch: 1 | iter: 85 | train loss: 1152.5241699219\n",
            "Epoch: 1 | iter: 86 | train loss: 1866.8791503906\n",
            "Epoch: 1 | iter: 87 | train loss: 2202.2722167969\n",
            "Epoch: 1 | iter: 88 | train loss: 2554.0302734375\n",
            "Epoch: 1 | iter: 89 | train loss: 2357.4599609375\n",
            "Epoch: 1 | iter: 90 | train loss: 1656.1389160156\n",
            "Epoch: 1 | iter: 91 | train loss: 2318.7485351562\n",
            "Epoch: 1 | iter: 92 | train loss: 2449.0729980469\n",
            "Epoch: 1 | iter: 93 | train loss: 2123.6091308594\n",
            "Epoch: 1 | iter: 94 | train loss: 1874.7774658203\n",
            "Epoch: 1 | iter: 95 | train loss: 1182.5748291016\n",
            "Epoch: 1 | iter: 96 | train loss: 1506.0372314453\n",
            "Epoch: 1 | iter: 97 | train loss: 2061.6708984375\n",
            "Epoch: 1 | iter: 98 | train loss: 1354.6180419922\n",
            "Epoch: 1 | iter: 99 | train loss: 2152.2871093750\n",
            "Epoch: 1 | iter: 100 | train loss: 1639.6096191406\n",
            "Epoch: 1 | iter: 101 | train loss: 1194.3249511719\n",
            "Epoch: 1 | iter: 102 | train loss: 1459.2098388672\n",
            "Epoch: 1 | iter: 103 | train loss: 1320.1665039062\n",
            "Epoch: 1 | iter: 104 | train loss: 1326.4754638672\n",
            "Epoch: 1 | iter: 105 | train loss: 1578.1413574219\n",
            "Epoch: 1 | iter: 106 | train loss: 1725.6707763672\n",
            "Epoch: 1 | iter: 107 | train loss: 1820.1040039062\n",
            "Epoch: 1 | iter: 108 | train loss: 2947.6582031250\n",
            "Epoch: 1 | iter: 109 | train loss: 1806.9866943359\n",
            "Epoch: 1 | iter: 110 | train loss: 2524.4108886719\n",
            "Epoch: 1 | iter: 111 | train loss: 986.9239501953\n",
            "Epoch: 1 | iter: 112 | train loss: 1871.9450683594\n",
            "Epoch: 1 | iter: 113 | train loss: 1923.3397216797\n",
            "Epoch: 1 | iter: 114 | train loss: 1191.4633789062\n",
            "Epoch: 1 | iter: 115 | train loss: 1157.8719482422\n",
            "Epoch: 1 | iter: 116 | train loss: 2082.7995605469\n",
            "Epoch: 1 | iter: 117 | train loss: 1100.9866943359\n",
            "Epoch: 1 | iter: 118 | train loss: 1609.2949218750\n",
            "Epoch: 1 | iter: 119 | train loss: 2068.3041992188\n",
            "Epoch: 1 | iter: 120 | train loss: 1371.8466796875\n",
            "Epoch: 1 | iter: 121 | train loss: 2662.6755371094\n",
            "Epoch: 1 | iter: 122 | train loss: 2407.7268066406\n",
            "Epoch: 1 | iter: 123 | train loss: 1809.2915039062\n",
            "Epoch: 1 | iter: 124 | train loss: 1137.3234863281\n",
            "Epoch: 1 | iter: 125 | train loss: 1982.5891113281\n",
            "Epoch: 1 | iter: 126 | train loss: 903.0404052734\n",
            "Epoch: 1 | iter: 127 | train loss: 1685.7584228516\n",
            "Epoch: 1 | iter: 128 | train loss: 2122.4980468750\n",
            "Epoch: 1 | iter: 129 | train loss: 1247.6181640625\n",
            "Epoch: 1 | iter: 130 | train loss: 1653.9023437500\n",
            "Epoch: 1 | iter: 131 | train loss: 1263.8171386719\n",
            "Epoch: 1 | iter: 132 | train loss: 1430.7741699219\n",
            "Epoch: 1 | iter: 133 | train loss: 2456.6708984375\n",
            "Epoch: 1 | iter: 134 | train loss: 2240.8872070312\n",
            "Epoch: 1 | iter: 135 | train loss: 2027.0913085938\n",
            "Epoch: 1 | iter: 136 | train loss: 2067.0053710938\n",
            "Epoch: 1 | iter: 137 | train loss: 1483.7548828125\n",
            "Epoch: 1 | iter: 138 | train loss: 1546.1925048828\n",
            "Epoch: 1 | iter: 139 | train loss: 1482.2387695312\n",
            "Epoch: 1 | iter: 140 | train loss: 1186.3729248047\n",
            "Epoch: 1 | iter: 141 | train loss: 1418.3646240234\n",
            "Epoch: 1 | iter: 142 | train loss: 1807.5646972656\n",
            "Epoch: 1 | iter: 143 | train loss: 1627.4642333984\n",
            "Epoch: 1 | iter: 144 | train loss: 1527.1251220703\n",
            "Epoch: 1 | iter: 145 | train loss: 2398.7636718750\n",
            "Epoch: 1 | iter: 146 | train loss: 1306.4090576172\n",
            "Epoch: 1 | iter: 147 | train loss: 1547.2940673828\n",
            "Epoch: 1 | iter: 148 | train loss: 2283.7062988281\n",
            "Epoch: 1 | iter: 149 | train loss: 1904.4663085938\n",
            "Epoch: 1 | iter: 150 | train loss: 2481.5688476562\n",
            "Epoch: 1 | iter: 151 | train loss: 1604.7252197266\n",
            "Epoch: 1 | iter: 152 | train loss: 1025.5231933594\n",
            "Epoch: 1 | iter: 153 | train loss: 1543.1607666016\n",
            "Epoch: 1 | iter: 154 | train loss: 2299.5541992188\n",
            "Epoch: 1 | iter: 155 | train loss: 1602.6010742188\n",
            "Epoch: 1 | iter: 156 | train loss: 2041.0314941406\n",
            "Epoch: 1 | iter: 157 | train loss: 1288.6961669922\n",
            "Epoch: 1 | iter: 158 | train loss: 1618.2092285156\n",
            "Epoch: 1 | iter: 159 | train loss: 1381.3311767578\n",
            "Epoch: 1 | iter: 160 | train loss: 2092.2307128906\n",
            "Epoch: 1 | iter: 161 | train loss: 1725.1127929688\n",
            "Epoch: 1 | iter: 162 | train loss: 1622.3450927734\n",
            "Epoch: 1 | iter: 163 | train loss: 2337.1069335938\n",
            "Epoch: 1 | iter: 164 | train loss: 914.0878295898\n",
            "Epoch: 1 | iter: 165 | train loss: 2848.3989257812\n",
            "Epoch: 1 | iter: 166 | train loss: 1883.8417968750\n",
            "Epoch: 1 | iter: 167 | train loss: 1426.5384521484\n",
            "Epoch: 1 | iter: 168 | train loss: 1972.9157714844\n",
            "Epoch: 1 | iter: 169 | train loss: 1683.4782714844\n",
            "Epoch: 1 | iter: 170 | train loss: 1950.6105957031\n",
            "Epoch: 1 | iter: 171 | train loss: 1518.2874755859\n",
            "Epoch: 1 | iter: 172 | train loss: 1508.8211669922\n",
            "Epoch: 1 | iter: 173 | train loss: 1913.8941650391\n",
            "Epoch: 1 | iter: 174 | train loss: 1856.0817871094\n",
            "Epoch: 1 | iter: 175 | train loss: 1398.4959716797\n",
            "Epoch: 1 | iter: 176 | train loss: 1468.7440185547\n",
            "Epoch: 1 | iter: 177 | train loss: 1309.5544433594\n",
            "Epoch: 1 | iter: 178 | train loss: 1945.2153320312\n",
            "Epoch: 1 | iter: 179 | train loss: 2771.2507324219\n",
            "Epoch: 1 | iter: 180 | train loss: 1924.4573974609\n",
            "Epoch: 1 | iter: 181 | train loss: 1961.0534667969\n",
            "Epoch: 1 | iter: 182 | train loss: 1710.4906005859\n",
            "Epoch: 1 | iter: 183 | train loss: 1482.1437988281\n",
            "Epoch: 1 | iter: 184 | train loss: 1432.8676757812\n",
            "Epoch: 1 | iter: 185 | train loss: 1409.5429687500\n",
            "Epoch: 1 | iter: 186 | train loss: 2360.8278808594\n",
            "Epoch: 1 | iter: 187 | train loss: 1622.2104492188\n",
            "Epoch: 1 | iter: 188 | train loss: 1294.5308837891\n",
            "Epoch: 1 | iter: 189 | train loss: 2709.0322265625\n",
            "Epoch: 1 | iter: 190 | train loss: 1588.3541259766\n",
            "Epoch: 1 | iter: 191 | train loss: 1364.8497314453\n",
            "Epoch: 1 | iter: 192 | train loss: 1858.6949462891\n",
            "Epoch: 1 | iter: 193 | train loss: 957.2991333008\n",
            "Epoch: 1 | iter: 194 | train loss: 1313.4365234375\n",
            "Epoch: 1 | iter: 195 | train loss: 1644.0665283203\n",
            "Epoch: 1 | iter: 196 | train loss: 1962.1396484375\n",
            "Epoch: 1 | iter: 197 | train loss: 1475.3133544922\n",
            "Epoch: 1 | iter: 198 | train loss: 1156.9896240234\n",
            "Epoch: 1 | iter: 199 | train loss: 1384.5686035156\n",
            "Epoch: 1 | iter: 200 | train loss: 1583.3422851562\n",
            "Epoch: 1 | iter: 201 | train loss: 1366.2585449219\n",
            "Epoch: 1 | iter: 202 | train loss: 2470.5764160156\n",
            "Epoch: 1 | iter: 203 | train loss: 1446.1501464844\n",
            "Epoch: 1 | iter: 204 | train loss: 2311.4460449219\n",
            "Epoch: 1 | iter: 205 | train loss: 1803.6257324219\n",
            "Epoch: 1 | iter: 206 | train loss: 1232.5456542969\n",
            "Epoch: 1 | iter: 207 | train loss: 1036.3996582031\n",
            "Epoch: 1 | iter: 208 | train loss: 1785.6533203125\n",
            "Epoch: 1 | iter: 209 | train loss: 1679.2241210938\n",
            "Epoch: 1 | iter: 210 | train loss: 2053.5034179688\n",
            "Epoch: 1 | iter: 211 | train loss: 1205.9822998047\n",
            "Epoch: 1 | iter: 212 | train loss: 1253.2060546875\n",
            "Epoch: 1 | iter: 213 | train loss: 1676.9277343750\n",
            "Epoch: 1 | iter: 214 | train loss: 1701.1900634766\n",
            "Epoch: 1 | iter: 215 | train loss: 1730.5924072266\n",
            "Epoch: 1 | iter: 216 | train loss: 1785.9903564453\n",
            "Epoch: 1 | iter: 217 | train loss: 1685.4194335938\n",
            "Epoch: 1 | iter: 218 | train loss: 1968.3353271484\n",
            "Epoch: 1 | iter: 219 | train loss: 1345.2834472656\n",
            "Epoch: 1 | iter: 220 | train loss: 1722.4410400391\n",
            "Epoch: 1 | iter: 221 | train loss: 1461.2960205078\n",
            "Epoch: 1 | iter: 222 | train loss: 1650.9150390625\n",
            "Epoch: 1 | iter: 223 | train loss: 1718.1785888672\n",
            "Epoch: 1 | iter: 224 | train loss: 1388.3333740234\n",
            "Epoch: 1 | iter: 225 | train loss: 1632.4667968750\n",
            "Epoch: 1 | iter: 226 | train loss: 940.8360595703\n",
            "Epoch: 1 | iter: 227 | train loss: 1797.9018554688\n",
            "Epoch: 1 | iter: 228 | train loss: 1139.6605224609\n",
            "Epoch: 1 | iter: 229 | train loss: 2569.9626464844\n",
            "Epoch: 1 | iter: 230 | train loss: 1813.8402099609\n",
            "Epoch: 1 | iter: 231 | train loss: 1383.8277587891\n",
            "Epoch: 1 | iter: 232 | train loss: 2369.8300781250\n",
            "Epoch: 1 | iter: 233 | train loss: 1653.3524169922\n",
            "Epoch: 1 | iter: 234 | train loss: 2479.4804687500\n",
            "Epoch: 1 | iter: 235 | train loss: 3047.5966796875\n",
            "Epoch: 1 | iter: 236 | train loss: 2280.3481445312\n",
            "Epoch: 1 | iter: 237 | train loss: 1599.4237060547\n",
            "Epoch: 1 | iter: 238 | train loss: 1773.3745117188\n",
            "Epoch: 1 | iter: 239 | train loss: 1589.7473144531\n",
            "Epoch: 1 | iter: 240 | train loss: 2217.2045898438\n",
            "Epoch: 1 | iter: 241 | train loss: 1930.0793457031\n",
            "Epoch: 1 | iter: 242 | train loss: 2345.4497070312\n",
            "Epoch: 1 | iter: 243 | train loss: 2730.8828125000\n",
            "Epoch: 1 | iter: 244 | train loss: 2196.8771972656\n",
            "Epoch: 1 | iter: 245 | train loss: 2543.6689453125\n",
            "Epoch: 1 | iter: 246 | train loss: 2246.0893554688\n",
            "Epoch: 1 | iter: 247 | train loss: 1853.6516113281\n",
            "Epoch: 1 | iter: 248 | train loss: 2622.5185546875\n",
            "Epoch: 1 | iter: 249 | train loss: 3129.7517089844\n",
            "Epoch: 1 | iter: 250 | train loss: 3057.0705566406\n",
            "Epoch: 1 | iter: 251 | train loss: 3277.6679687500\n",
            "Epoch: 1 | iter: 252 | train loss: 1890.9530029297\n",
            "Epoch: 1 | iter: 253 | train loss: 2248.6076660156\n",
            "Epoch: 1 | iter: 254 | train loss: 2275.1049804688\n",
            "Epoch: 1 | iter: 255 | train loss: 2582.2719726562\n",
            "Epoch: 1 | iter: 256 | train loss: 2057.8557128906\n",
            "Epoch: 1 | iter: 257 | train loss: 2114.3596191406\n",
            "Epoch: 1 | iter: 258 | train loss: 2673.7138671875\n",
            "Epoch: 1 | iter: 259 | train loss: 2113.3681640625\n",
            "Epoch: 1 | iter: 260 | train loss: 3168.5292968750\n",
            "Epoch: 1 | iter: 261 | train loss: 2164.1933593750\n",
            "Epoch: 1 | iter: 262 | train loss: 1784.8176269531\n",
            "Epoch: 1 | iter: 263 | train loss: 1938.6561279297\n",
            "Epoch: 1 | iter: 264 | train loss: 2066.5854492188\n",
            "Epoch: 1 | iter: 265 | train loss: 2693.8747558594\n",
            "Epoch: 1 | iter: 266 | train loss: 3035.0629882812\n",
            "Epoch: 1 | iter: 267 | train loss: 2475.4460449219\n",
            "Epoch: 1 | iter: 268 | train loss: 1935.2526855469\n",
            "Epoch: 1 | iter: 269 | train loss: 2294.2812500000\n",
            "Epoch: 1 | iter: 270 | train loss: 1886.5924072266\n",
            "Epoch: 1 | iter: 271 | train loss: 2953.6330566406\n",
            "Epoch: 1 | iter: 272 | train loss: 1796.3663330078\n",
            "Epoch: 1 | iter: 273 | train loss: 2592.3012695312\n",
            "Epoch: 1 | iter: 274 | train loss: 3046.6555175781\n",
            "Epoch: 1 | iter: 275 | train loss: 2952.9038085938\n",
            "Epoch: 1 | iter: 276 | train loss: 2563.2111816406\n",
            "Epoch: 1 | iter: 277 | train loss: 2998.3149414062\n",
            "Epoch: 1 | iter: 278 | train loss: 2471.7692871094\n",
            "Epoch: 1 | iter: 279 | train loss: 2852.1064453125\n",
            "Epoch: 1 | iter: 280 | train loss: 2103.5285644531\n",
            "Epoch: 1 | iter: 281 | train loss: 2705.9372558594\n",
            "Epoch: 1 | iter: 282 | train loss: 2522.8205566406\n",
            "Epoch: 1 | iter: 283 | train loss: 2424.4245605469\n",
            "Epoch: 1 | iter: 284 | train loss: 1835.8356933594\n",
            "Epoch: 1 | iter: 285 | train loss: 2333.3557128906\n",
            "Epoch: 1 | iter: 286 | train loss: 2664.1545410156\n",
            "Epoch: 1 | iter: 287 | train loss: 2475.4375000000\n",
            "Epoch: 1 | iter: 288 | train loss: 2438.9250488281\n",
            "Epoch: 1 | iter: 289 | train loss: 1983.4388427734\n",
            "Epoch: 1 | iter: 290 | train loss: 2129.0927734375\n",
            "Epoch: 1 | iter: 291 | train loss: 2047.5727539062\n",
            "Epoch: 1 | iter: 292 | train loss: 2457.6601562500\n",
            "Epoch: 1 | iter: 293 | train loss: 2427.6259765625\n",
            "Epoch: 1 | iter: 294 | train loss: 2407.7524414062\n",
            "Epoch: 1 | iter: 295 | train loss: 1957.1912841797\n",
            "Epoch: 1 | iter: 296 | train loss: 2452.1076660156\n",
            "Epoch: 1 | iter: 297 | train loss: 1941.8052978516\n",
            "Epoch: 1 | iter: 298 | train loss: 1793.1363525391\n",
            "Epoch: 1 | iter: 299 | train loss: 2326.5266113281\n",
            "Epoch: 1 | iter: 300 | train loss: 2533.2634277344\n",
            "Epoch: 1 | iter: 301 | train loss: 2480.5405273438\n",
            "Epoch: 1 | iter: 302 | train loss: 2897.8916015625\n",
            "Epoch: 1 | iter: 303 | train loss: 2545.6186523438\n",
            "Epoch: 1 | iter: 304 | train loss: 2245.9184570312\n",
            "Epoch: 1 | iter: 305 | train loss: 2319.6049804688\n",
            "Epoch: 1 | iter: 306 | train loss: 2375.1262207031\n",
            "Epoch: 1 | iter: 307 | train loss: 2962.6381835938\n",
            "Epoch: 1 | iter: 308 | train loss: 1901.7280273438\n",
            "Epoch: 1 | iter: 309 | train loss: 2352.9206542969\n",
            "Epoch: 1 | iter: 310 | train loss: 2729.1984863281\n",
            "Epoch: 1 | iter: 311 | train loss: 1925.7883300781\n",
            "Epoch: 1 | iter: 312 | train loss: 1851.7500000000\n",
            "Epoch: 1 | iter: 313 | train loss: 2326.5747070312\n",
            "Epoch: 1 | iter: 314 | train loss: 1804.6142578125\n",
            "Epoch: 1 | iter: 315 | train loss: 1845.6762695312\n",
            "Epoch: 1 | iter: 316 | train loss: 2174.2438964844\n",
            "Epoch: 1 | iter: 317 | train loss: 2833.3930664062\n",
            "Epoch: 1 | iter: 318 | train loss: 2199.9060058594\n",
            "Epoch: 1 | iter: 319 | train loss: 1448.4935302734\n",
            "Epoch: 1 | iter: 320 | train loss: 2140.7016601562\n",
            "Epoch: 1 | iter: 321 | train loss: 2600.2177734375\n",
            "Epoch: 1 | iter: 322 | train loss: 2409.2436523438\n",
            "Epoch: 1 | iter: 323 | train loss: 2435.8359375000\n",
            "Epoch: 1 | iter: 324 | train loss: 1946.4555664062\n",
            "Epoch: 1 | iter: 325 | train loss: 2036.4182128906\n",
            "Epoch: 1 | iter: 326 | train loss: 2218.6203613281\n",
            "Epoch: 1 | iter: 327 | train loss: 2724.3132324219\n",
            "Epoch: 1 | iter: 328 | train loss: 1633.5874023438\n",
            "Epoch: 1 | iter: 329 | train loss: 2601.8151855469\n",
            "Epoch: 1 | iter: 330 | train loss: 1811.7877197266\n",
            "Epoch: 1 | iter: 331 | train loss: 2089.1364746094\n",
            "Epoch: 1 | iter: 332 | train loss: 2473.4982910156\n",
            "Epoch: 1 | iter: 333 | train loss: 1982.9825439453\n",
            "Epoch: 1 | iter: 334 | train loss: 2712.2514648438\n",
            "Epoch: 1 | iter: 335 | train loss: 1802.1505126953\n",
            "Epoch: 1 | iter: 336 | train loss: 2308.7998046875\n",
            "Epoch: 1 | iter: 337 | train loss: 2281.6894531250\n",
            "Epoch: 1 | iter: 338 | train loss: 2709.4541015625\n",
            "Epoch: 1 | iter: 339 | train loss: 3009.0488281250\n",
            "Epoch: 1 | iter: 340 | train loss: 2551.7099609375\n",
            "Epoch: 1 | iter: 341 | train loss: 2282.4553222656\n",
            "Epoch: 1 | iter: 342 | train loss: 2962.9895019531\n",
            "Epoch: 1 | iter: 343 | train loss: 2213.9016113281\n",
            "Epoch: 1 | iter: 344 | train loss: 2778.5981445312\n",
            "Epoch: 1 | iter: 345 | train loss: 1497.1531982422\n",
            "Epoch: 1 | iter: 346 | train loss: 2131.6513671875\n",
            "Epoch: 1 | iter: 347 | train loss: 1984.7781982422\n",
            "Epoch: 1 | iter: 348 | train loss: 2136.1848144531\n",
            "Epoch: 1 | iter: 349 | train loss: 2538.3415527344\n",
            "Epoch: 1 | iter: 350 | train loss: 2137.5361328125\n",
            "Epoch: 1 | iter: 351 | train loss: 2677.2954101562\n",
            "Epoch: 1 | iter: 352 | train loss: 2706.2661132812\n",
            "Epoch: 1 | iter: 353 | train loss: 1885.3002929688\n",
            "Epoch: 1 | iter: 354 | train loss: 3286.3659667969\n",
            "Epoch: 1 | iter: 355 | train loss: 3033.1577148438\n",
            "Epoch: 1 | iter: 356 | train loss: 2217.5769042969\n",
            "Epoch: 1 | iter: 357 | train loss: 1973.8472900391\n",
            "Epoch: 1 | iter: 358 | train loss: 2203.1694335938\n",
            "Epoch: 1 | iter: 359 | train loss: 2462.3134765625\n",
            "Epoch: 1 | iter: 360 | train loss: 2406.6096191406\n",
            "Epoch: 1 | iter: 361 | train loss: 2026.2416992188\n",
            "Epoch: 1 | iter: 362 | train loss: 2081.1518554688\n",
            "Epoch: 1 | iter: 363 | train loss: 2693.3251953125\n",
            "Epoch: 1 | iter: 364 | train loss: 2082.9667968750\n",
            "Epoch: 1 | iter: 365 | train loss: 2017.6810302734\n",
            "Epoch: 1 | iter: 366 | train loss: 1600.8831787109\n",
            "Epoch: 1 | iter: 367 | train loss: 2432.9714355469\n",
            "Epoch: 1 | iter: 368 | train loss: 1481.0994873047\n",
            "Epoch: 1 | iter: 369 | train loss: 2522.4580078125\n",
            "Epoch: 1 | iter: 370 | train loss: 2539.7214355469\n",
            "Epoch: 1 | iter: 371 | train loss: 1965.8395996094\n",
            "Epoch: 1 | iter: 372 | train loss: 2426.7275390625\n",
            "Epoch: 1 | iter: 373 | train loss: 1970.9217529297\n",
            "Epoch: 1 | iter: 374 | train loss: 1675.1961669922\n",
            "Epoch: 1 | iter: 375 | train loss: 2364.6318359375\n",
            "Epoch: 1 | iter: 376 | train loss: 1767.6497802734\n",
            "Epoch: 1 | iter: 377 | train loss: 1944.4881591797\n",
            "Epoch: 1 | iter: 378 | train loss: 2574.0065917969\n",
            "Epoch: 1 | iter: 379 | train loss: 2153.2309570312\n",
            "Epoch: 1 | iter: 380 | train loss: 3361.0109863281\n",
            "Epoch: 1 | iter: 381 | train loss: 1785.1353759766\n",
            "Epoch: 1 | iter: 382 | train loss: 2381.5388183594\n",
            "Epoch: 1 | iter: 383 | train loss: 3125.4213867188\n",
            "Epoch: 1 | iter: 384 | train loss: 2981.2807617188\n",
            "Epoch: 1 | iter: 385 | train loss: 2303.9692382812\n",
            "Epoch: 1 | iter: 386 | train loss: 2966.6325683594\n",
            "Epoch: 1 | iter: 387 | train loss: 2535.7880859375\n",
            "Epoch: 1 | iter: 388 | train loss: 2625.5454101562\n",
            "Epoch: 1 | iter: 389 | train loss: 2984.5964355469\n",
            "Epoch: 1 | iter: 390 | train loss: 2774.8137207031\n",
            "Epoch: 1 | iter: 391 | train loss: 2633.4755859375\n",
            "Epoch: 1 | iter: 392 | train loss: 2724.9038085938\n",
            "Epoch: 1 | iter: 393 | train loss: 3524.8784179688\n",
            "Epoch: 1 | iter: 394 | train loss: 3281.4035644531\n",
            "Epoch: 1 | iter: 395 | train loss: 3164.6599121094\n",
            "Epoch: 1 | iter: 396 | train loss: 2751.6992187500\n",
            "Epoch: 1 | iter: 397 | train loss: 3368.8813476562\n",
            "Epoch: 1 | iter: 398 | train loss: 3041.2685546875\n",
            "Epoch: 1 | iter: 399 | train loss: 2542.6733398438\n",
            "Epoch: 1 | iter: 400 | train loss: 3469.2561035156\n",
            "Epoch: 1 | iter: 401 | train loss: 2741.7270507812\n",
            "Epoch: 1 | iter: 402 | train loss: 2440.8022460938\n",
            "Epoch: 1 | iter: 403 | train loss: 2358.1550292969\n",
            "Epoch: 1 | iter: 404 | train loss: 2299.3129882812\n",
            "Epoch: 1 | iter: 405 | train loss: 2723.8354492188\n",
            "Epoch: 1 | iter: 406 | train loss: 2404.2429199219\n",
            "Epoch: 1 | iter: 407 | train loss: 2649.6376953125\n",
            "Epoch: 1 | iter: 408 | train loss: 3075.6672363281\n",
            "Epoch: 1 | iter: 409 | train loss: 2985.4782714844\n",
            "Epoch: 1 | iter: 410 | train loss: 2660.0693359375\n",
            "Epoch: 1 | iter: 411 | train loss: 2479.0197753906\n",
            "Epoch: 1 | iter: 412 | train loss: 2602.4799804688\n",
            "Epoch: 1 | iter: 413 | train loss: 2535.6347656250\n",
            "Epoch: 1 | iter: 414 | train loss: 2281.5527343750\n",
            "Epoch: 1 | iter: 415 | train loss: 3121.0749511719\n",
            "Epoch: 1 | iter: 416 | train loss: 2504.5864257812\n",
            "Epoch: 1 | iter: 417 | train loss: 2922.3244628906\n",
            "Epoch: 1 | iter: 418 | train loss: 3531.3686523438\n",
            "Epoch: 1 | iter: 419 | train loss: 3039.1650390625\n",
            "Epoch: 1 | iter: 420 | train loss: 3051.0158691406\n",
            "Epoch: 1 | iter: 421 | train loss: 2705.2053222656\n",
            "Epoch: 1 | iter: 422 | train loss: 3137.8181152344\n",
            "Epoch: 1 | iter: 423 | train loss: 3098.8410644531\n",
            "Epoch: 1 | iter: 424 | train loss: 2640.8378906250\n",
            "Epoch: 1 | iter: 425 | train loss: 2849.9873046875\n",
            "Epoch: 1 | iter: 426 | train loss: 3304.9672851562\n",
            "Epoch: 1 | iter: 427 | train loss: 2737.9453125000\n",
            "Epoch: 1 | iter: 428 | train loss: 2416.3205566406\n",
            "Epoch: 1 | iter: 429 | train loss: 2342.1618652344\n",
            "Epoch: 1 | iter: 430 | train loss: 2464.5642089844\n",
            "Epoch: 1 | iter: 431 | train loss: 2307.0705566406\n",
            "Epoch: 1 | iter: 432 | train loss: 3525.0029296875\n",
            "Epoch: 1 | iter: 433 | train loss: 2410.4282226562\n",
            "Epoch: 1 | iter: 434 | train loss: 2376.6564941406\n",
            "Epoch: 1 | iter: 435 | train loss: 2872.8166503906\n",
            "Epoch: 1 | iter: 436 | train loss: 4156.8076171875\n",
            "Epoch: 1 | iter: 437 | train loss: 2259.6694335938\n",
            "Epoch: 1 | iter: 438 | train loss: 2243.8527832031\n",
            "Epoch: 1 | iter: 439 | train loss: 2370.4929199219\n",
            "Epoch: 1 | iter: 440 | train loss: 2291.3442382812\n",
            "Epoch: 1 | iter: 441 | train loss: 2508.4396972656\n",
            "Epoch: 1 | iter: 442 | train loss: 3145.9606933594\n",
            "Epoch: 1 | iter: 443 | train loss: 1506.6784667969\n",
            "Epoch: 1 | iter: 444 | train loss: 2962.2614746094\n",
            "Epoch: 1 | iter: 445 | train loss: 3633.3642578125\n",
            "Epoch: 1 | iter: 446 | train loss: 2604.6040039062\n",
            "Epoch: 1 | iter: 447 | train loss: 2469.7409667969\n",
            "Epoch: 1 | iter: 448 | train loss: 2866.4460449219\n",
            "Epoch: 1 | iter: 449 | train loss: 1440.6610107422\n",
            "Epoch: 1 | iter: 450 | train loss: 2435.2836914062\n",
            "Epoch: 1 | iter: 451 | train loss: 1159.2811279297\n",
            "Epoch: 1 | iter: 452 | train loss: 1167.8876953125\n",
            "Epoch: 1 | iter: 453 | train loss: 2263.6425781250\n",
            "Epoch: 1 | iter: 454 | train loss: 1252.7807617188\n",
            "Epoch: 1 | iter: 455 | train loss: 1980.7784423828\n",
            "Epoch: 1 | iter: 456 | train loss: 2322.0004882812\n",
            "Epoch: 1 | iter: 457 | train loss: 3120.9877929688\n",
            "Epoch: 1 | iter: 458 | train loss: 2165.2739257812\n",
            "Epoch: 1 | iter: 459 | train loss: 2392.5097656250\n",
            "Epoch: 1 | iter: 460 | train loss: 2240.8530273438\n",
            "Epoch: 1 | iter: 461 | train loss: 2024.8254394531\n",
            "Epoch: 1 | iter: 462 | train loss: 1216.5456542969\n",
            "Epoch: 1 | iter: 463 | train loss: 2510.1391601562\n",
            "Epoch: 1 | iter: 464 | train loss: 3670.3903808594\n",
            "Epoch: 1 | iter: 465 | train loss: 2468.7602539062\n",
            "Epoch: 1 | iter: 466 | train loss: 885.7143554688\n",
            "Epoch: 1 | iter: 467 | train loss: 3621.1879882812\n",
            "Epoch: 1 | iter: 468 | train loss: 1816.2106933594\n",
            "Epoch: 1 | iter: 469 | train loss: 1277.8323974609\n",
            "Epoch: 1 | iter: 470 | train loss: 1282.1373291016\n",
            "Epoch: 1 | iter: 471 | train loss: 3030.1110839844\n",
            "Epoch: 1 | iter: 472 | train loss: 1779.8865966797\n",
            "Epoch: 1 | iter: 473 | train loss: 1395.6761474609\n",
            "Epoch: 1 | iter: 474 | train loss: 2530.3813476562\n",
            "Epoch: 1 | iter: 475 | train loss: 2379.3884277344\n",
            "Epoch: 1 | iter: 476 | train loss: 2045.0361328125\n",
            "Epoch: 1 | iter: 477 | train loss: 1759.4875488281\n",
            "Epoch: 1 | iter: 478 | train loss: 1338.4340820312\n",
            "Epoch: 1 | iter: 479 | train loss: 1846.1397705078\n",
            "Epoch: 1 | iter: 480 | train loss: 2435.7333984375\n",
            "Epoch: 1 | iter: 481 | train loss: 2654.0754394531\n",
            "Epoch: 1 | iter: 482 | train loss: 1882.3225097656\n",
            "Epoch: 1 | iter: 483 | train loss: 1200.8585205078\n",
            "Epoch: 1 | iter: 484 | train loss: 1147.2459716797\n",
            "Epoch: 1 | iter: 485 | train loss: 2015.7512207031\n",
            "Epoch: 1 | iter: 486 | train loss: 1851.1511230469\n",
            "Epoch: 1 | iter: 487 | train loss: 2023.0909423828\n",
            "Epoch: 1 | iter: 488 | train loss: 1944.2653808594\n",
            "Epoch: 1 | iter: 489 | train loss: 1513.0642089844\n",
            "Epoch: 1 | iter: 490 | train loss: 1780.4483642578\n",
            "Epoch: 1 | iter: 491 | train loss: 1197.8751220703\n",
            "Epoch: 1 | iter: 492 | train loss: 1286.7763671875\n",
            "Epoch: 1 | iter: 493 | train loss: 1787.2219238281\n",
            "Epoch: 1 | iter: 494 | train loss: 1322.4373779297\n",
            "Epoch: 1 | iter: 495 | train loss: 2639.2302246094\n",
            "Epoch: 1 | iter: 496 | train loss: 4060.7172851562\n",
            "Epoch: 1 | iter: 497 | train loss: 3192.2399902344\n",
            "Epoch: 1 | iter: 498 | train loss: 2706.9833984375\n",
            "Epoch: 1 | iter: 499 | train loss: 2472.1203613281\n",
            "Epoch: 1 | iter: 500 | train loss: 3515.5769042969\n",
            "Epoch: 1 | iter: 501 | train loss: 2346.1213378906\n",
            "Epoch: 1 | iter: 502 | train loss: 1465.7575683594\n",
            "Epoch: 1 | iter: 503 | train loss: 1405.6474609375\n",
            "Epoch: 1 | iter: 504 | train loss: 2337.2937011719\n",
            "Epoch: 1 | iter: 505 | train loss: 2692.1262207031\n",
            "Epoch: 1 | iter: 506 | train loss: 2508.0893554688\n",
            "Epoch: 1 | iter: 507 | train loss: 2703.8364257812\n",
            "Epoch: 1 | iter: 508 | train loss: 2316.2756347656\n",
            "Epoch: 1 | iter: 509 | train loss: 1498.7314453125\n",
            "Epoch: 1 | iter: 510 | train loss: 4121.6352539062\n",
            "Epoch: 1 | iter: 511 | train loss: 3106.4436035156\n",
            "Epoch: 1 | iter: 512 | train loss: 2485.9602050781\n",
            "Epoch: 1 | iter: 513 | train loss: 2857.4326171875\n",
            "Epoch: 1 | iter: 514 | train loss: 2647.3757324219\n",
            "Epoch: 1 | iter: 515 | train loss: 2369.5415039062\n",
            "Epoch: 1 | iter: 516 | train loss: 2508.8447265625\n",
            "Epoch: 1 | iter: 517 | train loss: 2727.8188476562\n",
            "Epoch: 1 | iter: 518 | train loss: 2843.9301757812\n",
            "Epoch: 1 | iter: 519 | train loss: 1789.2075195312\n",
            "Epoch: 1 | iter: 520 | train loss: 2219.9135742188\n",
            "Epoch: 1 | iter: 521 | train loss: 1930.4003906250\n",
            "Epoch: 1 | iter: 522 | train loss: 3238.2014160156\n",
            "Epoch: 1 | iter: 523 | train loss: 2823.0874023438\n",
            "Epoch: 1 | iter: 524 | train loss: 2679.1413574219\n",
            "Epoch: 1 | iter: 525 | train loss: 3038.0822753906\n",
            "Epoch: 1 | iter: 526 | train loss: 3010.3569335938\n",
            "Epoch: 1 | iter: 527 | train loss: 2573.2573242188\n",
            "Epoch: 1 | iter: 528 | train loss: 2791.8969726562\n",
            "Epoch: 1 | iter: 529 | train loss: 2685.5424804688\n",
            "Epoch: 1 | iter: 530 | train loss: 3115.6220703125\n",
            "Epoch: 1 | iter: 531 | train loss: 2325.7673339844\n",
            "Epoch: 1 | iter: 532 | train loss: 3233.7031250000\n",
            "Epoch: 1 | iter: 533 | train loss: 1574.2480468750\n",
            "Epoch: 1 | iter: 534 | train loss: 4725.3911132812\n",
            "Epoch: 1 | iter: 535 | train loss: 1771.6369628906\n",
            "Epoch: 1 | iter: 536 | train loss: 1740.8394775391\n",
            "Epoch: 1 | iter: 537 | train loss: 1466.5141601562\n",
            "Epoch: 1 | iter: 538 | train loss: 2618.0405273438\n",
            "Epoch: 1 | iter: 539 | train loss: 2699.0148925781\n",
            "Epoch: 1 | iter: 540 | train loss: 2046.0017089844\n",
            "Epoch: 1 | iter: 541 | train loss: 1792.9377441406\n",
            "Epoch: 1 | iter: 542 | train loss: 2714.3164062500\n",
            "Epoch: 1 | iter: 543 | train loss: 1267.0816650391\n",
            "Epoch: 1 | iter: 544 | train loss: 2064.7983398438\n",
            "Epoch: 1 | iter: 545 | train loss: 3006.1574707031\n",
            "Epoch: 1 | iter: 546 | train loss: 1088.9649658203\n",
            "Epoch: 1 | iter: 547 | train loss: 2401.6911621094\n",
            "Epoch: 1 | iter: 548 | train loss: 1955.0454101562\n",
            "Epoch: 1 | iter: 549 | train loss: 1273.5551757812\n",
            "Epoch: 1 | iter: 550 | train loss: 1471.9667968750\n",
            "Epoch: 1 | iter: 551 | train loss: 1814.1837158203\n",
            "Epoch: 1 | iter: 552 | train loss: 2000.8997802734\n",
            "Epoch: 1 | iter: 553 | train loss: 2508.0268554688\n",
            "Epoch: 1 | iter: 554 | train loss: 1061.4659423828\n",
            "Epoch: 1 | iter: 555 | train loss: 2377.4924316406\n",
            "Epoch: 1 | iter: 556 | train loss: 3129.9670410156\n",
            "Epoch: 1 | iter: 557 | train loss: 2365.5375976562\n",
            "Epoch: 1 | iter: 558 | train loss: 1236.9295654297\n",
            "Epoch: 1 | iter: 559 | train loss: 1797.8220214844\n",
            "Epoch: 1 | iter: 560 | train loss: 2580.6982421875\n",
            "Epoch: 1 | iter: 561 | train loss: 2480.5358886719\n",
            "Epoch: 1 | iter: 562 | train loss: 2586.3974609375\n",
            "Epoch: 1 | iter: 563 | train loss: 1527.6850585938\n",
            "Epoch: 1 | iter: 564 | train loss: 2509.8984375000\n",
            "Epoch: 1 | iter: 565 | train loss: 2323.1455078125\n",
            "Epoch: 1 | iter: 566 | train loss: 2897.2392578125\n",
            "Epoch: 1 | iter: 567 | train loss: 2029.4360351562\n",
            "Epoch: 1 | iter: 568 | train loss: 1712.4313964844\n",
            "Epoch: 1 | iter: 569 | train loss: 2212.1750488281\n",
            "Epoch: 1 | iter: 570 | train loss: 2490.1989746094\n",
            "Epoch: 1 | iter: 571 | train loss: 2202.9531250000\n",
            "Epoch: 1 | iter: 572 | train loss: 2715.8657226562\n",
            "Epoch: 1 | iter: 573 | train loss: 3480.2875976562\n",
            "Epoch: 1 | iter: 574 | train loss: 1338.6981201172\n",
            "Epoch: 1 | iter: 575 | train loss: 3085.0681152344\n",
            "Epoch: 1 | iter: 576 | train loss: 2750.4326171875\n",
            "Epoch: 1 | iter: 577 | train loss: 2914.0200195312\n",
            "Epoch: 1 | iter: 578 | train loss: 3032.1064453125\n",
            "Epoch: 1 | iter: 579 | train loss: 2795.6489257812\n",
            "Epoch: 1 | iter: 580 | train loss: 2204.4843750000\n",
            "Epoch: 1 | iter: 581 | train loss: 1582.6604003906\n",
            "Epoch: 1 | iter: 582 | train loss: 3692.6997070312\n",
            "Epoch: 1 | iter: 583 | train loss: 2276.6801757812\n",
            "Epoch: 1 | iter: 584 | train loss: 2711.2158203125\n",
            "Epoch: 1 | iter: 585 | train loss: 2020.5378417969\n",
            "Epoch: 1 | iter: 586 | train loss: 2308.9082031250\n",
            "Epoch: 1 | iter: 587 | train loss: 1201.8040771484\n",
            "Epoch: 1 | iter: 588 | train loss: 4077.7995605469\n",
            "Epoch: 1 | iter: 589 | train loss: 2193.8330078125\n",
            "Epoch: 1 | iter: 590 | train loss: 2940.0773925781\n",
            "Epoch: 1 | iter: 591 | train loss: 2941.8969726562\n",
            "Epoch: 1 | iter: 592 | train loss: 2657.8818359375\n",
            "Epoch: 1 | iter: 593 | train loss: 2368.9331054688\n",
            "Epoch: 1 | iter: 594 | train loss: 3625.2836914062\n",
            "Epoch: 1 | iter: 595 | train loss: 1934.1212158203\n",
            "Epoch: 1 | iter: 596 | train loss: 2022.3879394531\n",
            "Epoch: 1 | iter: 597 | train loss: 1717.3424072266\n",
            "Epoch: 1 | iter: 598 | train loss: 2209.7363281250\n",
            "Epoch: 1 | iter: 599 | train loss: 3379.7287597656\n",
            "Epoch: 1 | iter: 600 | train loss: 4187.3339843750\n",
            "Epoch: 1 | iter: 601 | train loss: 2561.8894042969\n",
            "Epoch: 1 | iter: 602 | train loss: 2309.3664550781\n",
            "Epoch: 1 | iter: 603 | train loss: 2307.6801757812\n",
            "Epoch: 1 | iter: 604 | train loss: 1801.3701171875\n",
            "Epoch: 1 | iter: 605 | train loss: 2967.9289550781\n",
            "Epoch: 1 | iter: 606 | train loss: 1310.8027343750\n",
            "Epoch: 1 | iter: 607 | train loss: 1653.5046386719\n",
            "Epoch: 1 | iter: 608 | train loss: 2244.5183105469\n",
            "Epoch: 1 | iter: 609 | train loss: 3529.6223144531\n",
            "Epoch: 1 | iter: 610 | train loss: 2418.6662597656\n",
            "Epoch: 1 | iter: 611 | train loss: 2701.2070312500\n",
            "Epoch: 1 | iter: 612 | train loss: 2879.4106445312\n",
            "Epoch: 1 | iter: 613 | train loss: 3790.6018066406\n",
            "Epoch: 1 | iter: 614 | train loss: 2341.2246093750\n",
            "Epoch: 1 | iter: 615 | train loss: 3529.2424316406\n",
            "Epoch: 1 | iter: 616 | train loss: 2854.0910644531\n",
            "Epoch: 1 | iter: 617 | train loss: 2357.5139160156\n",
            "Epoch: 1 | iter: 618 | train loss: 2583.7985839844\n",
            "Epoch: 1 | iter: 619 | train loss: 2261.5383300781\n",
            "Epoch: 1 | iter: 620 | train loss: 1706.2864990234\n",
            "Epoch: 1 | iter: 621 | train loss: 1746.3256835938\n",
            "Epoch: 1 | iter: 622 | train loss: 3486.3000488281\n",
            "Epoch: 1 | iter: 623 | train loss: 2025.1066894531\n",
            "Epoch: 1 | iter: 624 | train loss: 3856.4101562500\n",
            "Epoch: 1 | iter: 625 | train loss: 3194.5686035156\n",
            "Epoch: 1 | iter: 626 | train loss: 4968.9653320312\n",
            "Epoch: 1 | iter: 627 | train loss: 3276.9973144531\n",
            "Epoch: 1 | iter: 628 | train loss: 5557.3242187500\n",
            "Epoch: 1 | iter: 629 | train loss: 5391.0791015625\n",
            "Epoch: 1 | iter: 630 | train loss: 4784.3427734375\n",
            "Epoch: 1 | iter: 631 | train loss: 5372.5800781250\n",
            "Epoch: 1 | iter: 632 | train loss: 4937.0117187500\n",
            "Epoch: 1 | iter: 633 | train loss: 4490.0336914062\n",
            "Epoch: 1 | iter: 634 | train loss: 3603.4394531250\n",
            "Epoch: 1 | iter: 635 | train loss: 3555.6083984375\n",
            "Epoch: 1 | iter: 636 | train loss: 3591.8427734375\n",
            "Epoch: 1 | iter: 637 | train loss: 4259.0537109375\n",
            "Epoch: 1 | iter: 638 | train loss: 5734.3427734375\n",
            "Epoch: 1 | iter: 639 | train loss: 3928.5119628906\n",
            "Epoch: 1 | iter: 640 | train loss: 4416.8784179688\n",
            "Epoch: 1 | iter: 641 | train loss: 3279.3383789062\n",
            "Epoch: 1 | iter: 642 | train loss: 3569.3496093750\n",
            "Epoch: 1 | iter: 643 | train loss: 4153.1997070312\n",
            "Epoch: 1 | iter: 644 | train loss: 3234.7448730469\n",
            "Epoch: 1 | iter: 645 | train loss: 3097.3847656250\n",
            "Epoch: 1 | iter: 646 | train loss: 4488.4311523438\n",
            "Epoch: 1 | iter: 647 | train loss: 3049.2084960938\n",
            "Epoch: 1 | iter: 648 | train loss: 3604.4985351562\n",
            "Epoch: 1 | iter: 649 | train loss: 2897.0842285156\n",
            "Epoch: 1 | iter: 650 | train loss: 3374.9780273438\n",
            "Epoch: 1 | iter: 651 | train loss: 3941.6125488281\n",
            "Epoch: 1 | iter: 652 | train loss: 3483.3398437500\n",
            "Epoch: 1 | iter: 653 | train loss: 3415.0622558594\n",
            "Epoch: 1 | iter: 654 | train loss: 3502.7338867188\n",
            "Epoch: 1 | iter: 655 | train loss: 2880.8007812500\n",
            "Epoch: 1 | iter: 656 | train loss: 3458.7487792969\n",
            "Epoch: 1 | iter: 657 | train loss: 3132.9890136719\n",
            "Epoch: 1 | iter: 658 | train loss: 4619.5415039062\n",
            "Epoch: 1 | iter: 659 | train loss: 3396.0651855469\n",
            "Epoch: 1 | iter: 660 | train loss: 3388.5771484375\n",
            "Epoch: 1 | iter: 661 | train loss: 4500.2187500000\n",
            "Epoch: 1 | iter: 662 | train loss: 3333.4340820312\n",
            "Epoch: 1 | iter: 663 | train loss: 2565.1518554688\n",
            "Epoch: 1 | iter: 664 | train loss: 3105.1826171875\n",
            "Epoch: 1 | iter: 665 | train loss: 3519.3793945312\n",
            "Epoch: 1 | iter: 666 | train loss: 3889.3547363281\n",
            "Epoch: 1 | iter: 667 | train loss: 3404.3364257812\n",
            "Epoch: 1 | iter: 668 | train loss: 3568.7387695312\n",
            "Epoch: 1 | iter: 669 | train loss: 3691.2993164062\n",
            "Epoch: 1 | iter: 670 | train loss: 4655.0839843750\n",
            "Epoch: 1 | iter: 671 | train loss: 3632.9599609375\n",
            "Epoch: 1 | iter: 672 | train loss: 2994.7778320312\n",
            "Epoch: 1 | iter: 673 | train loss: 3313.7248535156\n",
            "Epoch: 1 | iter: 674 | train loss: 3055.7709960938\n",
            "Epoch: 1 | iter: 675 | train loss: 3515.3007812500\n",
            "Epoch: 1 | iter: 676 | train loss: 3240.4187011719\n",
            "Epoch: 1 | iter: 677 | train loss: 5226.4731445312\n",
            "Epoch: 1 | iter: 678 | train loss: 2947.9228515625\n",
            "Epoch: 1 | iter: 679 | train loss: 3274.0478515625\n",
            "Epoch: 1 | iter: 680 | train loss: 3567.7309570312\n",
            "Epoch: 1 | iter: 681 | train loss: 3410.1684570312\n",
            "Epoch: 1 | iter: 682 | train loss: 3746.4780273438\n",
            "Epoch: 1 | iter: 683 | train loss: 3461.8427734375\n",
            "Epoch: 1 | iter: 684 | train loss: 3290.2358398438\n",
            "Epoch: 1 | iter: 685 | train loss: 2994.4707031250\n",
            "Epoch: 1 | iter: 686 | train loss: 3280.6892089844\n",
            "Epoch: 1 | iter: 687 | train loss: 3862.0541992188\n",
            "Epoch: 1 | iter: 688 | train loss: 3773.7001953125\n",
            "Epoch: 1 | iter: 689 | train loss: 3982.1923828125\n",
            "Epoch: 1 | iter: 690 | train loss: 2869.9609375000\n",
            "Epoch: 1 | iter: 691 | train loss: 3631.3671875000\n",
            "Epoch: 1 | iter: 692 | train loss: 3888.2629394531\n",
            "Epoch: 1 | iter: 693 | train loss: 3228.5012207031\n",
            "Epoch: 1 | iter: 694 | train loss: 3833.9545898438\n",
            "Epoch: 1 | iter: 695 | train loss: 3521.5375976562\n",
            "Epoch: 1 | iter: 696 | train loss: 3680.2138671875\n",
            "Epoch: 1 | iter: 697 | train loss: 3548.4091796875\n",
            "Epoch: 1 | iter: 698 | train loss: 3438.6201171875\n",
            "Epoch: 1 | iter: 699 | train loss: 5177.7539062500\n",
            "Epoch: 1 | iter: 700 | train loss: 4536.6762695312\n",
            "Epoch: 1 | iter: 701 | train loss: 4072.3115234375\n",
            "Epoch: 1 | iter: 702 | train loss: 4399.9335937500\n",
            "Epoch: 1 | iter: 703 | train loss: 4268.8222656250\n",
            "Epoch: 1 | iter: 704 | train loss: 3599.7490234375\n",
            "Epoch: 1 | iter: 705 | train loss: 4138.1083984375\n",
            "Epoch: 1 | iter: 706 | train loss: 4027.5578613281\n",
            "Epoch: 1 | iter: 707 | train loss: 4647.1455078125\n",
            "Epoch: 1 | iter: 708 | train loss: 4608.7939453125\n",
            "Epoch: 1 | iter: 709 | train loss: 5827.7490234375\n",
            "Epoch: 1 | iter: 710 | train loss: 5032.3417968750\n",
            "Epoch: 1 | iter: 711 | train loss: 4782.6728515625\n",
            "Epoch: 1 | iter: 712 | train loss: 5348.9570312500\n",
            "Epoch: 1 | iter: 713 | train loss: 4867.3994140625\n",
            "Epoch: 1 | iter: 714 | train loss: 4491.5742187500\n",
            "Epoch: 1 | iter: 715 | train loss: 4665.0703125000\n",
            "Epoch: 1 | iter: 716 | train loss: 4657.6337890625\n",
            "Epoch: 1 | iter: 717 | train loss: 4672.0576171875\n",
            "Epoch: 1 | iter: 718 | train loss: 6070.1357421875\n",
            "Epoch: 1 | iter: 719 | train loss: 4385.0766601562\n",
            "Epoch: 1 | iter: 720 | train loss: 4628.2724609375\n",
            "Epoch: 1 | iter: 721 | train loss: 4864.3022460938\n",
            "Epoch: 1 | iter: 722 | train loss: 5109.2343750000\n",
            "Epoch: 1 | iter: 723 | train loss: 5888.2597656250\n",
            "Epoch: 1 | iter: 724 | train loss: 4911.8359375000\n",
            "Epoch: 1 | iter: 725 | train loss: 5064.2338867188\n",
            "Epoch: 1 | iter: 726 | train loss: 4690.8105468750\n",
            "Epoch: 1 | iter: 727 | train loss: 5622.6411132812\n",
            "Epoch: 1 | iter: 728 | train loss: 5070.8378906250\n",
            "Epoch: 1 | iter: 729 | train loss: 5263.9062500000\n",
            "Epoch: 1 | iter: 730 | train loss: 4892.7661132812\n",
            "Epoch: 1 | iter: 731 | train loss: 4942.9277343750\n",
            "Epoch: 1 | iter: 732 | train loss: 4856.2128906250\n",
            "Epoch: 1 | iter: 733 | train loss: 4893.1523437500\n",
            "Epoch: 1 | iter: 734 | train loss: 5049.1821289062\n",
            "Epoch: 1 | iter: 735 | train loss: 5423.3310546875\n",
            "Epoch: 1 | iter: 736 | train loss: 4812.2236328125\n",
            "Epoch: 1 | iter: 737 | train loss: 5102.1591796875\n",
            "Epoch: 1 | iter: 738 | train loss: 5386.5156250000\n",
            "Epoch: 1 | iter: 739 | train loss: 5275.5727539062\n",
            "Epoch: 1 | iter: 740 | train loss: 5318.7680664062\n",
            "Epoch: 1 | iter: 741 | train loss: 5627.4257812500\n",
            "Epoch: 1 | iter: 742 | train loss: 5479.1582031250\n",
            "Epoch: 1 | iter: 743 | train loss: 4944.2524414062\n",
            "Epoch: 1 | iter: 744 | train loss: 4821.3466796875\n",
            "Epoch: 1 | iter: 745 | train loss: 4716.7880859375\n",
            "Epoch: 1 | iter: 746 | train loss: 4936.0683593750\n",
            "Epoch: 1 | iter: 747 | train loss: 5110.7309570312\n",
            "Epoch: 1 | iter: 748 | train loss: 4435.6870117188\n",
            "Epoch: 1 | iter: 749 | train loss: 5214.8315429688\n",
            "Epoch: 1 | iter: 750 | train loss: 4740.2758789062\n",
            "Epoch: 1 | iter: 751 | train loss: 4894.0927734375\n",
            "Epoch: 1 | iter: 752 | train loss: 5076.6416015625\n",
            "Epoch: 1 | iter: 753 | train loss: 4709.3994140625\n",
            "Epoch: 1 | iter: 754 | train loss: 6387.0942382812\n",
            "Epoch: 1 | iter: 755 | train loss: 4983.1259765625\n",
            "Epoch: 1 | iter: 756 | train loss: 4677.8681640625\n",
            "Epoch: 1 | iter: 757 | train loss: 5774.8535156250\n",
            "Epoch: 1 | iter: 758 | train loss: 5607.9843750000\n",
            "Epoch: 1 | iter: 759 | train loss: 5057.2949218750\n",
            "Epoch: 1 | iter: 760 | train loss: 5118.3134765625\n",
            "Epoch: 1 | iter: 761 | train loss: 5174.3139648438\n",
            "Epoch: 1 | iter: 762 | train loss: 5127.0737304688\n",
            "Epoch: 1 | iter: 763 | train loss: 5575.4082031250\n",
            "Epoch: 1 | iter: 764 | train loss: 5198.8818359375\n",
            "Epoch: 1 | iter: 765 | train loss: 5377.8798828125\n",
            "Epoch: 1 | iter: 766 | train loss: 5934.7148437500\n",
            "Epoch: 1 | iter: 767 | train loss: 5558.2133789062\n",
            "Epoch: 1 | iter: 768 | train loss: 5373.0820312500\n",
            "Epoch: 1 | iter: 769 | train loss: 4933.5849609375\n",
            "Epoch: 1 | iter: 770 | train loss: 5013.8945312500\n",
            "Epoch: 1 | iter: 771 | train loss: 4980.6972656250\n",
            "Epoch: 1 | iter: 772 | train loss: 5840.7324218750\n",
            "Epoch: 1 | iter: 773 | train loss: 5434.0849609375\n",
            "Epoch: 1 | iter: 774 | train loss: 5345.8066406250\n",
            "Epoch: 1 | iter: 775 | train loss: 5512.7270507812\n",
            "Epoch: 1 | iter: 776 | train loss: 4918.9521484375\n",
            "Epoch: 1 | iter: 777 | train loss: 5668.5126953125\n",
            "Epoch: 1 | iter: 778 | train loss: 4894.5087890625\n",
            "Epoch: 1 | iter: 779 | train loss: 5150.3798828125\n",
            "Epoch: 1 | iter: 780 | train loss: 4855.7294921875\n",
            "Epoch: 1 | iter: 781 | train loss: 4758.9736328125\n",
            "Epoch: 1 | iter: 782 | train loss: 4972.1083984375\n",
            "Epoch: 1 | iter: 783 | train loss: 4745.4726562500\n",
            "Epoch: 1 | iter: 784 | train loss: 5064.9345703125\n",
            "Epoch: 1 | iter: 785 | train loss: 5624.7343750000\n",
            "Epoch: 1 | iter: 786 | train loss: 5630.8046875000\n",
            "Epoch: 1 | iter: 787 | train loss: 6937.0498046875\n",
            "Epoch: 1 | iter: 788 | train loss: 4994.7836914062\n",
            "Epoch: 1 | iter: 789 | train loss: 5050.2812500000\n",
            "Epoch: 1 | iter: 790 | train loss: 4537.1596679688\n",
            "Epoch: 1 | iter: 791 | train loss: 4962.7553710938\n",
            "Epoch: 1 | iter: 792 | train loss: 4717.8613281250\n",
            "Epoch: 1 | iter: 793 | train loss: 5101.0078125000\n",
            "Epoch: 1 | iter: 794 | train loss: 5161.0390625000\n",
            "Epoch: 1 | iter: 795 | train loss: 4982.1430664062\n",
            "Epoch: 1 | iter: 796 | train loss: 5078.7651367188\n",
            "Epoch: 1 | iter: 797 | train loss: 4534.1259765625\n",
            "Epoch: 1 | iter: 798 | train loss: 5816.5722656250\n",
            "Epoch: 1 | iter: 799 | train loss: 5534.8486328125\n",
            "Epoch: 1 | iter: 800 | train loss: 4926.4570312500\n",
            "Epoch: 1 | iter: 801 | train loss: 5164.6835937500\n",
            "Epoch: 1 | iter: 802 | train loss: 4650.6572265625\n",
            "Epoch: 1 | iter: 803 | train loss: 4618.1098632812\n",
            "Epoch: 1 | iter: 804 | train loss: 5138.1474609375\n",
            "Epoch: 1 | iter: 805 | train loss: 5094.0424804688\n",
            "Epoch: 1 | iter: 806 | train loss: 4707.2978515625\n",
            "Epoch: 1 | iter: 807 | train loss: 4568.9003906250\n",
            "Epoch: 1 | iter: 808 | train loss: 4558.7509765625\n",
            "Epoch: 1 | iter: 809 | train loss: 4641.7666015625\n",
            "Epoch: 1 | iter: 810 | train loss: 5353.1445312500\n",
            "Epoch: 1 | iter: 811 | train loss: 4472.6572265625\n",
            "Epoch: 1 | iter: 812 | train loss: 4548.0429687500\n",
            "Epoch: 1 | iter: 813 | train loss: 4577.0048828125\n",
            "Epoch: 1 | iter: 814 | train loss: 6086.9790039062\n",
            "Epoch: 1 | iter: 815 | train loss: 4777.8530273438\n",
            "Epoch: 1 | iter: 816 | train loss: 5098.9804687500\n",
            "Epoch: 1 | iter: 817 | train loss: 4403.9794921875\n",
            "Epoch: 1 | iter: 818 | train loss: 5545.1611328125\n",
            "Epoch: 1 | iter: 819 | train loss: 4964.8847656250\n",
            "Epoch: 1 | iter: 820 | train loss: 5717.5795898438\n",
            "Epoch: 1 | iter: 821 | train loss: 4615.9843750000\n",
            "Epoch: 1 | iter: 822 | train loss: 6095.5683593750\n",
            "Epoch: 1 | iter: 823 | train loss: 4815.6416015625\n",
            "Epoch: 1 | iter: 824 | train loss: 4348.8876953125\n",
            "Epoch: 1 | iter: 825 | train loss: 5104.6127929688\n",
            "Epoch: 1 | iter: 826 | train loss: 5180.4072265625\n",
            "Epoch: 1 | iter: 827 | train loss: 5610.8984375000\n",
            "Epoch: 1 | iter: 828 | train loss: 4652.3950195312\n",
            "Epoch: 1 | iter: 829 | train loss: 4485.6220703125\n",
            "Epoch: 1 | iter: 830 | train loss: 3885.6108398438\n",
            "Epoch: 1 | iter: 831 | train loss: 3929.6640625000\n",
            "Epoch: 1 | iter: 832 | train loss: 5043.1250000000\n",
            "Epoch: 1 | iter: 833 | train loss: 4384.3671875000\n",
            "Epoch: 1 | iter: 834 | train loss: 5014.7001953125\n",
            "Epoch: 1 | iter: 835 | train loss: 3627.1542968750\n",
            "Epoch: 1 | iter: 836 | train loss: 4553.7875976562\n",
            "Epoch: 1 | iter: 837 | train loss: 6173.7685546875\n",
            "Epoch: 1 | iter: 838 | train loss: 3793.3464355469\n",
            "Epoch: 1 | iter: 839 | train loss: 4801.8300781250\n",
            "Epoch: 1 | iter: 840 | train loss: 3341.4545898438\n",
            "Epoch: 1 | iter: 841 | train loss: 3363.2290039062\n",
            "Epoch: 1 | iter: 842 | train loss: 3515.0659179688\n",
            "Epoch: 1 | iter: 843 | train loss: 3801.5317382812\n",
            "Epoch: 1 | iter: 844 | train loss: 3927.7668457031\n",
            "Epoch: 1 | iter: 845 | train loss: 3314.0451660156\n",
            "Epoch: 1 | iter: 846 | train loss: 3935.1494140625\n",
            "Epoch: 1 | iter: 847 | train loss: 3917.2202148438\n",
            "Epoch: 1 | iter: 848 | train loss: 3084.3525390625\n",
            "Epoch: 1 | iter: 849 | train loss: 3527.3259277344\n",
            "Epoch: 1 | iter: 850 | train loss: 2994.1835937500\n",
            "Epoch: 1 | iter: 851 | train loss: 3636.3188476562\n",
            "Epoch: 1 | iter: 852 | train loss: 2768.7314453125\n",
            "Epoch: 1 | iter: 853 | train loss: 3007.7749023438\n",
            "Epoch: 1 | iter: 854 | train loss: 3672.9726562500\n",
            "Epoch: 1 | iter: 855 | train loss: 3118.4909667969\n",
            "Epoch: 1 | iter: 856 | train loss: 3081.2465820312\n",
            "Epoch: 1 | iter: 857 | train loss: 3014.0458984375\n",
            "Epoch: 1 | iter: 858 | train loss: 3875.6831054688\n",
            "Epoch: 1 | iter: 859 | train loss: 3152.8769531250\n",
            "Epoch: 1 | iter: 860 | train loss: 3130.0935058594\n",
            "Epoch: 1 | iter: 861 | train loss: 2678.3332519531\n",
            "Epoch: 1 | iter: 862 | train loss: 3513.2241210938\n",
            "Epoch: 1 | iter: 863 | train loss: 2268.1132812500\n",
            "Epoch: 1 | iter: 864 | train loss: 3250.9980468750\n",
            "Epoch: 1 | iter: 865 | train loss: 2532.0173339844\n",
            "Epoch: 1 | iter: 866 | train loss: 3229.0888671875\n",
            "Epoch: 1 | iter: 867 | train loss: 3424.3557128906\n",
            "Epoch: 1 | iter: 868 | train loss: 5368.2705078125\n",
            "Epoch: 1 | iter: 869 | train loss: 3466.4863281250\n",
            "Epoch: 1 | iter: 870 | train loss: 2677.2180175781\n",
            "Epoch: 1 | iter: 871 | train loss: 2687.1787109375\n",
            "Epoch: 1 | iter: 872 | train loss: 2839.5859375000\n",
            "Epoch: 1 | iter: 873 | train loss: 3736.4458007812\n",
            "Epoch: 1 | iter: 874 | train loss: 3007.2429199219\n",
            "Epoch: 1 | iter: 875 | train loss: 5716.2216796875\n",
            "Epoch: 1 | iter: 876 | train loss: 3321.9291992188\n",
            "Epoch: 1 | iter: 877 | train loss: 4435.2431640625\n",
            "Epoch: 1 | iter: 878 | train loss: 2564.9741210938\n",
            "Epoch: 1 | iter: 879 | train loss: 2756.7380371094\n",
            "Epoch: 1 | iter: 880 | train loss: 3262.9973144531\n",
            "Epoch: 1 | iter: 881 | train loss: 2638.1372070312\n",
            "Epoch: 1 | iter: 882 | train loss: 3124.1403808594\n",
            "Epoch: 1 | iter: 883 | train loss: 4067.3266601562\n",
            "Epoch: 1 | iter: 884 | train loss: 2868.8583984375\n",
            "Epoch: 1 | iter: 885 | train loss: 2773.3605957031\n",
            "Epoch: 1 | iter: 886 | train loss: 3887.1020507812\n",
            "Epoch: 1 | iter: 887 | train loss: 3233.4157714844\n",
            "Epoch: 1 | iter: 888 | train loss: 3318.5307617188\n",
            "Epoch: 1 | iter: 889 | train loss: 2515.2714843750\n",
            "Epoch: 1 | iter: 890 | train loss: 5202.4428710938\n",
            "Epoch: 1 | iter: 891 | train loss: 3568.7592773438\n",
            "Epoch: 1 | iter: 892 | train loss: 5206.8710937500\n",
            "Epoch: 1 | iter: 893 | train loss: 2957.3930664062\n",
            "Epoch: 1 | iter: 894 | train loss: 3912.1577148438\n",
            "Epoch: 1 | iter: 895 | train loss: 2045.2416992188\n",
            "Epoch: 1 | iter: 896 | train loss: 2775.4233398438\n",
            "Epoch: 1 | iter: 897 | train loss: 3493.7895507812\n",
            "Epoch: 1 | iter: 898 | train loss: 3442.5815429688\n",
            "Epoch: 1 | iter: 899 | train loss: 2521.0112304688\n",
            "Epoch: 1 | iter: 900 | train loss: 3757.7829589844\n",
            "Epoch: 1 | iter: 901 | train loss: 4519.0087890625\n",
            "Epoch: 1 | iter: 902 | train loss: 2572.6313476562\n",
            "Epoch: 1 | iter: 903 | train loss: 3531.9975585938\n",
            "Epoch: 1 | iter: 904 | train loss: 2941.4824218750\n",
            "Epoch: 1 | iter: 905 | train loss: 3842.9753417969\n",
            "Epoch: 1 | iter: 906 | train loss: 3228.1481933594\n",
            "Epoch: 1 | iter: 907 | train loss: 4245.3759765625\n",
            "Epoch: 1 | iter: 908 | train loss: 2393.3317871094\n",
            "Epoch: 1 | iter: 909 | train loss: 2301.0695800781\n",
            "Epoch: 1 | iter: 910 | train loss: 3376.1745605469\n",
            "Epoch: 1 | iter: 911 | train loss: 4930.4350585938\n",
            "Epoch: 1 | iter: 912 | train loss: 2592.9711914062\n",
            "Epoch: 1 | iter: 913 | train loss: 2533.7214355469\n",
            "Epoch: 1 | iter: 914 | train loss: 5331.4233398438\n",
            "Epoch: 1 | iter: 915 | train loss: 3018.4221191406\n",
            "Epoch: 1 | iter: 916 | train loss: 3772.5502929688\n",
            "Epoch: 1 | iter: 917 | train loss: 4603.7539062500\n",
            "Epoch: 1 | iter: 918 | train loss: 2803.4357910156\n",
            "Epoch: 1 | iter: 919 | train loss: 2710.7958984375\n",
            "Epoch: 1 | iter: 920 | train loss: 2851.9365234375\n",
            "Epoch: 1 | iter: 921 | train loss: 3224.6953125000\n",
            "Epoch: 1 | iter: 922 | train loss: 2921.0449218750\n",
            "Epoch: 1 | iter: 923 | train loss: 4270.2031250000\n",
            "Epoch: 1 | iter: 924 | train loss: 3573.2026367188\n",
            "Epoch: 1 | iter: 925 | train loss: 3227.8884277344\n",
            "Epoch: 1 | iter: 926 | train loss: 4962.6547851562\n",
            "Epoch: 1 | iter: 927 | train loss: 4177.2866210938\n",
            "Epoch: 1 | iter: 928 | train loss: 5012.1093750000\n",
            "Epoch: 1 | iter: 929 | train loss: 3307.0942382812\n",
            "Epoch: 1 | iter: 930 | train loss: 2710.7080078125\n",
            "Epoch: 1 | iter: 931 | train loss: 3575.4433593750\n",
            "Epoch: 1 | iter: 932 | train loss: 4248.2280273438\n",
            "Epoch: 1 | iter: 933 | train loss: 3188.9243164062\n",
            "Epoch: 1 | iter: 934 | train loss: 3661.6335449219\n",
            "Epoch: 1 | iter: 935 | train loss: 4256.6425781250\n",
            "Epoch: 1 | iter: 936 | train loss: 2742.7780761719\n",
            "Epoch: 1 | iter: 937 | train loss: 3515.9587402344\n",
            "Epoch: 1 | iter: 938 | train loss: 2102.6970214844\n",
            "Epoch: 1 | iter: 939 | train loss: 5314.3906250000\n",
            "Epoch: 1 | iter: 940 | train loss: 3194.3442382812\n",
            "Epoch: 1 | iter: 941 | train loss: 2567.9233398438\n",
            "Epoch: 1 | iter: 942 | train loss: 3112.5375976562\n",
            "Epoch: 1 | iter: 943 | train loss: 4623.9423828125\n",
            "Epoch: 1 | iter: 944 | train loss: 2202.6894531250\n",
            "Epoch: 1 | iter: 945 | train loss: 2950.2187500000\n",
            "Epoch: 1 | iter: 946 | train loss: 5242.4199218750\n",
            "Epoch: 1 | iter: 947 | train loss: 2678.3515625000\n",
            "Epoch: 1 | iter: 948 | train loss: 3346.5239257812\n",
            "Epoch: 1 | iter: 949 | train loss: 2739.0483398438\n",
            "Epoch: 1 | iter: 950 | train loss: 4237.8969726562\n",
            "Epoch: 1 | iter: 951 | train loss: 3217.1813964844\n",
            "Epoch: 1 | iter: 952 | train loss: 3246.5117187500\n",
            "Epoch: 1 | iter: 953 | train loss: 3105.9250488281\n",
            "Epoch: 1 | iter: 954 | train loss: 3434.3259277344\n",
            "Epoch: 1 | iter: 955 | train loss: 4243.7778320312\n",
            "Epoch: 1 | iter: 956 | train loss: 2618.9189453125\n",
            "Epoch: 1 | iter: 957 | train loss: 2599.0610351562\n",
            "Epoch: 1 | iter: 958 | train loss: 3644.0600585938\n",
            "Epoch: 1 | iter: 959 | train loss: 3288.2355957031\n",
            "Epoch: 1 | iter: 960 | train loss: 2374.2487792969\n",
            "Epoch: 1 | iter: 961 | train loss: 3222.7446289062\n",
            "Epoch: 1 | iter: 962 | train loss: 2470.7692871094\n",
            "Epoch: 1 | iter: 963 | train loss: 3339.8041992188\n",
            "Epoch: 1 | iter: 964 | train loss: 3017.6176757812\n",
            "Epoch: 1 | iter: 965 | train loss: 3797.0488281250\n",
            "Epoch: 1 | iter: 966 | train loss: 4021.3955078125\n",
            "Epoch: 1 | iter: 967 | train loss: 2505.2702636719\n",
            "Epoch: 1 | iter: 968 | train loss: 3449.9819335938\n",
            "Epoch: 1 | iter: 969 | train loss: 2205.5068359375\n",
            "Epoch: 1 | iter: 970 | train loss: 2587.4990234375\n",
            "Epoch: 1 | iter: 971 | train loss: 1833.2717285156\n",
            "Epoch: 1 | iter: 972 | train loss: 3801.4624023438\n",
            "Epoch: 1 | iter: 973 | train loss: 3353.0874023438\n",
            "Epoch: 1 | iter: 974 | train loss: 2837.9291992188\n",
            "Epoch: 1 | iter: 975 | train loss: 2163.1972656250\n",
            "Epoch: 1 | iter: 976 | train loss: 3711.9858398438\n",
            "Epoch: 1 | iter: 977 | train loss: 2728.9062500000\n",
            "Epoch: 1 | iter: 978 | train loss: 3971.1796875000\n",
            "Epoch: 1 | iter: 979 | train loss: 2791.5424804688\n",
            "Epoch: 1 | iter: 980 | train loss: 2386.2163085938\n",
            "Epoch: 1 | iter: 981 | train loss: 3313.2199707031\n",
            "Epoch: 1 | iter: 982 | train loss: 4726.0332031250\n",
            "Epoch: 1 | iter: 983 | train loss: 2622.2265625000\n",
            "Epoch: 1 | iter: 984 | train loss: 3610.1010742188\n",
            "Epoch: 1 | iter: 985 | train loss: 3587.8188476562\n",
            "Epoch: 1 | iter: 986 | train loss: 3012.5131835938\n",
            "Epoch: 1 | iter: 987 | train loss: 5274.7988281250\n",
            "Epoch: 1 | iter: 988 | train loss: 3264.6872558594\n",
            "Epoch: 1 | iter: 989 | train loss: 3699.7099609375\n",
            "Epoch: 1 | iter: 990 | train loss: 3633.0429687500\n",
            "Epoch: 1 | iter: 991 | train loss: 2741.5297851562\n",
            "Epoch: 1 | iter: 992 | train loss: 4793.0224609375\n",
            "Epoch: 1 | iter: 993 | train loss: 3475.3271484375\n",
            "Epoch: 1 | iter: 994 | train loss: 2524.6501464844\n",
            "Epoch: 1 | iter: 995 | train loss: 3304.5664062500\n",
            "Epoch: 1 | iter: 996 | train loss: 2631.2041015625\n",
            "Epoch: 1 | iter: 997 | train loss: 3361.1074218750\n",
            "Epoch: 1 | iter: 998 | train loss: 2792.5373535156\n",
            "Epoch: 1 | iter: 999 | train loss: 2202.7238769531\n",
            "Epoch: 1 | iter: 1000 | train loss: 2282.9929199219\n",
            "Epoch: 1 | iter: 1001 | train loss: 5163.5439453125\n",
            "Epoch: 1 | iter: 1002 | train loss: 3074.6162109375\n",
            "Epoch: 1 | iter: 1003 | train loss: 3404.9992675781\n",
            "Epoch: 1 | iter: 1004 | train loss: 1908.6870117188\n",
            "Epoch: 1 | iter: 1005 | train loss: 2386.3146972656\n",
            "Epoch: 1 | iter: 1006 | train loss: 2820.0117187500\n",
            "Epoch: 1 | iter: 1007 | train loss: 2866.7131347656\n",
            "Epoch: 1 | iter: 1008 | train loss: 4064.7436523438\n",
            "Epoch: 1 | iter: 1009 | train loss: 2690.3388671875\n",
            "Epoch: 1 | iter: 1010 | train loss: 5515.4072265625\n",
            "Epoch: 1 | iter: 1011 | train loss: 3864.6713867188\n",
            "Epoch: 1 | iter: 1012 | train loss: 2321.7788085938\n",
            "Epoch: 1 | iter: 1013 | train loss: 3362.5935058594\n",
            "Epoch: 1 | iter: 1014 | train loss: 2305.0957031250\n",
            "Epoch: 1 | iter: 1015 | train loss: 2656.8933105469\n",
            "Epoch: 1 | iter: 1016 | train loss: 2142.7241210938\n",
            "Epoch: 1 | iter: 1017 | train loss: 3241.0156250000\n",
            "Epoch: 1 | iter: 1018 | train loss: 2638.3972167969\n",
            "Epoch: 1 | iter: 1019 | train loss: 1864.9067382812\n",
            "Epoch: 1 | iter: 1020 | train loss: 3372.3715820312\n",
            "Epoch: 1 | iter: 1021 | train loss: 2704.5390625000\n",
            "Epoch: 1 | iter: 1022 | train loss: 2643.4375000000\n",
            "Epoch: 1 | iter: 1023 | train loss: 1990.4521484375\n",
            "Epoch: 1 | iter: 1024 | train loss: 2001.8747558594\n",
            "Epoch: 1 | iter: 1025 | train loss: 1737.2011718750\n",
            "Epoch: 1 | iter: 1026 | train loss: 2543.1655273438\n",
            "Epoch: 1 | iter: 1027 | train loss: 1676.2229003906\n",
            "Epoch: 1 | iter: 1028 | train loss: 2435.1928710938\n",
            "Epoch: 1 | iter: 1029 | train loss: 2243.6315917969\n",
            "Epoch: 1 | iter: 1030 | train loss: 3989.3276367188\n",
            "Epoch: 1 | iter: 1031 | train loss: 3621.7912597656\n",
            "Epoch: 1 | iter: 1032 | train loss: 2014.4561767578\n",
            "Epoch: 1 | iter: 1033 | train loss: 2884.6083984375\n",
            "Epoch: 1 | iter: 1034 | train loss: 1497.5018310547\n",
            "Epoch: 1 | iter: 1035 | train loss: 3568.1323242188\n",
            "Epoch: 1 | iter: 1036 | train loss: 2665.4992675781\n",
            "Epoch: 1 | iter: 1037 | train loss: 1248.1721191406\n",
            "Epoch: 1 | iter: 1038 | train loss: 2269.4160156250\n",
            "Epoch: 1 | iter: 1039 | train loss: 1886.2912597656\n",
            "Epoch: 1 | iter: 1040 | train loss: 1402.4748535156\n",
            "Epoch: 1 | iter: 1041 | train loss: 1652.4682617188\n",
            "Epoch: 1 | iter: 1042 | train loss: 1744.6409912109\n",
            "Epoch: 1 | iter: 1043 | train loss: 2388.7131347656\n",
            "Epoch: 1 | iter: 1044 | train loss: 2149.4062500000\n",
            "Epoch: 1 | iter: 1045 | train loss: 2062.1723632812\n",
            "Epoch: 1 | iter: 1046 | train loss: 1788.6448974609\n",
            "Epoch: 1 | iter: 1047 | train loss: 1355.2011718750\n",
            "Epoch: 1 | iter: 1048 | train loss: 2284.8720703125\n",
            "Epoch: 1 | iter: 1049 | train loss: 2401.3933105469\n",
            "Epoch: 1 | iter: 1050 | train loss: 3332.8425292969\n",
            "Epoch: 1 | iter: 1051 | train loss: 2755.0026855469\n",
            "Epoch: 1 | iter: 1052 | train loss: 2995.1684570312\n",
            "Epoch: 1 | iter: 1053 | train loss: 2001.1694335938\n",
            "Epoch: 1 | iter: 1054 | train loss: 2296.9370117188\n",
            "Epoch: 1 | iter: 1055 | train loss: 2728.9746093750\n",
            "Epoch: 1 | iter: 1056 | train loss: 2565.1306152344\n",
            "Epoch: 1 | iter: 1057 | train loss: 1918.7956542969\n",
            "Epoch: 1 | iter: 1058 | train loss: 1884.6506347656\n",
            "Epoch: 1 | iter: 1059 | train loss: 2014.3530273438\n",
            "Epoch: 1 | iter: 1060 | train loss: 1517.4311523438\n",
            "Epoch: 1 | iter: 1061 | train loss: 2325.0576171875\n",
            "Epoch: 1 | iter: 1062 | train loss: 2220.1528320312\n",
            "Epoch: 1 | iter: 1063 | train loss: 1983.8918457031\n",
            "Epoch: 1 | iter: 1064 | train loss: 2929.3037109375\n",
            "Epoch: 1 | iter: 1065 | train loss: 2239.4450683594\n",
            "Epoch: 1 | iter: 1066 | train loss: 1599.7705078125\n",
            "Epoch: 1 | iter: 1067 | train loss: 3023.0766601562\n",
            "Epoch: 1 | iter: 1068 | train loss: 2053.6401367188\n",
            "Epoch: 1 | iter: 1069 | train loss: 2977.4645996094\n",
            "Epoch: 1 | iter: 1070 | train loss: 1853.7606201172\n",
            "Epoch: 1 | iter: 1071 | train loss: 2503.3872070312\n",
            "Epoch: 1 | iter: 1072 | train loss: 2277.8105468750\n",
            "Epoch: 1 | iter: 1073 | train loss: 2463.0502929688\n",
            "Epoch: 1 | iter: 1074 | train loss: 1529.4267578125\n",
            "Epoch: 1 | iter: 1075 | train loss: 1878.1625976562\n",
            "Epoch: 1 | iter: 1076 | train loss: 2237.9887695312\n",
            "Epoch: 1 | iter: 1077 | train loss: 3020.4833984375\n",
            "Epoch: 1 | iter: 1078 | train loss: 2637.3076171875\n",
            "Epoch: 1 | iter: 1079 | train loss: 2617.9924316406\n",
            "Epoch: 1 | iter: 1080 | train loss: 3174.8793945312\n",
            "Epoch: 1 | iter: 1081 | train loss: 3651.7519531250\n",
            "Epoch: 1 | iter: 1082 | train loss: 2324.8225097656\n",
            "Epoch: 1 | iter: 1083 | train loss: 2134.4060058594\n",
            "Epoch: 1 | iter: 1084 | train loss: 2102.5317382812\n",
            "Epoch: 1 | iter: 1085 | train loss: 3628.9211425781\n",
            "Epoch: 1 | iter: 1086 | train loss: 2149.2470703125\n",
            "Epoch: 1 | iter: 1087 | train loss: 3985.1166992188\n",
            "Epoch: 1 | iter: 1088 | train loss: 2335.1982421875\n",
            "Epoch: 1 | iter: 1089 | train loss: 3407.4638671875\n",
            "Epoch: 1 | iter: 1090 | train loss: 2335.7617187500\n",
            "Epoch: 1 | iter: 1091 | train loss: 1796.7082519531\n",
            "Epoch: 1 | iter: 1092 | train loss: 2342.7958984375\n",
            "Epoch: 1 | iter: 1093 | train loss: 1920.2968750000\n",
            "Epoch: 1 | iter: 1094 | train loss: 4751.6103515625\n",
            "Epoch: 1 | iter: 1095 | train loss: 2439.9497070312\n",
            "Epoch: 1 | iter: 1096 | train loss: 1893.7214355469\n",
            "Epoch: 1 | iter: 1097 | train loss: 2575.4382324219\n",
            "Epoch: 1 | iter: 1098 | train loss: 2431.2482910156\n",
            "Epoch: 1 | iter: 1099 | train loss: 3722.8002929688\n",
            "Epoch: 1 | iter: 1100 | train loss: 2718.1357421875\n",
            "Epoch: 1 | iter: 1101 | train loss: 1774.0988769531\n",
            "Epoch: 1 | iter: 1102 | train loss: 1355.8520507812\n",
            "Epoch: 1 | iter: 1103 | train loss: 2543.1545410156\n",
            "Epoch: 1 | iter: 1104 | train loss: 1793.3913574219\n",
            "Epoch: 1 | iter: 1105 | train loss: 3787.9638671875\n",
            "Epoch: 1 | iter: 1106 | train loss: 2674.6381835938\n",
            "Epoch: 1 | iter: 1107 | train loss: 2064.9746093750\n",
            "Epoch: 1 | iter: 1108 | train loss: 2073.2875976562\n",
            "Epoch: 1 | iter: 1109 | train loss: 3046.9140625000\n",
            "Epoch: 1 | iter: 1110 | train loss: 2614.9067382812\n",
            "Epoch: 1 | iter: 1111 | train loss: 2440.1733398438\n",
            "Epoch: 1 | iter: 1112 | train loss: 2330.3388671875\n",
            "Epoch: 1 | iter: 1113 | train loss: 2257.7626953125\n",
            "Epoch: 1 | iter: 1114 | train loss: 4049.2084960938\n",
            "Epoch: 1 | iter: 1115 | train loss: 1422.0527343750\n",
            "Epoch: 1 | iter: 1116 | train loss: 1738.9589843750\n",
            "Epoch: 1 | iter: 1117 | train loss: 2281.6743164062\n",
            "Epoch: 1 | iter: 1118 | train loss: 1558.4995117188\n",
            "Epoch: 1 | iter: 1119 | train loss: 3401.3212890625\n",
            "Epoch: 1 | iter: 1120 | train loss: 2173.9790039062\n",
            "Epoch: 1 | iter: 1121 | train loss: 2430.6062011719\n",
            "Epoch: 1 | iter: 1122 | train loss: 2276.3359375000\n",
            "Epoch: 1 | iter: 1123 | train loss: 2463.9006347656\n",
            "Epoch: 1 | iter: 1124 | train loss: 2547.9492187500\n",
            "Epoch: 1 | iter: 1125 | train loss: 2794.2128906250\n",
            "Epoch: 1 | iter: 1126 | train loss: 1923.8212890625\n",
            "Epoch: 1 | iter: 1127 | train loss: 2659.0351562500\n",
            "Epoch: 1 | iter: 1128 | train loss: 2309.3254394531\n",
            "Epoch: 1 | iter: 1129 | train loss: 3244.0517578125\n",
            "Epoch: 1 | iter: 1130 | train loss: 2386.4892578125\n",
            "Epoch: 1 | iter: 1131 | train loss: 2444.4711914062\n",
            "Epoch: 1 | iter: 1132 | train loss: 3129.0927734375\n",
            "Epoch: 1 | iter: 1133 | train loss: 2292.6354980469\n",
            "Epoch: 1 | iter: 1134 | train loss: 1570.9208984375\n",
            "Epoch: 1 | iter: 1135 | train loss: 2730.4929199219\n",
            "Epoch: 1 | iter: 1136 | train loss: 2538.7353515625\n",
            "Epoch: 1 | iter: 1137 | train loss: 2798.8098144531\n",
            "Epoch: 1 | iter: 1138 | train loss: 1933.6516113281\n",
            "Epoch: 1 | iter: 1139 | train loss: 2785.5268554688\n",
            "Epoch: 1 | iter: 1140 | train loss: 2472.8144531250\n",
            "Epoch: 1 | iter: 1141 | train loss: 2403.6191406250\n",
            "Epoch: 1 | iter: 1142 | train loss: 1680.4934082031\n",
            "Epoch: 1 | iter: 1143 | train loss: 2221.6943359375\n",
            "Epoch: 1 | iter: 1144 | train loss: 2553.1044921875\n",
            "Epoch: 1 | iter: 1145 | train loss: 3666.4052734375\n",
            "Epoch: 1 | iter: 1146 | train loss: 2819.0668945312\n",
            "Epoch: 1 | iter: 1147 | train loss: 1654.1214599609\n",
            "Epoch: 1 | iter: 1148 | train loss: 2174.3984375000\n",
            "Epoch: 1 | iter: 1149 | train loss: 2503.0417480469\n",
            "Epoch: 1 | iter: 1150 | train loss: 2562.6867675781\n",
            "Epoch: 1 | iter: 1151 | train loss: 2895.5620117188\n",
            "Epoch: 1 | iter: 1152 | train loss: 2031.9763183594\n",
            "Epoch: 1 | iter: 1153 | train loss: 3100.3759765625\n",
            "Epoch: 1 | iter: 1154 | train loss: 2689.4960937500\n",
            "Epoch: 1 | iter: 1155 | train loss: 3552.9763183594\n",
            "Epoch: 1 | iter: 1156 | train loss: 2609.2128906250\n",
            "Epoch: 1 | iter: 1157 | train loss: 2972.1003417969\n",
            "Epoch: 1 | iter: 1158 | train loss: 2457.1945800781\n",
            "Epoch: 1 | iter: 1159 | train loss: 2289.6843261719\n",
            "Epoch: 1 | iter: 1160 | train loss: 2521.0317382812\n",
            "Epoch: 1 | iter: 1161 | train loss: 3253.7299804688\n",
            "Epoch: 1 | iter: 1162 | train loss: 2452.3903808594\n",
            "Epoch: 1 | iter: 1163 | train loss: 2277.1298828125\n",
            "Epoch: 1 | iter: 1164 | train loss: 2175.6967773438\n",
            "Epoch: 1 | iter: 1165 | train loss: 4505.2070312500\n",
            "Epoch: 1 | iter: 1166 | train loss: 2874.8576660156\n",
            "Epoch: 1 | iter: 1167 | train loss: 2174.8520507812\n",
            "Epoch: 1 | iter: 1168 | train loss: 2948.7666015625\n",
            "Epoch: 1 | iter: 1169 | train loss: 3055.2072753906\n",
            "Epoch: 1 | iter: 1170 | train loss: 2930.8652343750\n",
            "Epoch: 1 | iter: 1171 | train loss: 3092.0205078125\n",
            "Epoch: 1 | iter: 1172 | train loss: 3631.7438964844\n",
            "Epoch: 1 | iter: 1173 | train loss: 3087.2622070312\n",
            "Epoch: 1 | iter: 1174 | train loss: 3180.4667968750\n",
            "Epoch: 1 | iter: 1175 | train loss: 2157.3432617188\n",
            "Epoch: 1 | iter: 1176 | train loss: 2940.0676269531\n",
            "Epoch: 1 | iter: 1177 | train loss: 2996.1828613281\n",
            "Epoch: 1 | iter: 1178 | train loss: 2547.3461914062\n",
            "Epoch: 1 | iter: 1179 | train loss: 2255.8979492188\n",
            "Epoch: 1 | iter: 1180 | train loss: 3089.3552246094\n",
            "Epoch: 1 | iter: 1181 | train loss: 2576.5671386719\n",
            "Epoch: 1 | iter: 1182 | train loss: 2128.9086914062\n",
            "Epoch: 1 | iter: 1183 | train loss: 2303.3437500000\n",
            "Epoch: 1 | iter: 1184 | train loss: 2002.6359863281\n",
            "Epoch: 1 | iter: 1185 | train loss: 3015.1169433594\n",
            "Epoch: 1 | iter: 1186 | train loss: 3113.7624511719\n",
            "Epoch: 1 | iter: 1187 | train loss: 2913.7705078125\n",
            "Epoch: 1 | iter: 1188 | train loss: 2573.0678710938\n",
            "Epoch: 1 | iter: 1189 | train loss: 2474.0903320312\n",
            "Epoch: 1 | iter: 1190 | train loss: 2927.9138183594\n",
            "Epoch: 1 | iter: 1191 | train loss: 2880.5527343750\n",
            "Epoch: 1 | iter: 1192 | train loss: 3028.0512695312\n",
            "Epoch: 1 | iter: 1193 | train loss: 2873.7883300781\n",
            "Epoch: 1 | iter: 1194 | train loss: 3338.7485351562\n",
            "Epoch: 1 | iter: 1195 | train loss: 4068.3400878906\n",
            "Epoch: 1 | iter: 1196 | train loss: 2837.3527832031\n",
            "Epoch: 1 | iter: 1197 | train loss: 2695.1323242188\n",
            "Epoch: 1 | iter: 1198 | train loss: 2941.3808593750\n",
            "Epoch: 1 | iter: 1199 | train loss: 2264.2741699219\n",
            "Epoch: 1 | iter: 1200 | train loss: 2546.6264648438\n",
            "Epoch: 1 | iter: 1201 | train loss: 3599.4570312500\n",
            "Epoch: 1 | iter: 1202 | train loss: 2584.5961914062\n",
            "Epoch: 1 | iter: 1203 | train loss: 3085.1616210938\n",
            "Epoch: 1 | iter: 1204 | train loss: 2629.9252929688\n",
            "Epoch: 1 | iter: 1205 | train loss: 2763.2666015625\n",
            "Epoch: 1 | iter: 1206 | train loss: 2718.5036621094\n",
            "Epoch: 1 | iter: 1207 | train loss: 2383.2290039062\n",
            "Epoch: 1 | iter: 1208 | train loss: 2955.3564453125\n",
            "Epoch: 1 | iter: 1209 | train loss: 2207.7863769531\n",
            "Epoch: 1 | iter: 1210 | train loss: 2070.7258300781\n",
            "Epoch: 1 | iter: 1211 | train loss: 2520.4763183594\n",
            "Epoch: 1 | iter: 1212 | train loss: 2612.8889160156\n",
            "Epoch: 1 | iter: 1213 | train loss: 1943.2982177734\n",
            "Epoch: 1 | iter: 1214 | train loss: 2372.9487304688\n",
            "Epoch: 1 | iter: 1215 | train loss: 2291.5590820312\n",
            "Epoch: 1 | iter: 1216 | train loss: 2759.1457519531\n",
            "Epoch: 1 | iter: 1217 | train loss: 3140.4538574219\n",
            "Epoch: 1 | iter: 1218 | train loss: 3595.1687011719\n",
            "Epoch: 1 | iter: 1219 | train loss: 2504.0793457031\n",
            "Epoch: 1 | iter: 1220 | train loss: 2660.2758789062\n",
            "Epoch: 1 | iter: 1221 | train loss: 2383.1796875000\n",
            "Epoch: 1 | iter: 1222 | train loss: 2591.8254394531\n",
            "Epoch: 1 | iter: 1223 | train loss: 1953.3603515625\n",
            "Epoch: 1 | iter: 1224 | train loss: 2566.4895019531\n",
            "Epoch: 1 | iter: 1225 | train loss: 2162.7224121094\n",
            "Epoch: 1 | iter: 1226 | train loss: 2636.8703613281\n",
            "Epoch: 1 | iter: 1227 | train loss: 2680.7534179688\n",
            "Epoch: 1 | iter: 1228 | train loss: 2843.0690917969\n",
            "Epoch: 1 | iter: 1229 | train loss: 3355.4125976562\n",
            "Epoch: 1 | iter: 1230 | train loss: 2276.2424316406\n",
            "Epoch: 1 | iter: 1231 | train loss: 2126.3317871094\n",
            "Epoch: 1 | iter: 1232 | train loss: 2948.5981445312\n",
            "Epoch: 1 | iter: 1233 | train loss: 2562.6208496094\n",
            "Epoch: 1 | iter: 1234 | train loss: 2414.2619628906\n",
            "Epoch: 1 | iter: 1235 | train loss: 3135.1586914062\n",
            "Epoch: 1 | iter: 1236 | train loss: 2341.0144042969\n",
            "Epoch: 1 | iter: 1237 | train loss: 2347.8061523438\n",
            "Epoch: 1 | iter: 1238 | train loss: 2509.0979003906\n",
            "Epoch: 1 | iter: 1239 | train loss: 2575.4997558594\n",
            "Epoch: 1 | iter: 1240 | train loss: 2679.2885742188\n",
            "Epoch: 1 | iter: 1241 | train loss: 1974.6253662109\n",
            "Epoch: 1 | iter: 1242 | train loss: 2701.3605957031\n",
            "Epoch: 1 | iter: 1243 | train loss: 2691.1735839844\n",
            "Epoch: 1 | iter: 1244 | train loss: 2666.7255859375\n",
            "Epoch: 1 | iter: 1245 | train loss: 2535.2539062500\n",
            "Epoch: 1 | iter: 1246 | train loss: 2141.7343750000\n",
            "Epoch: 1 | iter: 1247 | train loss: 2869.2990722656\n",
            "Epoch: 1 | iter: 1248 | train loss: 3268.2543945312\n",
            "Epoch: 1 | iter: 1249 | train loss: 2897.4472656250\n",
            "Epoch: 1 | iter: 1250 | train loss: 2719.0251464844\n",
            "Epoch: 1 | iter: 1251 | train loss: 3038.4858398438\n",
            "Epoch: 1 | iter: 1252 | train loss: 2599.0820312500\n",
            "Epoch: 1 | iter: 1253 | train loss: 3240.0305175781\n",
            "Epoch: 1 | iter: 1254 | train loss: 3045.6367187500\n",
            "Epoch: 1 | iter: 1255 | train loss: 4000.7761230469\n",
            "Epoch: 1 | iter: 1256 | train loss: 2669.0053710938\n",
            "Epoch: 1 | iter: 1257 | train loss: 2776.5830078125\n",
            "Epoch: 1 | iter: 1258 | train loss: 2617.9619140625\n",
            "Epoch: 1 | iter: 1259 | train loss: 2407.7351074219\n",
            "Epoch: 1 | iter: 1260 | train loss: 3659.9555664062\n",
            "Epoch: 1 | iter: 1261 | train loss: 1758.5551757812\n",
            "Epoch: 1 | iter: 1262 | train loss: 3456.0209960938\n",
            "Epoch: 1 | iter: 1263 | train loss: 1926.4956054688\n",
            "Epoch: 1 | iter: 1264 | train loss: 1637.1417236328\n",
            "Epoch: 1 | iter: 1265 | train loss: 2105.5834960938\n",
            "Epoch: 1 | iter: 1266 | train loss: 2120.9108886719\n",
            "Epoch: 1 | iter: 1267 | train loss: 1562.8781738281\n",
            "Epoch: 1 | iter: 1268 | train loss: 1914.9415283203\n",
            "Epoch: 1 | iter: 1269 | train loss: 3286.7900390625\n",
            "Epoch: 1 | iter: 1270 | train loss: 1643.3012695312\n",
            "Epoch: 1 | iter: 1271 | train loss: 2042.0435791016\n",
            "Epoch: 1 | iter: 1272 | train loss: 2207.4880371094\n",
            "Epoch: 1 | iter: 1273 | train loss: 2217.9799804688\n",
            "Epoch: 1 | iter: 1274 | train loss: 1525.3942871094\n",
            "Epoch: 1 | iter: 1275 | train loss: 2354.4458007812\n",
            "Epoch: 1 | iter: 1276 | train loss: 2691.4956054688\n",
            "Epoch: 1 | iter: 1277 | train loss: 2107.7529296875\n",
            "Epoch: 1 | iter: 1278 | train loss: 2008.3989257812\n",
            "Epoch: 1 | iter: 1279 | train loss: 3387.0534667969\n",
            "Epoch: 1 | iter: 1280 | train loss: 2524.8395996094\n",
            "Epoch: 1 | iter: 1281 | train loss: 2233.7402343750\n",
            "Epoch: 1 | iter: 1282 | train loss: 2133.5061035156\n",
            "Epoch: 1 | iter: 1283 | train loss: 2073.3547363281\n",
            "Epoch: 1 | iter: 1284 | train loss: 2211.9267578125\n",
            "Epoch: 1 | iter: 1285 | train loss: 1886.1116943359\n",
            "Epoch: 1 | iter: 1286 | train loss: 2230.0888671875\n",
            "Epoch: 1 | iter: 1287 | train loss: 2297.6464843750\n",
            "Epoch: 1 | iter: 1288 | train loss: 2102.6284179688\n",
            "Epoch: 1 | iter: 1289 | train loss: 2109.1904296875\n",
            "Epoch: 1 | iter: 1290 | train loss: 2117.2390136719\n",
            "Epoch: 1 | iter: 1291 | train loss: 2225.1459960938\n",
            "Epoch: 1 | iter: 1292 | train loss: 2224.5881347656\n",
            "Epoch: 1 | iter: 1293 | train loss: 3777.4355468750\n",
            "Epoch: 1 | iter: 1294 | train loss: 3616.7592773438\n",
            "Epoch: 1 | iter: 1295 | train loss: 3131.6416015625\n",
            "Epoch: 1 | iter: 1296 | train loss: 3077.7529296875\n",
            "Epoch: 1 | iter: 1297 | train loss: 3334.4699707031\n",
            "Epoch: 1 | iter: 1298 | train loss: 2511.0200195312\n",
            "Epoch: 1 | iter: 1299 | train loss: 2198.9108886719\n",
            "Epoch: 1 | iter: 1300 | train loss: 2735.0209960938\n",
            "Epoch: 1 | iter: 1301 | train loss: 2292.9785156250\n",
            "Epoch: 1 | iter: 1302 | train loss: 2310.7734375000\n",
            "Epoch: 1 | iter: 1303 | train loss: 1871.2822265625\n",
            "Epoch: 1 | iter: 1304 | train loss: 2828.7939453125\n",
            "Epoch: 1 | iter: 1305 | train loss: 2514.9726562500\n",
            "Epoch: 1 | iter: 1306 | train loss: 2870.7668457031\n",
            "Epoch: 1 | iter: 1307 | train loss: 2967.6379394531\n",
            "Epoch: 1 | iter: 1308 | train loss: 2161.2807617188\n",
            "Epoch: 1 | iter: 1309 | train loss: 2682.9824218750\n",
            "Epoch: 1 | iter: 1310 | train loss: 3246.4824218750\n",
            "Epoch: 1 | iter: 1311 | train loss: 2871.2260742188\n",
            "Epoch: 1 | iter: 1312 | train loss: 3095.8232421875\n",
            "Epoch: 1 | iter: 1313 | train loss: 3816.2456054688\n",
            "Epoch: 1 | iter: 1314 | train loss: 2225.8107910156\n",
            "Epoch: 1 | iter: 1315 | train loss: 3469.0754394531\n",
            "Epoch: 1 | iter: 1316 | train loss: 3287.4580078125\n",
            "Epoch: 1 | iter: 1317 | train loss: 2643.7661132812\n",
            "Epoch: 1 | iter: 1318 | train loss: 3184.5583496094\n",
            "Epoch: 1 | iter: 1319 | train loss: 3159.0266113281\n",
            "Epoch: 1 | iter: 1320 | train loss: 3313.7553710938\n",
            "Epoch: 1 | iter: 1321 | train loss: 2683.2060546875\n",
            "Epoch: 1 | iter: 1322 | train loss: 2593.1652832031\n",
            "Epoch: 1 | iter: 1323 | train loss: 2664.2163085938\n",
            "Epoch: 1 | iter: 1324 | train loss: 2727.7700195312\n",
            "Epoch: 1 | iter: 1325 | train loss: 3204.6132812500\n",
            "Epoch: 1 | iter: 1326 | train loss: 2809.5266113281\n",
            "Epoch: 1 | iter: 1327 | train loss: 2511.1572265625\n",
            "Epoch: 1 | iter: 1328 | train loss: 2857.3222656250\n",
            "Epoch: 1 | iter: 1329 | train loss: 2694.0979003906\n",
            "Epoch: 2 | iter: 0 | train loss: 2634.3635253906\n",
            "Epoch: 2 | iter: 1 | train loss: 2362.2614746094\n",
            "Epoch: 2 | iter: 2 | train loss: 2408.9963378906\n",
            "Epoch: 2 | iter: 3 | train loss: 1824.8542480469\n",
            "Epoch: 2 | iter: 4 | train loss: 2261.3012695312\n",
            "Epoch: 2 | iter: 5 | train loss: 2126.3403320312\n",
            "Epoch: 2 | iter: 6 | train loss: 1570.2155761719\n",
            "Epoch: 2 | iter: 7 | train loss: 1835.9355468750\n",
            "Epoch: 2 | iter: 8 | train loss: 1549.4333496094\n",
            "Epoch: 2 | iter: 9 | train loss: 2374.0612792969\n",
            "Epoch: 2 | iter: 10 | train loss: 2131.7832031250\n",
            "Epoch: 2 | iter: 11 | train loss: 1613.3238525391\n",
            "Epoch: 2 | iter: 12 | train loss: 2367.4228515625\n",
            "Epoch: 2 | iter: 13 | train loss: 2192.6955566406\n",
            "Epoch: 2 | iter: 14 | train loss: 1768.0786132812\n",
            "Epoch: 2 | iter: 15 | train loss: 1817.3364257812\n",
            "Epoch: 2 | iter: 16 | train loss: 1535.2689208984\n",
            "Epoch: 2 | iter: 17 | train loss: 968.4436035156\n",
            "Epoch: 2 | iter: 18 | train loss: 2224.8107910156\n",
            "Epoch: 2 | iter: 19 | train loss: 1363.5270996094\n",
            "Epoch: 2 | iter: 20 | train loss: 2369.5839843750\n",
            "Epoch: 2 | iter: 21 | train loss: 1481.8217773438\n",
            "Epoch: 2 | iter: 22 | train loss: 1845.5996093750\n",
            "Epoch: 2 | iter: 23 | train loss: 1934.3378906250\n",
            "Epoch: 2 | iter: 24 | train loss: 1932.3352050781\n",
            "Epoch: 2 | iter: 25 | train loss: 2933.5312500000\n",
            "Epoch: 2 | iter: 26 | train loss: 1918.5996093750\n",
            "Epoch: 2 | iter: 27 | train loss: 1112.9974365234\n",
            "Epoch: 2 | iter: 28 | train loss: 2272.2495117188\n",
            "Epoch: 2 | iter: 29 | train loss: 1913.0502929688\n",
            "Epoch: 2 | iter: 30 | train loss: 2480.1684570312\n",
            "Epoch: 2 | iter: 31 | train loss: 1745.5755615234\n",
            "Epoch: 2 | iter: 32 | train loss: 1430.4248046875\n",
            "Epoch: 2 | iter: 33 | train loss: 2207.3469238281\n",
            "Epoch: 2 | iter: 34 | train loss: 1776.0646972656\n",
            "Epoch: 2 | iter: 35 | train loss: 1682.8052978516\n",
            "Epoch: 2 | iter: 36 | train loss: 1738.4238281250\n",
            "Epoch: 2 | iter: 37 | train loss: 1828.5843505859\n",
            "Epoch: 2 | iter: 38 | train loss: 2003.8984375000\n",
            "Epoch: 2 | iter: 39 | train loss: 1975.0197753906\n",
            "Epoch: 2 | iter: 40 | train loss: 1325.9178466797\n",
            "Epoch: 2 | iter: 41 | train loss: 1334.1525878906\n",
            "Epoch: 2 | iter: 42 | train loss: 1489.3038330078\n",
            "Epoch: 2 | iter: 43 | train loss: 2327.9128417969\n",
            "Epoch: 2 | iter: 44 | train loss: 1690.5910644531\n",
            "Epoch: 2 | iter: 45 | train loss: 1463.7656250000\n",
            "Epoch: 2 | iter: 46 | train loss: 1177.5793457031\n",
            "Epoch: 2 | iter: 47 | train loss: 999.7230224609\n",
            "Epoch: 2 | iter: 48 | train loss: 2105.3330078125\n",
            "Epoch: 2 | iter: 49 | train loss: 1517.5074462891\n",
            "Epoch: 2 | iter: 50 | train loss: 1452.5718994141\n",
            "Epoch: 2 | iter: 51 | train loss: 2734.3178710938\n",
            "Epoch: 2 | iter: 52 | train loss: 1376.2474365234\n",
            "Epoch: 2 | iter: 53 | train loss: 1506.8843994141\n",
            "Epoch: 2 | iter: 54 | train loss: 1781.7441406250\n",
            "Epoch: 2 | iter: 55 | train loss: 1375.6955566406\n",
            "Epoch: 2 | iter: 56 | train loss: 1557.9504394531\n",
            "Epoch: 2 | iter: 57 | train loss: 1377.5480957031\n",
            "Epoch: 2 | iter: 58 | train loss: 1133.0141601562\n",
            "Epoch: 2 | iter: 59 | train loss: 1827.1777343750\n",
            "Epoch: 2 | iter: 60 | train loss: 989.8029174805\n",
            "Epoch: 2 | iter: 61 | train loss: 1307.9536132812\n",
            "Epoch: 2 | iter: 62 | train loss: 1673.7799072266\n",
            "Epoch: 2 | iter: 63 | train loss: 1835.7495117188\n",
            "Epoch: 2 | iter: 64 | train loss: 1751.7321777344\n",
            "Epoch: 2 | iter: 65 | train loss: 1706.0775146484\n",
            "Epoch: 2 | iter: 66 | train loss: 1775.0073242188\n",
            "Epoch: 2 | iter: 67 | train loss: 1601.8457031250\n",
            "Epoch: 2 | iter: 68 | train loss: 1722.4924316406\n",
            "Epoch: 2 | iter: 69 | train loss: 1486.7443847656\n",
            "Epoch: 2 | iter: 70 | train loss: 1130.3505859375\n",
            "Epoch: 2 | iter: 71 | train loss: 1543.5063476562\n",
            "Epoch: 2 | iter: 72 | train loss: 1720.0104980469\n",
            "Epoch: 2 | iter: 73 | train loss: 1605.2917480469\n",
            "Epoch: 2 | iter: 74 | train loss: 1519.0568847656\n",
            "Epoch: 2 | iter: 75 | train loss: 1252.8948974609\n",
            "Epoch: 2 | iter: 76 | train loss: 2027.5208740234\n",
            "Epoch: 2 | iter: 77 | train loss: 868.2431030273\n",
            "Epoch: 2 | iter: 78 | train loss: 1629.1236572266\n",
            "Epoch: 2 | iter: 79 | train loss: 1181.5367431641\n",
            "Epoch: 2 | iter: 80 | train loss: 2429.5344238281\n",
            "Epoch: 2 | iter: 81 | train loss: 1421.5352783203\n",
            "Epoch: 2 | iter: 82 | train loss: 1483.8041992188\n",
            "Epoch: 2 | iter: 83 | train loss: 1362.7552490234\n",
            "Epoch: 2 | iter: 84 | train loss: 1244.8789062500\n",
            "Epoch: 2 | iter: 85 | train loss: 1061.2980957031\n",
            "Epoch: 2 | iter: 86 | train loss: 1644.2219238281\n",
            "Epoch: 2 | iter: 87 | train loss: 1554.9826660156\n",
            "Epoch: 2 | iter: 88 | train loss: 1752.7663574219\n",
            "Epoch: 2 | iter: 89 | train loss: 1665.0739746094\n",
            "Epoch: 2 | iter: 90 | train loss: 1619.8070068359\n",
            "Epoch: 2 | iter: 91 | train loss: 1835.2122802734\n",
            "Epoch: 2 | iter: 92 | train loss: 1901.9853515625\n",
            "Epoch: 2 | iter: 93 | train loss: 1165.4486083984\n",
            "Epoch: 2 | iter: 94 | train loss: 1382.9165039062\n",
            "Epoch: 2 | iter: 95 | train loss: 1270.4918212891\n",
            "Epoch: 2 | iter: 96 | train loss: 1420.4007568359\n",
            "Epoch: 2 | iter: 97 | train loss: 1923.8867187500\n",
            "Epoch: 2 | iter: 98 | train loss: 1757.4785156250\n",
            "Epoch: 2 | iter: 99 | train loss: 2294.2817382812\n",
            "Epoch: 2 | iter: 100 | train loss: 1718.8662109375\n",
            "Epoch: 2 | iter: 101 | train loss: 2059.7304687500\n",
            "Epoch: 2 | iter: 102 | train loss: 2015.1337890625\n",
            "Epoch: 2 | iter: 103 | train loss: 1760.8283691406\n",
            "Epoch: 2 | iter: 104 | train loss: 1624.6154785156\n",
            "Epoch: 2 | iter: 105 | train loss: 1771.7600097656\n",
            "Epoch: 2 | iter: 106 | train loss: 2136.3166503906\n",
            "Epoch: 2 | iter: 107 | train loss: 2416.3962402344\n",
            "Epoch: 2 | iter: 108 | train loss: 3077.1511230469\n",
            "Epoch: 2 | iter: 109 | train loss: 2234.3146972656\n",
            "Epoch: 2 | iter: 110 | train loss: 2961.5463867188\n",
            "Epoch: 2 | iter: 111 | train loss: 2621.5280761719\n",
            "Epoch: 2 | iter: 112 | train loss: 2738.4128417969\n",
            "Epoch: 2 | iter: 113 | train loss: 3184.4257812500\n",
            "Epoch: 2 | iter: 114 | train loss: 1814.5717773438\n",
            "Epoch: 2 | iter: 115 | train loss: 2119.7348632812\n",
            "Epoch: 2 | iter: 116 | train loss: 3744.3115234375\n",
            "Epoch: 2 | iter: 117 | train loss: 2331.3242187500\n",
            "Epoch: 2 | iter: 118 | train loss: 2260.6816406250\n",
            "Epoch: 2 | iter: 119 | train loss: 2596.9311523438\n",
            "Epoch: 2 | iter: 120 | train loss: 2354.9326171875\n",
            "Epoch: 2 | iter: 121 | train loss: 3325.0253906250\n",
            "Epoch: 2 | iter: 122 | train loss: 2533.2211914062\n",
            "Epoch: 2 | iter: 123 | train loss: 2058.5920410156\n",
            "Epoch: 2 | iter: 124 | train loss: 2184.5634765625\n",
            "Epoch: 2 | iter: 125 | train loss: 2532.4658203125\n",
            "Epoch: 2 | iter: 126 | train loss: 2196.0476074219\n",
            "Epoch: 2 | iter: 127 | train loss: 2239.6660156250\n",
            "Epoch: 2 | iter: 128 | train loss: 2491.2670898438\n",
            "Epoch: 2 | iter: 129 | train loss: 1739.8996582031\n",
            "Epoch: 2 | iter: 130 | train loss: 2053.9255371094\n",
            "Epoch: 2 | iter: 131 | train loss: 1794.3066406250\n",
            "Epoch: 2 | iter: 132 | train loss: 1402.6315917969\n",
            "Epoch: 2 | iter: 133 | train loss: 2519.7695312500\n",
            "Epoch: 2 | iter: 134 | train loss: 2598.2136230469\n",
            "Epoch: 2 | iter: 135 | train loss: 2204.4379882812\n",
            "Epoch: 2 | iter: 136 | train loss: 2607.7070312500\n",
            "Epoch: 2 | iter: 137 | train loss: 1743.8359375000\n",
            "Epoch: 2 | iter: 138 | train loss: 2028.6335449219\n",
            "Epoch: 2 | iter: 139 | train loss: 2004.0717773438\n",
            "Epoch: 2 | iter: 140 | train loss: 2211.0859375000\n",
            "Epoch: 2 | iter: 141 | train loss: 2071.5844726562\n",
            "Epoch: 2 | iter: 142 | train loss: 2683.0200195312\n",
            "Epoch: 2 | iter: 143 | train loss: 3061.8295898438\n",
            "Epoch: 2 | iter: 144 | train loss: 2157.9367675781\n",
            "Epoch: 2 | iter: 145 | train loss: 2161.4460449219\n",
            "Epoch: 2 | iter: 146 | train loss: 1353.8469238281\n",
            "Epoch: 2 | iter: 147 | train loss: 1784.0268554688\n",
            "Epoch: 2 | iter: 148 | train loss: 2901.4306640625\n",
            "Epoch: 2 | iter: 149 | train loss: 2703.6594238281\n",
            "Epoch: 2 | iter: 150 | train loss: 3605.8906250000\n",
            "Epoch: 2 | iter: 151 | train loss: 2381.9750976562\n",
            "Epoch: 2 | iter: 152 | train loss: 1951.0144042969\n",
            "Epoch: 2 | iter: 153 | train loss: 2097.7126464844\n",
            "Epoch: 2 | iter: 154 | train loss: 3094.7924804688\n",
            "Epoch: 2 | iter: 155 | train loss: 1941.2956542969\n",
            "Epoch: 2 | iter: 156 | train loss: 2097.0620117188\n",
            "Epoch: 2 | iter: 157 | train loss: 2285.7790527344\n",
            "Epoch: 2 | iter: 158 | train loss: 2203.4135742188\n",
            "Epoch: 2 | iter: 159 | train loss: 2008.4709472656\n",
            "Epoch: 2 | iter: 160 | train loss: 2047.2612304688\n",
            "Epoch: 2 | iter: 161 | train loss: 2287.8691406250\n",
            "Epoch: 2 | iter: 162 | train loss: 2361.5112304688\n",
            "Epoch: 2 | iter: 163 | train loss: 3012.0004882812\n",
            "Epoch: 2 | iter: 164 | train loss: 2914.2084960938\n",
            "Epoch: 2 | iter: 165 | train loss: 2782.8481445312\n",
            "Epoch: 2 | iter: 166 | train loss: 2824.7587890625\n",
            "Epoch: 2 | iter: 167 | train loss: 2201.4438476562\n",
            "Epoch: 2 | iter: 168 | train loss: 2489.9121093750\n",
            "Epoch: 2 | iter: 169 | train loss: 2595.9008789062\n",
            "Epoch: 2 | iter: 170 | train loss: 3055.2612304688\n",
            "Epoch: 2 | iter: 171 | train loss: 2671.2534179688\n",
            "Epoch: 2 | iter: 172 | train loss: 2068.8137207031\n",
            "Epoch: 2 | iter: 173 | train loss: 2852.6845703125\n",
            "Epoch: 2 | iter: 174 | train loss: 2794.3505859375\n",
            "Epoch: 2 | iter: 175 | train loss: 2283.1000976562\n",
            "Epoch: 2 | iter: 176 | train loss: 2991.9780273438\n",
            "Epoch: 2 | iter: 177 | train loss: 2873.0214843750\n",
            "Epoch: 2 | iter: 178 | train loss: 3484.8735351562\n",
            "Epoch: 2 | iter: 179 | train loss: 3285.1298828125\n",
            "Epoch: 2 | iter: 180 | train loss: 3406.6401367188\n",
            "Epoch: 2 | iter: 181 | train loss: 2665.7312011719\n",
            "Epoch: 2 | iter: 182 | train loss: 3415.1899414062\n",
            "Epoch: 2 | iter: 183 | train loss: 2986.6220703125\n",
            "Epoch: 2 | iter: 184 | train loss: 3091.6667480469\n",
            "Epoch: 2 | iter: 185 | train loss: 3697.5329589844\n",
            "Epoch: 2 | iter: 186 | train loss: 3580.6401367188\n",
            "Epoch: 2 | iter: 187 | train loss: 3260.2246093750\n",
            "Epoch: 2 | iter: 188 | train loss: 2370.1826171875\n",
            "Epoch: 2 | iter: 189 | train loss: 4149.3491210938\n",
            "Epoch: 2 | iter: 190 | train loss: 3065.1508789062\n",
            "Epoch: 2 | iter: 191 | train loss: 2981.0776367188\n",
            "Epoch: 2 | iter: 192 | train loss: 4516.7148437500\n",
            "Epoch: 2 | iter: 193 | train loss: 2614.5031738281\n",
            "Epoch: 2 | iter: 194 | train loss: 2821.0803222656\n",
            "Epoch: 2 | iter: 195 | train loss: 2455.6777343750\n",
            "Epoch: 2 | iter: 196 | train loss: 3162.7822265625\n",
            "Epoch: 2 | iter: 197 | train loss: 2185.0063476562\n",
            "Epoch: 2 | iter: 198 | train loss: 1779.1057128906\n",
            "Epoch: 2 | iter: 199 | train loss: 2587.2255859375\n",
            "Epoch: 2 | iter: 200 | train loss: 2557.2070312500\n",
            "Epoch: 2 | iter: 201 | train loss: 3184.8310546875\n",
            "Epoch: 2 | iter: 202 | train loss: 3873.6840820312\n",
            "Epoch: 2 | iter: 203 | train loss: 2510.4584960938\n",
            "Epoch: 2 | iter: 204 | train loss: 3308.5319824219\n",
            "Epoch: 2 | iter: 205 | train loss: 2839.3334960938\n",
            "Epoch: 2 | iter: 206 | train loss: 1945.9287109375\n",
            "Epoch: 2 | iter: 207 | train loss: 1974.1407470703\n",
            "Epoch: 2 | iter: 208 | train loss: 2523.2722167969\n",
            "Epoch: 2 | iter: 209 | train loss: 2141.5136718750\n",
            "Epoch: 2 | iter: 210 | train loss: 2299.9248046875\n",
            "Epoch: 2 | iter: 211 | train loss: 1667.3719482422\n",
            "Epoch: 2 | iter: 212 | train loss: 1887.2095947266\n",
            "Epoch: 2 | iter: 213 | train loss: 2175.2690429688\n",
            "Epoch: 2 | iter: 214 | train loss: 2325.2729492188\n",
            "Epoch: 2 | iter: 215 | train loss: 2460.4775390625\n",
            "Epoch: 2 | iter: 216 | train loss: 2427.9233398438\n",
            "Epoch: 2 | iter: 217 | train loss: 2643.4165039062\n",
            "Epoch: 2 | iter: 218 | train loss: 2512.6191406250\n",
            "Epoch: 2 | iter: 219 | train loss: 2308.8664550781\n",
            "Epoch: 2 | iter: 220 | train loss: 2809.4853515625\n",
            "Epoch: 2 | iter: 221 | train loss: 1978.2580566406\n",
            "Epoch: 2 | iter: 222 | train loss: 2232.0493164062\n",
            "Epoch: 2 | iter: 223 | train loss: 2443.6538085938\n",
            "Epoch: 2 | iter: 224 | train loss: 1818.4060058594\n",
            "Epoch: 2 | iter: 225 | train loss: 2537.5065917969\n",
            "Epoch: 2 | iter: 226 | train loss: 1849.7261962891\n",
            "Epoch: 2 | iter: 227 | train loss: 2860.4567871094\n",
            "Epoch: 2 | iter: 228 | train loss: 1537.7604980469\n",
            "Epoch: 2 | iter: 229 | train loss: 3100.4323730469\n",
            "Epoch: 2 | iter: 230 | train loss: 1823.9008789062\n",
            "Epoch: 2 | iter: 231 | train loss: 1637.9069824219\n",
            "Epoch: 2 | iter: 232 | train loss: 3120.2756347656\n",
            "Epoch: 2 | iter: 233 | train loss: 1585.8432617188\n",
            "Epoch: 2 | iter: 234 | train loss: 2852.1154785156\n",
            "Epoch: 2 | iter: 235 | train loss: 3715.6298828125\n",
            "Epoch: 2 | iter: 236 | train loss: 2480.4580078125\n",
            "Epoch: 2 | iter: 237 | train loss: 1906.5229492188\n",
            "Epoch: 2 | iter: 238 | train loss: 2042.2905273438\n",
            "Epoch: 2 | iter: 239 | train loss: 1165.0256347656\n",
            "Epoch: 2 | iter: 240 | train loss: 2393.5024414062\n",
            "Epoch: 2 | iter: 241 | train loss: 2018.5151367188\n",
            "Epoch: 2 | iter: 242 | train loss: 2645.8408203125\n",
            "Epoch: 2 | iter: 243 | train loss: 2913.1474609375\n",
            "Epoch: 2 | iter: 244 | train loss: 2285.9819335938\n",
            "Epoch: 2 | iter: 245 | train loss: 2531.2397460938\n",
            "Epoch: 2 | iter: 246 | train loss: 3325.0275878906\n",
            "Epoch: 2 | iter: 247 | train loss: 1757.5833740234\n",
            "Epoch: 2 | iter: 248 | train loss: 2737.6877441406\n",
            "Epoch: 2 | iter: 249 | train loss: 2208.9843750000\n",
            "Epoch: 2 | iter: 250 | train loss: 2767.7661132812\n",
            "Epoch: 2 | iter: 251 | train loss: 1679.2690429688\n",
            "Epoch: 2 | iter: 252 | train loss: 1226.3562011719\n",
            "Epoch: 2 | iter: 253 | train loss: 1453.4851074219\n",
            "Epoch: 2 | iter: 254 | train loss: 1427.6286621094\n",
            "Epoch: 2 | iter: 255 | train loss: 1052.5662841797\n",
            "Epoch: 2 | iter: 256 | train loss: 1761.7042236328\n",
            "Epoch: 2 | iter: 257 | train loss: 1040.8963623047\n",
            "Epoch: 2 | iter: 258 | train loss: 3550.9689941406\n",
            "Epoch: 2 | iter: 259 | train loss: 1974.5516357422\n",
            "Epoch: 2 | iter: 260 | train loss: 1743.7370605469\n",
            "Epoch: 2 | iter: 261 | train loss: 1650.0913085938\n",
            "Epoch: 2 | iter: 262 | train loss: 1881.9068603516\n",
            "Epoch: 2 | iter: 263 | train loss: 1101.8790283203\n",
            "Epoch: 2 | iter: 264 | train loss: 1929.9318847656\n",
            "Epoch: 2 | iter: 265 | train loss: 2504.3334960938\n",
            "Epoch: 2 | iter: 266 | train loss: 1460.9893798828\n",
            "Epoch: 2 | iter: 267 | train loss: 1949.3262939453\n",
            "Epoch: 2 | iter: 268 | train loss: 1536.4976806641\n",
            "Epoch: 2 | iter: 269 | train loss: 2347.5136718750\n",
            "Epoch: 2 | iter: 270 | train loss: 1607.7226562500\n",
            "Epoch: 2 | iter: 271 | train loss: 2230.9450683594\n",
            "Epoch: 2 | iter: 272 | train loss: 1493.6403808594\n",
            "Epoch: 2 | iter: 273 | train loss: 2979.0729980469\n",
            "Epoch: 2 | iter: 274 | train loss: 1816.7508544922\n",
            "Epoch: 2 | iter: 275 | train loss: 1642.0610351562\n",
            "Epoch: 2 | iter: 276 | train loss: 2099.4414062500\n",
            "Epoch: 2 | iter: 277 | train loss: 2260.8503417969\n",
            "Epoch: 2 | iter: 278 | train loss: 1970.5802001953\n",
            "Epoch: 2 | iter: 279 | train loss: 1850.2563476562\n",
            "Epoch: 2 | iter: 280 | train loss: 1813.2175292969\n",
            "Epoch: 2 | iter: 281 | train loss: 1770.2902832031\n",
            "Epoch: 2 | iter: 282 | train loss: 2356.8854980469\n",
            "Epoch: 2 | iter: 283 | train loss: 2178.1162109375\n",
            "Epoch: 2 | iter: 284 | train loss: 1324.5686035156\n",
            "Epoch: 2 | iter: 285 | train loss: 1680.6408691406\n",
            "Epoch: 2 | iter: 286 | train loss: 1996.0377197266\n",
            "Epoch: 2 | iter: 287 | train loss: 2185.8635253906\n",
            "Epoch: 2 | iter: 288 | train loss: 2544.2150878906\n",
            "Epoch: 2 | iter: 289 | train loss: 1036.3973388672\n",
            "Epoch: 2 | iter: 290 | train loss: 1347.4985351562\n",
            "Epoch: 2 | iter: 291 | train loss: 1796.6198730469\n",
            "Epoch: 2 | iter: 292 | train loss: 2095.2346191406\n",
            "Epoch: 2 | iter: 293 | train loss: 1355.6762695312\n",
            "Epoch: 2 | iter: 294 | train loss: 1904.2230224609\n",
            "Epoch: 2 | iter: 295 | train loss: 1739.7465820312\n",
            "Epoch: 2 | iter: 296 | train loss: 2570.6545410156\n",
            "Epoch: 2 | iter: 297 | train loss: 1797.4731445312\n",
            "Epoch: 2 | iter: 298 | train loss: 1426.5053710938\n",
            "Epoch: 2 | iter: 299 | train loss: 1757.5684814453\n",
            "Epoch: 2 | iter: 300 | train loss: 2592.5212402344\n",
            "Epoch: 2 | iter: 301 | train loss: 2338.3483886719\n",
            "Epoch: 2 | iter: 302 | train loss: 3480.9697265625\n",
            "Epoch: 2 | iter: 303 | train loss: 2633.7404785156\n",
            "Epoch: 2 | iter: 304 | train loss: 2930.2973632812\n",
            "Epoch: 2 | iter: 305 | train loss: 2234.8049316406\n",
            "Epoch: 2 | iter: 306 | train loss: 2312.8369140625\n",
            "Epoch: 2 | iter: 307 | train loss: 2543.3251953125\n",
            "Epoch: 2 | iter: 308 | train loss: 1715.0146484375\n",
            "Epoch: 2 | iter: 309 | train loss: 1898.6074218750\n",
            "Epoch: 2 | iter: 310 | train loss: 2504.1342773438\n",
            "Epoch: 2 | iter: 311 | train loss: 1758.9082031250\n",
            "Epoch: 2 | iter: 312 | train loss: 1371.2326660156\n",
            "Epoch: 2 | iter: 313 | train loss: 1994.6286621094\n",
            "Epoch: 2 | iter: 314 | train loss: 1623.8864746094\n",
            "Epoch: 2 | iter: 315 | train loss: 945.6362915039\n",
            "Epoch: 2 | iter: 316 | train loss: 1847.9478759766\n",
            "Epoch: 2 | iter: 317 | train loss: 2186.4887695312\n",
            "Epoch: 2 | iter: 318 | train loss: 2214.2827148438\n",
            "Epoch: 2 | iter: 319 | train loss: 1557.4180908203\n",
            "Epoch: 2 | iter: 320 | train loss: 1282.7100830078\n",
            "Epoch: 2 | iter: 321 | train loss: 1807.6132812500\n",
            "Epoch: 2 | iter: 322 | train loss: 1823.5979003906\n",
            "Epoch: 2 | iter: 323 | train loss: 2810.5949707031\n",
            "Epoch: 2 | iter: 324 | train loss: 1224.1208496094\n",
            "Epoch: 2 | iter: 325 | train loss: 1264.5683593750\n",
            "Epoch: 2 | iter: 326 | train loss: 1519.4738769531\n",
            "Epoch: 2 | iter: 327 | train loss: 2341.1882324219\n",
            "Epoch: 2 | iter: 328 | train loss: 1646.0805664062\n",
            "Epoch: 2 | iter: 329 | train loss: 2024.3911132812\n",
            "Epoch: 2 | iter: 330 | train loss: 1791.6269531250\n",
            "Epoch: 2 | iter: 331 | train loss: 963.9000244141\n",
            "Epoch: 2 | iter: 332 | train loss: 2763.9851074219\n",
            "Epoch: 2 | iter: 333 | train loss: 1370.9853515625\n",
            "Epoch: 2 | iter: 334 | train loss: 1824.5991210938\n",
            "Epoch: 2 | iter: 335 | train loss: 914.1082763672\n",
            "Epoch: 2 | iter: 336 | train loss: 1167.1809082031\n",
            "Epoch: 2 | iter: 337 | train loss: 1269.8455810547\n",
            "Epoch: 2 | iter: 338 | train loss: 1421.3099365234\n",
            "Epoch: 2 | iter: 339 | train loss: 2084.9711914062\n",
            "Epoch: 2 | iter: 340 | train loss: 1408.7288818359\n",
            "Epoch: 2 | iter: 341 | train loss: 1775.0397949219\n",
            "Epoch: 2 | iter: 342 | train loss: 2190.5520019531\n",
            "Epoch: 2 | iter: 343 | train loss: 1995.8666992188\n",
            "Epoch: 2 | iter: 344 | train loss: 2317.8920898438\n",
            "Epoch: 2 | iter: 345 | train loss: 1585.2824707031\n",
            "Epoch: 2 | iter: 346 | train loss: 1379.1610107422\n",
            "Epoch: 2 | iter: 347 | train loss: 867.5390625000\n",
            "Epoch: 2 | iter: 348 | train loss: 1847.0656738281\n",
            "Epoch: 2 | iter: 349 | train loss: 1322.5170898438\n",
            "Epoch: 2 | iter: 350 | train loss: 1470.0139160156\n",
            "Epoch: 2 | iter: 351 | train loss: 1368.4394531250\n",
            "Epoch: 2 | iter: 352 | train loss: 1942.8212890625\n",
            "Epoch: 2 | iter: 353 | train loss: 1890.5793457031\n",
            "Epoch: 2 | iter: 354 | train loss: 1975.9527587891\n",
            "Epoch: 2 | iter: 355 | train loss: 2010.4003906250\n",
            "Epoch: 2 | iter: 356 | train loss: 1929.0354003906\n",
            "Epoch: 2 | iter: 357 | train loss: 1590.7845458984\n",
            "Epoch: 2 | iter: 358 | train loss: 1337.9282226562\n",
            "Epoch: 2 | iter: 359 | train loss: 1884.7235107422\n",
            "Epoch: 2 | iter: 360 | train loss: 1536.9656982422\n",
            "Epoch: 2 | iter: 361 | train loss: 1282.6097412109\n",
            "Epoch: 2 | iter: 362 | train loss: 1632.2985839844\n",
            "Epoch: 2 | iter: 363 | train loss: 2232.8618164062\n",
            "Epoch: 2 | iter: 364 | train loss: 2498.8195800781\n",
            "Epoch: 2 | iter: 365 | train loss: 1710.5600585938\n",
            "Epoch: 2 | iter: 366 | train loss: 1804.9124755859\n",
            "Epoch: 2 | iter: 367 | train loss: 818.2429199219\n",
            "Epoch: 2 | iter: 368 | train loss: 1629.2993164062\n",
            "Epoch: 2 | iter: 369 | train loss: 2593.6806640625\n",
            "Epoch: 2 | iter: 370 | train loss: 2500.3366699219\n",
            "Epoch: 2 | iter: 371 | train loss: 2275.7312011719\n",
            "Epoch: 2 | iter: 372 | train loss: 2728.9921875000\n",
            "Epoch: 2 | iter: 373 | train loss: 1768.7231445312\n",
            "Epoch: 2 | iter: 374 | train loss: 2221.0488281250\n",
            "Epoch: 2 | iter: 375 | train loss: 2801.9731445312\n",
            "Epoch: 2 | iter: 376 | train loss: 2139.3405761719\n",
            "Epoch: 2 | iter: 377 | train loss: 1639.5548095703\n",
            "Epoch: 2 | iter: 378 | train loss: 2468.5935058594\n",
            "Epoch: 2 | iter: 379 | train loss: 2423.0522460938\n",
            "Epoch: 2 | iter: 380 | train loss: 4631.0346679688\n",
            "Epoch: 2 | iter: 381 | train loss: 3044.9306640625\n",
            "Epoch: 2 | iter: 382 | train loss: 2768.1362304688\n",
            "Epoch: 2 | iter: 383 | train loss: 2345.2822265625\n",
            "Epoch: 2 | iter: 384 | train loss: 1509.2800292969\n",
            "Epoch: 2 | iter: 385 | train loss: 684.1604003906\n",
            "Epoch: 2 | iter: 386 | train loss: 2701.1264648438\n",
            "Epoch: 2 | iter: 387 | train loss: 827.7454833984\n",
            "Epoch: 2 | iter: 388 | train loss: 1447.1219482422\n",
            "Epoch: 2 | iter: 389 | train loss: 1641.0603027344\n",
            "Epoch: 2 | iter: 390 | train loss: 1188.6280517578\n",
            "Epoch: 2 | iter: 391 | train loss: 1284.7248535156\n",
            "Epoch: 2 | iter: 392 | train loss: 751.8954467773\n",
            "Epoch: 2 | iter: 393 | train loss: 2096.1525878906\n",
            "Epoch: 2 | iter: 394 | train loss: 808.6325683594\n",
            "Epoch: 2 | iter: 395 | train loss: 1735.3988037109\n",
            "Epoch: 2 | iter: 396 | train loss: 1489.6406250000\n",
            "Epoch: 2 | iter: 397 | train loss: 1175.5529785156\n",
            "Epoch: 2 | iter: 398 | train loss: 1558.7260742188\n",
            "Epoch: 2 | iter: 399 | train loss: 1099.4914550781\n",
            "Epoch: 2 | iter: 400 | train loss: 1763.4614257812\n",
            "Epoch: 2 | iter: 401 | train loss: 1034.8402099609\n",
            "Epoch: 2 | iter: 402 | train loss: 1175.6201171875\n",
            "Epoch: 2 | iter: 403 | train loss: 1458.5673828125\n",
            "Epoch: 2 | iter: 404 | train loss: 1636.1577148438\n",
            "Epoch: 2 | iter: 405 | train loss: 1394.2404785156\n",
            "Epoch: 2 | iter: 406 | train loss: 1392.9437255859\n",
            "Epoch: 2 | iter: 407 | train loss: 1595.7543945312\n",
            "Epoch: 2 | iter: 408 | train loss: 2174.9082031250\n",
            "Epoch: 2 | iter: 409 | train loss: 1560.2265625000\n",
            "Epoch: 2 | iter: 410 | train loss: 2032.4493408203\n",
            "Epoch: 2 | iter: 411 | train loss: 916.2176513672\n",
            "Epoch: 2 | iter: 412 | train loss: 1382.6691894531\n",
            "Epoch: 2 | iter: 413 | train loss: 1507.9133300781\n",
            "Epoch: 2 | iter: 414 | train loss: 1334.9960937500\n",
            "Epoch: 2 | iter: 415 | train loss: 2034.5354003906\n",
            "Epoch: 2 | iter: 416 | train loss: 1083.2061767578\n",
            "Epoch: 2 | iter: 417 | train loss: 2031.8571777344\n",
            "Epoch: 2 | iter: 418 | train loss: 1988.7316894531\n",
            "Epoch: 2 | iter: 419 | train loss: 1006.3088378906\n",
            "Epoch: 2 | iter: 420 | train loss: 1570.7124023438\n",
            "Epoch: 2 | iter: 421 | train loss: 940.0480957031\n",
            "Epoch: 2 | iter: 422 | train loss: 1462.6708984375\n",
            "Epoch: 2 | iter: 423 | train loss: 1743.8682861328\n",
            "Epoch: 2 | iter: 424 | train loss: 1047.1064453125\n",
            "Epoch: 2 | iter: 425 | train loss: 1433.1223144531\n",
            "Epoch: 2 | iter: 426 | train loss: 2322.6274414062\n",
            "Epoch: 2 | iter: 427 | train loss: 1528.2593994141\n",
            "Epoch: 2 | iter: 428 | train loss: 1044.5281982422\n",
            "Epoch: 2 | iter: 429 | train loss: 708.7261962891\n",
            "Epoch: 2 | iter: 430 | train loss: 914.5433349609\n",
            "Epoch: 2 | iter: 431 | train loss: 853.2303466797\n",
            "Epoch: 2 | iter: 432 | train loss: 2071.4121093750\n",
            "Epoch: 2 | iter: 433 | train loss: 964.5216064453\n",
            "Epoch: 2 | iter: 434 | train loss: 1508.0334472656\n",
            "Epoch: 2 | iter: 435 | train loss: 1475.5012207031\n",
            "Epoch: 2 | iter: 436 | train loss: 2543.5202636719\n",
            "Epoch: 2 | iter: 437 | train loss: 1134.8106689453\n",
            "Epoch: 2 | iter: 438 | train loss: 1138.9246826172\n",
            "Epoch: 2 | iter: 439 | train loss: 1115.8681640625\n",
            "Epoch: 2 | iter: 440 | train loss: 1163.7617187500\n",
            "Epoch: 2 | iter: 441 | train loss: 1171.2225341797\n",
            "Epoch: 2 | iter: 442 | train loss: 1882.9570312500\n",
            "Epoch: 2 | iter: 443 | train loss: 579.3193969727\n",
            "Epoch: 2 | iter: 444 | train loss: 1639.3317871094\n",
            "Epoch: 2 | iter: 445 | train loss: 2345.8830566406\n",
            "Epoch: 2 | iter: 446 | train loss: 1463.7415771484\n",
            "Epoch: 2 | iter: 447 | train loss: 1324.0181884766\n",
            "Epoch: 2 | iter: 448 | train loss: 2084.7072753906\n",
            "Epoch: 2 | iter: 449 | train loss: 714.0129394531\n",
            "Epoch: 2 | iter: 450 | train loss: 1305.6645507812\n",
            "Epoch: 2 | iter: 451 | train loss: 718.5206298828\n",
            "Epoch: 2 | iter: 452 | train loss: 690.3027954102\n",
            "Epoch: 2 | iter: 453 | train loss: 903.3020629883\n",
            "Epoch: 2 | iter: 454 | train loss: 901.4193725586\n",
            "Epoch: 2 | iter: 455 | train loss: 788.0026855469\n",
            "Epoch: 2 | iter: 456 | train loss: 1914.7448730469\n",
            "Epoch: 2 | iter: 457 | train loss: 2023.2768554688\n",
            "Epoch: 2 | iter: 458 | train loss: 1786.9514160156\n",
            "Epoch: 2 | iter: 459 | train loss: 1779.2038574219\n",
            "Epoch: 2 | iter: 460 | train loss: 1857.1848144531\n",
            "Epoch: 2 | iter: 461 | train loss: 1432.7551269531\n",
            "Epoch: 2 | iter: 462 | train loss: 1228.8825683594\n",
            "Epoch: 2 | iter: 463 | train loss: 2017.4514160156\n",
            "Epoch: 2 | iter: 464 | train loss: 2396.9975585938\n",
            "Epoch: 2 | iter: 465 | train loss: 1410.1262207031\n",
            "Epoch: 2 | iter: 466 | train loss: 805.7435913086\n",
            "Epoch: 2 | iter: 467 | train loss: 2167.0581054688\n",
            "Epoch: 2 | iter: 468 | train loss: 1279.6810302734\n",
            "Epoch: 2 | iter: 469 | train loss: 666.0963134766\n",
            "Epoch: 2 | iter: 470 | train loss: 1460.6579589844\n",
            "Epoch: 2 | iter: 471 | train loss: 2283.0278320312\n",
            "Epoch: 2 | iter: 472 | train loss: 2313.7778320312\n",
            "Epoch: 2 | iter: 473 | train loss: 1305.9268798828\n",
            "Epoch: 2 | iter: 474 | train loss: 2576.3903808594\n",
            "Epoch: 2 | iter: 475 | train loss: 1682.5080566406\n",
            "Epoch: 2 | iter: 476 | train loss: 1270.8442382812\n",
            "Epoch: 2 | iter: 477 | train loss: 578.6282958984\n",
            "Epoch: 2 | iter: 478 | train loss: 1628.8786621094\n",
            "Epoch: 2 | iter: 479 | train loss: 1551.8273925781\n",
            "Epoch: 2 | iter: 480 | train loss: 1842.8916015625\n",
            "Epoch: 2 | iter: 481 | train loss: 2303.3552246094\n",
            "Epoch: 2 | iter: 482 | train loss: 1988.6744384766\n",
            "Epoch: 2 | iter: 483 | train loss: 855.1436157227\n",
            "Epoch: 2 | iter: 484 | train loss: 884.4679565430\n",
            "Epoch: 2 | iter: 485 | train loss: 1177.0043945312\n",
            "Epoch: 2 | iter: 486 | train loss: 1163.2902832031\n",
            "Epoch: 2 | iter: 487 | train loss: 2013.9089355469\n",
            "Epoch: 2 | iter: 488 | train loss: 2018.9313964844\n",
            "Epoch: 2 | iter: 489 | train loss: 1576.6660156250\n",
            "Epoch: 2 | iter: 490 | train loss: 2113.1250000000\n",
            "Epoch: 2 | iter: 491 | train loss: 952.4306640625\n",
            "Epoch: 2 | iter: 492 | train loss: 1509.4157714844\n",
            "Epoch: 2 | iter: 493 | train loss: 1925.3088378906\n",
            "Epoch: 2 | iter: 494 | train loss: 960.1975097656\n",
            "Epoch: 2 | iter: 495 | train loss: 2332.8374023438\n",
            "Epoch: 2 | iter: 496 | train loss: 2164.5332031250\n",
            "Epoch: 2 | iter: 497 | train loss: 1584.9019775391\n",
            "Epoch: 2 | iter: 498 | train loss: 1892.9759521484\n",
            "Epoch: 2 | iter: 499 | train loss: 1581.4892578125\n",
            "Epoch: 2 | iter: 500 | train loss: 2350.8774414062\n",
            "Epoch: 2 | iter: 501 | train loss: 1896.3137207031\n",
            "Epoch: 2 | iter: 502 | train loss: 1322.9201660156\n",
            "Epoch: 2 | iter: 503 | train loss: 1501.5260009766\n",
            "Epoch: 2 | iter: 504 | train loss: 2410.0395507812\n",
            "Epoch: 2 | iter: 505 | train loss: 1208.2812500000\n",
            "Epoch: 2 | iter: 506 | train loss: 2354.7761230469\n",
            "Epoch: 2 | iter: 507 | train loss: 1445.9027099609\n",
            "Epoch: 2 | iter: 508 | train loss: 2001.8049316406\n",
            "Epoch: 2 | iter: 509 | train loss: 911.1137084961\n",
            "Epoch: 2 | iter: 510 | train loss: 2207.2749023438\n",
            "Epoch: 2 | iter: 511 | train loss: 1905.6459960938\n",
            "Epoch: 2 | iter: 512 | train loss: 1395.9843750000\n",
            "Epoch: 2 | iter: 513 | train loss: 1397.3741455078\n",
            "Epoch: 2 | iter: 514 | train loss: 2306.3125000000\n",
            "Epoch: 2 | iter: 515 | train loss: 2187.5302734375\n",
            "Epoch: 2 | iter: 516 | train loss: 1326.7791748047\n",
            "Epoch: 2 | iter: 517 | train loss: 1510.6026611328\n",
            "Epoch: 2 | iter: 518 | train loss: 1516.4067382812\n",
            "Epoch: 2 | iter: 519 | train loss: 1155.8514404297\n",
            "Epoch: 2 | iter: 520 | train loss: 1155.5169677734\n",
            "Epoch: 2 | iter: 521 | train loss: 1121.1865234375\n",
            "Epoch: 2 | iter: 522 | train loss: 1436.6585693359\n",
            "Epoch: 2 | iter: 523 | train loss: 1386.0272216797\n",
            "Epoch: 2 | iter: 524 | train loss: 1779.5040283203\n",
            "Epoch: 2 | iter: 525 | train loss: 1831.7888183594\n",
            "Epoch: 2 | iter: 526 | train loss: 1295.6058349609\n",
            "Epoch: 2 | iter: 527 | train loss: 497.3565063477\n",
            "Epoch: 2 | iter: 528 | train loss: 2332.9343261719\n",
            "Epoch: 2 | iter: 529 | train loss: 469.5608520508\n",
            "Epoch: 2 | iter: 530 | train loss: 1997.0496826172\n",
            "Epoch: 2 | iter: 531 | train loss: 1764.3770751953\n",
            "Epoch: 2 | iter: 532 | train loss: 1715.3087158203\n",
            "Epoch: 2 | iter: 533 | train loss: 1210.0087890625\n",
            "Epoch: 2 | iter: 534 | train loss: 2316.0085449219\n",
            "Epoch: 2 | iter: 535 | train loss: 1493.4617919922\n",
            "Epoch: 2 | iter: 536 | train loss: 1184.0566406250\n",
            "Epoch: 2 | iter: 537 | train loss: 932.9495239258\n",
            "Epoch: 2 | iter: 538 | train loss: 1292.5460205078\n",
            "Epoch: 2 | iter: 539 | train loss: 1441.7384033203\n",
            "Epoch: 2 | iter: 540 | train loss: 1347.3489990234\n",
            "Epoch: 2 | iter: 541 | train loss: 1149.8455810547\n",
            "Epoch: 2 | iter: 542 | train loss: 1781.5672607422\n",
            "Epoch: 2 | iter: 543 | train loss: 924.5159912109\n",
            "Epoch: 2 | iter: 544 | train loss: 2259.8708496094\n",
            "Epoch: 2 | iter: 545 | train loss: 2571.0852050781\n",
            "Epoch: 2 | iter: 546 | train loss: 971.4093017578\n",
            "Epoch: 2 | iter: 547 | train loss: 1992.8044433594\n",
            "Epoch: 2 | iter: 548 | train loss: 1132.1422119141\n",
            "Epoch: 2 | iter: 549 | train loss: 828.0501098633\n",
            "Epoch: 2 | iter: 550 | train loss: 1374.3724365234\n",
            "Epoch: 2 | iter: 551 | train loss: 1385.2299804688\n",
            "Epoch: 2 | iter: 552 | train loss: 2008.8199462891\n",
            "Epoch: 2 | iter: 553 | train loss: 1740.5626220703\n",
            "Epoch: 2 | iter: 554 | train loss: 1058.9576416016\n",
            "Epoch: 2 | iter: 555 | train loss: 867.1925048828\n",
            "Epoch: 2 | iter: 556 | train loss: 2641.6340332031\n",
            "Epoch: 2 | iter: 557 | train loss: 2348.3935546875\n",
            "Epoch: 2 | iter: 558 | train loss: 971.7349243164\n",
            "Epoch: 2 | iter: 559 | train loss: 1983.9022216797\n",
            "Epoch: 2 | iter: 560 | train loss: 1605.8565673828\n",
            "Epoch: 2 | iter: 561 | train loss: 1065.4240722656\n",
            "Epoch: 2 | iter: 562 | train loss: 1167.2581787109\n",
            "Epoch: 2 | iter: 563 | train loss: 1685.7441406250\n",
            "Epoch: 2 | iter: 564 | train loss: 1463.4492187500\n",
            "Epoch: 2 | iter: 565 | train loss: 1240.9788818359\n",
            "Epoch: 2 | iter: 566 | train loss: 1915.4528808594\n",
            "Epoch: 2 | iter: 567 | train loss: 1164.3280029297\n",
            "Epoch: 2 | iter: 568 | train loss: 1934.0456542969\n",
            "Epoch: 2 | iter: 569 | train loss: 679.8239746094\n",
            "Epoch: 2 | iter: 570 | train loss: 1753.4766845703\n",
            "Epoch: 2 | iter: 571 | train loss: 1874.1707763672\n",
            "Epoch: 2 | iter: 572 | train loss: 1529.3909912109\n",
            "Epoch: 2 | iter: 573 | train loss: 2947.4655761719\n",
            "Epoch: 2 | iter: 574 | train loss: 1310.3629150391\n",
            "Epoch: 2 | iter: 575 | train loss: 2115.4768066406\n",
            "Epoch: 2 | iter: 576 | train loss: 2072.1716308594\n",
            "Epoch: 2 | iter: 577 | train loss: 2105.5468750000\n",
            "Epoch: 2 | iter: 578 | train loss: 2535.7429199219\n",
            "Epoch: 2 | iter: 579 | train loss: 1944.3852539062\n",
            "Epoch: 2 | iter: 580 | train loss: 1381.0037841797\n",
            "Epoch: 2 | iter: 581 | train loss: 1108.6906738281\n",
            "Epoch: 2 | iter: 582 | train loss: 2570.0520019531\n",
            "Epoch: 2 | iter: 583 | train loss: 1208.0908203125\n",
            "Epoch: 2 | iter: 584 | train loss: 2079.0554199219\n",
            "Epoch: 2 | iter: 585 | train loss: 1270.7502441406\n",
            "Epoch: 2 | iter: 586 | train loss: 1684.4327392578\n",
            "Epoch: 2 | iter: 587 | train loss: 1024.9248046875\n",
            "Epoch: 2 | iter: 588 | train loss: 2078.9304199219\n",
            "Epoch: 2 | iter: 589 | train loss: 1197.3236083984\n",
            "Epoch: 2 | iter: 590 | train loss: 1647.4658203125\n",
            "Epoch: 2 | iter: 591 | train loss: 1623.8752441406\n",
            "Epoch: 2 | iter: 592 | train loss: 1842.8486328125\n",
            "Epoch: 2 | iter: 593 | train loss: 1485.6185302734\n",
            "Epoch: 2 | iter: 594 | train loss: 2423.7783203125\n",
            "Epoch: 2 | iter: 595 | train loss: 1469.6270751953\n",
            "Epoch: 2 | iter: 596 | train loss: 2070.9306640625\n",
            "Epoch: 2 | iter: 597 | train loss: 1761.9509277344\n",
            "Epoch: 2 | iter: 598 | train loss: 1980.2092285156\n",
            "Epoch: 2 | iter: 599 | train loss: 1906.2828369141\n",
            "Epoch: 2 | iter: 600 | train loss: 2028.5512695312\n",
            "Epoch: 2 | iter: 601 | train loss: 1879.2844238281\n",
            "Epoch: 2 | iter: 602 | train loss: 1011.6327514648\n",
            "Epoch: 2 | iter: 603 | train loss: 1014.5489501953\n",
            "Epoch: 2 | iter: 604 | train loss: 1509.2429199219\n",
            "Epoch: 2 | iter: 605 | train loss: 1758.4887695312\n",
            "Epoch: 2 | iter: 606 | train loss: 1047.6657714844\n",
            "Epoch: 2 | iter: 607 | train loss: 1252.2963867188\n",
            "Epoch: 2 | iter: 608 | train loss: 1902.5252685547\n",
            "Epoch: 2 | iter: 609 | train loss: 2125.2348632812\n",
            "Epoch: 2 | iter: 610 | train loss: 1367.5246582031\n",
            "Epoch: 2 | iter: 611 | train loss: 1575.0012207031\n",
            "Epoch: 2 | iter: 612 | train loss: 1781.0000000000\n",
            "Epoch: 2 | iter: 613 | train loss: 2186.5971679688\n",
            "Epoch: 2 | iter: 614 | train loss: 1224.8577880859\n",
            "Epoch: 2 | iter: 615 | train loss: 2566.1047363281\n",
            "Epoch: 2 | iter: 616 | train loss: 1322.4885253906\n",
            "Epoch: 2 | iter: 617 | train loss: 1352.7601318359\n",
            "Epoch: 2 | iter: 618 | train loss: 1097.5830078125\n",
            "Epoch: 2 | iter: 619 | train loss: 1164.5018310547\n",
            "Epoch: 2 | iter: 620 | train loss: 838.1063232422\n",
            "Epoch: 2 | iter: 621 | train loss: 883.4774169922\n",
            "Epoch: 2 | iter: 622 | train loss: 2047.3397216797\n",
            "Epoch: 2 | iter: 623 | train loss: 565.5661621094\n",
            "Epoch: 2 | iter: 624 | train loss: 1690.9953613281\n",
            "Epoch: 2 | iter: 625 | train loss: 1112.7374267578\n",
            "Epoch: 2 | iter: 626 | train loss: 2067.8640136719\n",
            "Epoch: 2 | iter: 627 | train loss: 1094.6531982422\n",
            "Epoch: 2 | iter: 628 | train loss: 2351.8559570312\n",
            "Epoch: 2 | iter: 629 | train loss: 2150.9389648438\n",
            "Epoch: 2 | iter: 630 | train loss: 1670.2236328125\n",
            "Epoch: 2 | iter: 631 | train loss: 1961.2434082031\n",
            "Epoch: 2 | iter: 632 | train loss: 1895.2786865234\n",
            "Epoch: 2 | iter: 633 | train loss: 1356.7113037109\n",
            "Epoch: 2 | iter: 634 | train loss: 1375.7027587891\n",
            "Epoch: 2 | iter: 635 | train loss: 1297.9935302734\n",
            "Epoch: 2 | iter: 636 | train loss: 777.3375244141\n",
            "Epoch: 2 | iter: 637 | train loss: 1759.0554199219\n",
            "Epoch: 2 | iter: 638 | train loss: 2859.6696777344\n",
            "Epoch: 2 | iter: 639 | train loss: 1208.6990966797\n",
            "Epoch: 2 | iter: 640 | train loss: 2038.6833496094\n",
            "Epoch: 2 | iter: 641 | train loss: 1503.7873535156\n",
            "Epoch: 2 | iter: 642 | train loss: 1615.9367675781\n",
            "Epoch: 2 | iter: 643 | train loss: 1733.2825927734\n",
            "Epoch: 2 | iter: 644 | train loss: 1260.8995361328\n",
            "Epoch: 2 | iter: 645 | train loss: 923.9435424805\n",
            "Epoch: 2 | iter: 646 | train loss: 2721.8581542969\n",
            "Epoch: 2 | iter: 647 | train loss: 917.1998901367\n",
            "Epoch: 2 | iter: 648 | train loss: 1475.0054931641\n",
            "Epoch: 2 | iter: 649 | train loss: 1081.9432373047\n",
            "Epoch: 2 | iter: 650 | train loss: 1863.9613037109\n",
            "Epoch: 2 | iter: 651 | train loss: 1423.0544433594\n",
            "Epoch: 2 | iter: 652 | train loss: 2140.1706542969\n",
            "Epoch: 2 | iter: 653 | train loss: 2050.1884765625\n",
            "Epoch: 2 | iter: 654 | train loss: 1513.8310546875\n",
            "Epoch: 2 | iter: 655 | train loss: 1185.4254150391\n",
            "Epoch: 2 | iter: 656 | train loss: 1753.6478271484\n",
            "Epoch: 2 | iter: 657 | train loss: 1483.0477294922\n",
            "Epoch: 2 | iter: 658 | train loss: 1659.7233886719\n",
            "Epoch: 2 | iter: 659 | train loss: 1561.2309570312\n",
            "Epoch: 2 | iter: 660 | train loss: 1582.1966552734\n",
            "Epoch: 2 | iter: 661 | train loss: 2096.8542480469\n",
            "Epoch: 2 | iter: 662 | train loss: 1721.0208740234\n",
            "Epoch: 2 | iter: 663 | train loss: 655.2610473633\n",
            "Epoch: 2 | iter: 664 | train loss: 1753.3845214844\n",
            "Epoch: 2 | iter: 665 | train loss: 2000.0803222656\n",
            "Epoch: 2 | iter: 666 | train loss: 1311.6589355469\n",
            "Epoch: 2 | iter: 667 | train loss: 1205.6652832031\n",
            "Epoch: 2 | iter: 668 | train loss: 1302.6232910156\n",
            "Epoch: 2 | iter: 669 | train loss: 1982.8547363281\n",
            "Epoch: 2 | iter: 670 | train loss: 1518.8181152344\n",
            "Epoch: 2 | iter: 671 | train loss: 1670.0054931641\n",
            "Epoch: 2 | iter: 672 | train loss: 949.7193603516\n",
            "Epoch: 2 | iter: 673 | train loss: 1039.4434814453\n",
            "Epoch: 2 | iter: 674 | train loss: 1023.5516967773\n",
            "Epoch: 2 | iter: 675 | train loss: 1685.9001464844\n",
            "Epoch: 2 | iter: 676 | train loss: 952.2356567383\n",
            "Epoch: 2 | iter: 677 | train loss: 1506.9678955078\n",
            "Epoch: 2 | iter: 678 | train loss: 436.3793945312\n",
            "Epoch: 2 | iter: 679 | train loss: 1144.6577148438\n",
            "Epoch: 2 | iter: 680 | train loss: 955.9752807617\n",
            "Epoch: 2 | iter: 681 | train loss: 882.8477172852\n",
            "Epoch: 2 | iter: 682 | train loss: 1875.9267578125\n",
            "Epoch: 2 | iter: 683 | train loss: 899.2288208008\n",
            "Epoch: 2 | iter: 684 | train loss: 1474.0093994141\n",
            "Epoch: 2 | iter: 685 | train loss: 514.4873046875\n",
            "Epoch: 2 | iter: 686 | train loss: 1158.6054687500\n",
            "Epoch: 2 | iter: 687 | train loss: 1874.2757568359\n",
            "Epoch: 2 | iter: 688 | train loss: 1769.6242675781\n",
            "Epoch: 2 | iter: 689 | train loss: 1244.0654296875\n",
            "Epoch: 2 | iter: 690 | train loss: 798.5256347656\n",
            "Epoch: 2 | iter: 691 | train loss: 1405.3188476562\n",
            "Epoch: 2 | iter: 692 | train loss: 2451.4240722656\n",
            "Epoch: 2 | iter: 693 | train loss: 771.0348510742\n",
            "Epoch: 2 | iter: 694 | train loss: 1416.3132324219\n",
            "Epoch: 2 | iter: 695 | train loss: 1168.7170410156\n",
            "Epoch: 2 | iter: 696 | train loss: 1095.8751220703\n",
            "Epoch: 2 | iter: 697 | train loss: 978.6226806641\n",
            "Epoch: 2 | iter: 698 | train loss: 1078.7706298828\n",
            "Epoch: 2 | iter: 699 | train loss: 1446.4329833984\n",
            "Epoch: 2 | iter: 700 | train loss: 1871.1881103516\n",
            "Epoch: 2 | iter: 701 | train loss: 1726.3399658203\n",
            "Epoch: 2 | iter: 702 | train loss: 1639.8238525391\n",
            "Epoch: 2 | iter: 703 | train loss: 1505.2014160156\n",
            "Epoch: 2 | iter: 704 | train loss: 766.4309082031\n",
            "Epoch: 2 | iter: 705 | train loss: 1486.4871826172\n",
            "Epoch: 2 | iter: 706 | train loss: 952.7077026367\n",
            "Epoch: 2 | iter: 707 | train loss: 1163.8138427734\n",
            "Epoch: 2 | iter: 708 | train loss: 1651.1660156250\n",
            "Epoch: 2 | iter: 709 | train loss: 1475.6311035156\n",
            "Epoch: 2 | iter: 710 | train loss: 1955.8258056641\n",
            "Epoch: 2 | iter: 711 | train loss: 794.6149291992\n",
            "Epoch: 2 | iter: 712 | train loss: 1986.2581787109\n",
            "Epoch: 2 | iter: 713 | train loss: 1056.9034423828\n",
            "Epoch: 2 | iter: 714 | train loss: 475.2217712402\n",
            "Epoch: 2 | iter: 715 | train loss: 1687.1879882812\n",
            "Epoch: 2 | iter: 716 | train loss: 1513.0999755859\n",
            "Epoch: 2 | iter: 717 | train loss: 956.0662231445\n",
            "Epoch: 2 | iter: 718 | train loss: 328.2121887207\n",
            "Epoch: 2 | iter: 719 | train loss: 961.6267089844\n",
            "Epoch: 2 | iter: 720 | train loss: 920.8563842773\n",
            "Epoch: 2 | iter: 721 | train loss: 1712.0487060547\n",
            "Epoch: 2 | iter: 722 | train loss: 1931.4466552734\n",
            "Epoch: 2 | iter: 723 | train loss: 1139.7365722656\n",
            "Epoch: 2 | iter: 724 | train loss: 1015.1823730469\n",
            "Epoch: 2 | iter: 725 | train loss: 1454.3400878906\n",
            "Epoch: 2 | iter: 726 | train loss: 1296.6315917969\n",
            "Epoch: 2 | iter: 727 | train loss: 1640.8427734375\n",
            "Epoch: 2 | iter: 728 | train loss: 2022.7788085938\n",
            "Epoch: 2 | iter: 729 | train loss: 1238.7735595703\n",
            "Epoch: 2 | iter: 730 | train loss: 954.5715942383\n",
            "Epoch: 2 | iter: 731 | train loss: 836.8770141602\n",
            "Epoch: 2 | iter: 732 | train loss: 1353.4468994141\n",
            "Epoch: 2 | iter: 733 | train loss: 1602.6281738281\n",
            "Epoch: 2 | iter: 734 | train loss: 1682.6352539062\n",
            "Epoch: 2 | iter: 735 | train loss: 1533.0816650391\n",
            "Epoch: 2 | iter: 736 | train loss: 2415.1870117188\n",
            "Epoch: 2 | iter: 737 | train loss: 1349.6169433594\n",
            "Epoch: 2 | iter: 738 | train loss: 1152.9033203125\n",
            "Epoch: 2 | iter: 739 | train loss: 1511.4490966797\n",
            "Epoch: 2 | iter: 740 | train loss: 1103.7927246094\n",
            "Epoch: 2 | iter: 741 | train loss: 1696.9772949219\n",
            "Epoch: 2 | iter: 742 | train loss: 1527.7653808594\n",
            "Epoch: 2 | iter: 743 | train loss: 1631.9020996094\n",
            "Epoch: 2 | iter: 744 | train loss: 1207.0456542969\n",
            "Epoch: 2 | iter: 745 | train loss: 819.7945556641\n",
            "Epoch: 2 | iter: 746 | train loss: 1260.0740966797\n",
            "Epoch: 2 | iter: 747 | train loss: 1752.7443847656\n",
            "Epoch: 2 | iter: 748 | train loss: 1522.7667236328\n",
            "Epoch: 2 | iter: 749 | train loss: 2102.8894042969\n",
            "Epoch: 2 | iter: 750 | train loss: 1952.4946289062\n",
            "Epoch: 2 | iter: 751 | train loss: 1076.8521728516\n",
            "Epoch: 2 | iter: 752 | train loss: 1974.1821289062\n",
            "Epoch: 2 | iter: 753 | train loss: 1681.9432373047\n",
            "Epoch: 2 | iter: 754 | train loss: 1847.8392333984\n",
            "Epoch: 2 | iter: 755 | train loss: 1637.4013671875\n",
            "Epoch: 2 | iter: 756 | train loss: 1592.2684326172\n",
            "Epoch: 2 | iter: 757 | train loss: 1968.2315673828\n",
            "Epoch: 2 | iter: 758 | train loss: 2108.8793945312\n",
            "Epoch: 2 | iter: 759 | train loss: 1404.8500976562\n",
            "Epoch: 2 | iter: 760 | train loss: 1668.6589355469\n",
            "Epoch: 2 | iter: 761 | train loss: 1227.1124267578\n",
            "Epoch: 2 | iter: 762 | train loss: 1275.7720947266\n",
            "Epoch: 2 | iter: 763 | train loss: 1603.6192626953\n",
            "Epoch: 2 | iter: 764 | train loss: 1587.7707519531\n",
            "Epoch: 2 | iter: 765 | train loss: 1976.9697265625\n",
            "Epoch: 2 | iter: 766 | train loss: 2048.1645507812\n",
            "Epoch: 2 | iter: 767 | train loss: 1085.9293212891\n",
            "Epoch: 2 | iter: 768 | train loss: 1237.6357421875\n",
            "Epoch: 2 | iter: 769 | train loss: 2037.9809570312\n",
            "Epoch: 2 | iter: 770 | train loss: 1727.7387695312\n",
            "Epoch: 2 | iter: 771 | train loss: 2052.2329101562\n",
            "Epoch: 2 | iter: 772 | train loss: 912.6659545898\n",
            "Epoch: 2 | iter: 773 | train loss: 2478.7795410156\n",
            "Epoch: 2 | iter: 774 | train loss: 1575.0142822266\n",
            "Epoch: 2 | iter: 775 | train loss: 1445.5557861328\n",
            "Epoch: 2 | iter: 776 | train loss: 2349.9885253906\n",
            "Epoch: 2 | iter: 777 | train loss: 2437.9233398438\n",
            "Epoch: 2 | iter: 778 | train loss: 2588.9152832031\n",
            "Epoch: 2 | iter: 779 | train loss: 1755.5863037109\n",
            "Epoch: 2 | iter: 780 | train loss: 2266.1940917969\n",
            "Epoch: 2 | iter: 781 | train loss: 1665.1646728516\n",
            "Epoch: 2 | iter: 782 | train loss: 1534.4785156250\n",
            "Epoch: 2 | iter: 783 | train loss: 2158.7673339844\n",
            "Epoch: 2 | iter: 784 | train loss: 2544.0783691406\n",
            "Epoch: 2 | iter: 785 | train loss: 1262.6270751953\n",
            "Epoch: 2 | iter: 786 | train loss: 2757.0839843750\n",
            "Epoch: 2 | iter: 787 | train loss: 1746.7893066406\n",
            "Epoch: 2 | iter: 788 | train loss: 1027.9155273438\n",
            "Epoch: 2 | iter: 789 | train loss: 2241.6630859375\n",
            "Epoch: 2 | iter: 790 | train loss: 2215.3928222656\n",
            "Epoch: 2 | iter: 791 | train loss: 2290.2512207031\n",
            "Epoch: 2 | iter: 792 | train loss: 2409.8369140625\n",
            "Epoch: 2 | iter: 793 | train loss: 1820.8140869141\n",
            "Epoch: 2 | iter: 794 | train loss: 1298.4157714844\n",
            "Epoch: 2 | iter: 795 | train loss: 1810.4611816406\n",
            "Epoch: 2 | iter: 796 | train loss: 2120.2314453125\n",
            "Epoch: 2 | iter: 797 | train loss: 1641.0396728516\n",
            "Epoch: 2 | iter: 798 | train loss: 962.3735961914\n",
            "Epoch: 2 | iter: 799 | train loss: 480.3815002441\n",
            "Epoch: 2 | iter: 800 | train loss: 1539.3502197266\n",
            "Epoch: 2 | iter: 801 | train loss: 942.9140625000\n",
            "Epoch: 2 | iter: 802 | train loss: 1449.5023193359\n",
            "Epoch: 2 | iter: 803 | train loss: 2390.8281250000\n",
            "Epoch: 2 | iter: 804 | train loss: 1786.0172119141\n",
            "Epoch: 2 | iter: 805 | train loss: 3041.4953613281\n",
            "Epoch: 2 | iter: 806 | train loss: 1319.8552246094\n",
            "Epoch: 2 | iter: 807 | train loss: 2750.8300781250\n",
            "Epoch: 2 | iter: 808 | train loss: 2604.0322265625\n",
            "Epoch: 2 | iter: 809 | train loss: 1948.4862060547\n",
            "Epoch: 2 | iter: 810 | train loss: 869.2975463867\n",
            "Epoch: 2 | iter: 811 | train loss: 1797.0694580078\n",
            "Epoch: 2 | iter: 812 | train loss: 2503.6369628906\n",
            "Epoch: 2 | iter: 813 | train loss: 1493.1683349609\n",
            "Epoch: 2 | iter: 814 | train loss: 2670.8806152344\n",
            "Epoch: 2 | iter: 815 | train loss: 1575.5305175781\n",
            "Epoch: 2 | iter: 816 | train loss: 2157.7802734375\n",
            "Epoch: 2 | iter: 817 | train loss: 942.7785644531\n",
            "Epoch: 2 | iter: 818 | train loss: 2292.1250000000\n",
            "Epoch: 2 | iter: 819 | train loss: 2350.0620117188\n",
            "Epoch: 2 | iter: 820 | train loss: 1358.3592529297\n",
            "Epoch: 2 | iter: 821 | train loss: 1542.7220458984\n",
            "Epoch: 2 | iter: 822 | train loss: 1964.5640869141\n",
            "Epoch: 2 | iter: 823 | train loss: 1236.4385986328\n",
            "Epoch: 2 | iter: 824 | train loss: 1211.2960205078\n",
            "Epoch: 2 | iter: 825 | train loss: 1368.4663085938\n",
            "Epoch: 2 | iter: 826 | train loss: 1490.2077636719\n",
            "Epoch: 2 | iter: 827 | train loss: 854.3596801758\n",
            "Epoch: 2 | iter: 828 | train loss: 2817.7937011719\n",
            "Epoch: 2 | iter: 829 | train loss: 911.9501953125\n",
            "Epoch: 2 | iter: 830 | train loss: 972.7258911133\n",
            "Epoch: 2 | iter: 831 | train loss: 1937.1909179688\n",
            "Epoch: 2 | iter: 832 | train loss: 822.7726440430\n",
            "Epoch: 2 | iter: 833 | train loss: 1848.4125976562\n",
            "Epoch: 2 | iter: 834 | train loss: 1497.5772705078\n",
            "Epoch: 2 | iter: 835 | train loss: 2678.6044921875\n",
            "Epoch: 2 | iter: 836 | train loss: 1902.7087402344\n",
            "Epoch: 2 | iter: 837 | train loss: 2234.4299316406\n",
            "Epoch: 2 | iter: 838 | train loss: 2302.5563964844\n",
            "Epoch: 2 | iter: 839 | train loss: 2859.1472167969\n",
            "Epoch: 2 | iter: 840 | train loss: 2305.1499023438\n",
            "Epoch: 2 | iter: 841 | train loss: 2453.9936523438\n",
            "Epoch: 2 | iter: 842 | train loss: 2530.6193847656\n",
            "Epoch: 2 | iter: 843 | train loss: 2431.2702636719\n",
            "Epoch: 2 | iter: 844 | train loss: 1918.8559570312\n",
            "Epoch: 2 | iter: 845 | train loss: 1296.9222412109\n",
            "Epoch: 2 | iter: 846 | train loss: 2764.9282226562\n",
            "Epoch: 2 | iter: 847 | train loss: 1346.0895996094\n",
            "Epoch: 2 | iter: 848 | train loss: 1339.6302490234\n",
            "Epoch: 2 | iter: 849 | train loss: 2514.0478515625\n",
            "Epoch: 2 | iter: 850 | train loss: 945.5307006836\n",
            "Epoch: 2 | iter: 851 | train loss: 1797.4139404297\n",
            "Epoch: 2 | iter: 852 | train loss: 1543.5206298828\n",
            "Epoch: 2 | iter: 853 | train loss: 1659.5603027344\n",
            "Epoch: 2 | iter: 854 | train loss: 1801.1582031250\n",
            "Epoch: 2 | iter: 855 | train loss: 2317.8374023438\n",
            "Epoch: 2 | iter: 856 | train loss: 2395.5212402344\n",
            "Epoch: 2 | iter: 857 | train loss: 1829.7489013672\n",
            "Epoch: 2 | iter: 858 | train loss: 1472.5576171875\n",
            "Epoch: 2 | iter: 859 | train loss: 2828.3454589844\n",
            "Epoch: 2 | iter: 860 | train loss: 1687.6033935547\n",
            "Epoch: 2 | iter: 861 | train loss: 2473.5751953125\n",
            "Epoch: 2 | iter: 862 | train loss: 1993.4825439453\n",
            "Epoch: 2 | iter: 863 | train loss: 1543.4062500000\n",
            "Epoch: 2 | iter: 864 | train loss: 2762.0683593750\n",
            "Epoch: 2 | iter: 865 | train loss: 1660.7316894531\n",
            "Epoch: 2 | iter: 866 | train loss: 2211.7912597656\n",
            "Epoch: 2 | iter: 867 | train loss: 2106.9033203125\n",
            "Epoch: 2 | iter: 868 | train loss: 1798.7564697266\n",
            "Epoch: 2 | iter: 869 | train loss: 1871.4444580078\n",
            "Epoch: 2 | iter: 870 | train loss: 2034.2432861328\n",
            "Epoch: 2 | iter: 871 | train loss: 1223.2135009766\n",
            "Epoch: 2 | iter: 872 | train loss: 1286.5345458984\n",
            "Epoch: 2 | iter: 873 | train loss: 1194.1748046875\n",
            "Epoch: 2 | iter: 874 | train loss: 1885.8349609375\n",
            "Epoch: 2 | iter: 875 | train loss: 1748.9475097656\n",
            "Epoch: 2 | iter: 876 | train loss: 1593.5253906250\n",
            "Epoch: 2 | iter: 877 | train loss: 1287.5561523438\n",
            "Epoch: 2 | iter: 878 | train loss: 837.0396118164\n",
            "Epoch: 2 | iter: 879 | train loss: 1003.1019897461\n",
            "Epoch: 2 | iter: 880 | train loss: 1637.6683349609\n",
            "Epoch: 2 | iter: 881 | train loss: 986.7330322266\n",
            "Epoch: 2 | iter: 882 | train loss: 1279.0148925781\n",
            "Epoch: 2 | iter: 883 | train loss: 2082.1289062500\n",
            "Epoch: 2 | iter: 884 | train loss: 1598.3370361328\n",
            "Epoch: 2 | iter: 885 | train loss: 697.0212402344\n",
            "Epoch: 2 | iter: 886 | train loss: 1224.5288085938\n",
            "Epoch: 2 | iter: 887 | train loss: 1425.3082275391\n",
            "Epoch: 2 | iter: 888 | train loss: 1628.4099121094\n",
            "Epoch: 2 | iter: 889 | train loss: 972.3611450195\n",
            "Epoch: 2 | iter: 890 | train loss: 2023.4222412109\n",
            "Epoch: 2 | iter: 891 | train loss: 2043.4053955078\n",
            "Epoch: 2 | iter: 892 | train loss: 1749.2326660156\n",
            "Epoch: 2 | iter: 893 | train loss: 1777.3709716797\n",
            "Epoch: 2 | iter: 894 | train loss: 2008.3333740234\n",
            "Epoch: 2 | iter: 895 | train loss: 1990.2263183594\n",
            "Epoch: 2 | iter: 896 | train loss: 1475.4891357422\n",
            "Epoch: 2 | iter: 897 | train loss: 1346.4603271484\n",
            "Epoch: 2 | iter: 898 | train loss: 1674.6082763672\n",
            "Epoch: 2 | iter: 899 | train loss: 2508.8171386719\n",
            "Epoch: 2 | iter: 900 | train loss: 1290.7969970703\n",
            "Epoch: 2 | iter: 901 | train loss: 1454.6325683594\n",
            "Epoch: 2 | iter: 902 | train loss: 1960.2471923828\n",
            "Epoch: 2 | iter: 903 | train loss: 1089.8529052734\n",
            "Epoch: 2 | iter: 904 | train loss: 1185.0106201172\n",
            "Epoch: 2 | iter: 905 | train loss: 2038.8481445312\n",
            "Epoch: 2 | iter: 906 | train loss: 2259.5749511719\n",
            "Epoch: 2 | iter: 907 | train loss: 1070.9739990234\n",
            "Epoch: 2 | iter: 908 | train loss: 2194.2990722656\n",
            "Epoch: 2 | iter: 909 | train loss: 2371.1262207031\n",
            "Epoch: 2 | iter: 910 | train loss: 2226.2458496094\n",
            "Epoch: 2 | iter: 911 | train loss: 1376.1846923828\n",
            "Epoch: 2 | iter: 912 | train loss: 2421.9599609375\n",
            "Epoch: 2 | iter: 913 | train loss: 2593.5397949219\n",
            "Epoch: 2 | iter: 914 | train loss: 2166.7338867188\n",
            "Epoch: 2 | iter: 915 | train loss: 2353.8342285156\n",
            "Epoch: 2 | iter: 916 | train loss: 1115.4069824219\n",
            "Epoch: 2 | iter: 917 | train loss: 2449.1555175781\n",
            "Epoch: 2 | iter: 918 | train loss: 2234.6086425781\n",
            "Epoch: 2 | iter: 919 | train loss: 2118.4895019531\n",
            "Epoch: 2 | iter: 920 | train loss: 2056.9475097656\n",
            "Epoch: 2 | iter: 921 | train loss: 2856.3063964844\n",
            "Epoch: 2 | iter: 922 | train loss: 1077.4704589844\n",
            "Epoch: 2 | iter: 923 | train loss: 894.9647827148\n",
            "Epoch: 2 | iter: 924 | train loss: 2405.8005371094\n",
            "Epoch: 2 | iter: 925 | train loss: 1340.2515869141\n",
            "Epoch: 2 | iter: 926 | train loss: 1065.9733886719\n",
            "Epoch: 2 | iter: 927 | train loss: 1980.9393310547\n",
            "Epoch: 2 | iter: 928 | train loss: 1143.0979003906\n",
            "Epoch: 2 | iter: 929 | train loss: 1885.7952880859\n",
            "Epoch: 2 | iter: 930 | train loss: 1347.5502929688\n",
            "Epoch: 2 | iter: 931 | train loss: 2216.5917968750\n",
            "Epoch: 2 | iter: 932 | train loss: 2081.9309082031\n",
            "Epoch: 2 | iter: 933 | train loss: 1697.2910156250\n",
            "Epoch: 2 | iter: 934 | train loss: 2505.0590820312\n",
            "Epoch: 2 | iter: 935 | train loss: 2297.7751464844\n",
            "Epoch: 2 | iter: 936 | train loss: 1700.6406250000\n",
            "Epoch: 2 | iter: 937 | train loss: 1336.0815429688\n",
            "Epoch: 2 | iter: 938 | train loss: 2816.3737792969\n",
            "Epoch: 2 | iter: 939 | train loss: 1535.2541503906\n",
            "Epoch: 2 | iter: 940 | train loss: 2179.5090332031\n",
            "Epoch: 2 | iter: 941 | train loss: 2069.2888183594\n",
            "Epoch: 2 | iter: 942 | train loss: 2351.2946777344\n",
            "Epoch: 2 | iter: 943 | train loss: 1839.2971191406\n",
            "Epoch: 2 | iter: 944 | train loss: 2290.0493164062\n",
            "Epoch: 2 | iter: 945 | train loss: 2474.9560546875\n",
            "Epoch: 2 | iter: 946 | train loss: 2464.9799804688\n",
            "Epoch: 2 | iter: 947 | train loss: 2154.4934082031\n",
            "Epoch: 2 | iter: 948 | train loss: 2410.4758300781\n",
            "Epoch: 2 | iter: 949 | train loss: 2417.0310058594\n",
            "Epoch: 2 | iter: 950 | train loss: 2660.7829589844\n",
            "Epoch: 2 | iter: 951 | train loss: 2046.6970214844\n",
            "Epoch: 2 | iter: 952 | train loss: 2375.8618164062\n",
            "Epoch: 2 | iter: 953 | train loss: 2398.6306152344\n",
            "Epoch: 2 | iter: 954 | train loss: 2104.5891113281\n",
            "Epoch: 2 | iter: 955 | train loss: 2455.0861816406\n",
            "Epoch: 2 | iter: 956 | train loss: 2380.6606445312\n",
            "Epoch: 2 | iter: 957 | train loss: 2380.4160156250\n",
            "Epoch: 2 | iter: 958 | train loss: 2521.0605468750\n",
            "Epoch: 2 | iter: 959 | train loss: 1918.6141357422\n",
            "Epoch: 2 | iter: 960 | train loss: 2593.1826171875\n",
            "Epoch: 2 | iter: 961 | train loss: 2043.8798828125\n",
            "Epoch: 2 | iter: 962 | train loss: 3054.6821289062\n",
            "Epoch: 2 | iter: 963 | train loss: 2283.6567382812\n",
            "Epoch: 2 | iter: 964 | train loss: 2071.8339843750\n",
            "Epoch: 2 | iter: 965 | train loss: 3348.2753906250\n",
            "Epoch: 2 | iter: 966 | train loss: 2657.3901367188\n",
            "Epoch: 2 | iter: 967 | train loss: 2060.9545898438\n",
            "Epoch: 2 | iter: 968 | train loss: 2254.6096191406\n",
            "Epoch: 2 | iter: 969 | train loss: 2758.5866699219\n",
            "Epoch: 2 | iter: 970 | train loss: 2469.4792480469\n",
            "Epoch: 2 | iter: 971 | train loss: 2306.6904296875\n",
            "Epoch: 2 | iter: 972 | train loss: 3325.6760253906\n",
            "Epoch: 2 | iter: 973 | train loss: 2323.3957519531\n",
            "Epoch: 2 | iter: 974 | train loss: 2823.0517578125\n",
            "Epoch: 2 | iter: 975 | train loss: 2503.0727539062\n",
            "Epoch: 2 | iter: 976 | train loss: 1704.2882080078\n",
            "Epoch: 2 | iter: 977 | train loss: 2522.3471679688\n",
            "Epoch: 2 | iter: 978 | train loss: 2362.3984375000\n",
            "Epoch: 2 | iter: 979 | train loss: 3047.8466796875\n",
            "Epoch: 2 | iter: 980 | train loss: 2556.9477539062\n",
            "Epoch: 2 | iter: 981 | train loss: 2965.5874023438\n",
            "Epoch: 2 | iter: 982 | train loss: 2463.5434570312\n",
            "Epoch: 2 | iter: 983 | train loss: 2494.4763183594\n",
            "Epoch: 2 | iter: 984 | train loss: 3054.6301269531\n",
            "Epoch: 2 | iter: 985 | train loss: 3293.4379882812\n",
            "Epoch: 2 | iter: 986 | train loss: 2741.7583007812\n",
            "Epoch: 2 | iter: 987 | train loss: 2062.6684570312\n",
            "Epoch: 2 | iter: 988 | train loss: 3241.0839843750\n",
            "Epoch: 2 | iter: 989 | train loss: 2589.3054199219\n",
            "Epoch: 2 | iter: 990 | train loss: 3062.3005371094\n",
            "Epoch: 2 | iter: 991 | train loss: 2622.9907226562\n",
            "Epoch: 2 | iter: 992 | train loss: 2854.4001464844\n",
            "Epoch: 2 | iter: 993 | train loss: 2557.9909667969\n",
            "Epoch: 2 | iter: 994 | train loss: 2780.8630371094\n",
            "Epoch: 2 | iter: 995 | train loss: 2728.7768554688\n",
            "Epoch: 2 | iter: 996 | train loss: 2180.5046386719\n",
            "Epoch: 2 | iter: 997 | train loss: 3256.2893066406\n",
            "Epoch: 2 | iter: 998 | train loss: 2502.0197753906\n",
            "Epoch: 2 | iter: 999 | train loss: 2876.7583007812\n",
            "Epoch: 2 | iter: 1000 | train loss: 2341.5859375000\n",
            "Epoch: 2 | iter: 1001 | train loss: 3025.5720214844\n",
            "Epoch: 2 | iter: 1002 | train loss: 2627.0856933594\n",
            "Epoch: 2 | iter: 1003 | train loss: 2668.4448242188\n",
            "Epoch: 2 | iter: 1004 | train loss: 3073.7414550781\n",
            "Epoch: 2 | iter: 1005 | train loss: 2771.2092285156\n",
            "Epoch: 2 | iter: 1006 | train loss: 2770.3991699219\n",
            "Epoch: 2 | iter: 1007 | train loss: 2736.3078613281\n",
            "Epoch: 2 | iter: 1008 | train loss: 3514.0920410156\n",
            "Epoch: 2 | iter: 1009 | train loss: 2935.7685546875\n",
            "Epoch: 2 | iter: 1010 | train loss: 2942.9584960938\n",
            "Epoch: 2 | iter: 1011 | train loss: 3216.8183593750\n",
            "Epoch: 2 | iter: 1012 | train loss: 3115.1228027344\n",
            "Epoch: 2 | iter: 1013 | train loss: 2487.8859863281\n",
            "Epoch: 2 | iter: 1014 | train loss: 2748.2597656250\n",
            "Epoch: 2 | iter: 1015 | train loss: 2589.9602050781\n",
            "Epoch: 2 | iter: 1016 | train loss: 2905.7170410156\n",
            "Epoch: 2 | iter: 1017 | train loss: 3468.8735351562\n",
            "Epoch: 2 | iter: 1018 | train loss: 3338.9865722656\n",
            "Epoch: 2 | iter: 1019 | train loss: 3092.0925292969\n",
            "Epoch: 2 | iter: 1020 | train loss: 2405.3205566406\n",
            "Epoch: 2 | iter: 1021 | train loss: 2973.8884277344\n",
            "Epoch: 2 | iter: 1022 | train loss: 3502.2258300781\n",
            "Epoch: 2 | iter: 1023 | train loss: 2950.6188964844\n",
            "Epoch: 2 | iter: 1024 | train loss: 2980.8933105469\n",
            "Epoch: 2 | iter: 1025 | train loss: 3247.7644042969\n",
            "Epoch: 2 | iter: 1026 | train loss: 3211.3215332031\n",
            "Epoch: 2 | iter: 1027 | train loss: 3024.3215332031\n",
            "Epoch: 2 | iter: 1028 | train loss: 3297.8718261719\n",
            "Epoch: 2 | iter: 1029 | train loss: 3266.5363769531\n",
            "Epoch: 2 | iter: 1030 | train loss: 3080.0939941406\n",
            "Epoch: 2 | iter: 1031 | train loss: 3409.6323242188\n",
            "Epoch: 2 | iter: 1032 | train loss: 3506.4555664062\n",
            "Epoch: 2 | iter: 1033 | train loss: 3514.5129394531\n",
            "Epoch: 2 | iter: 1034 | train loss: 2740.8210449219\n",
            "Epoch: 2 | iter: 1035 | train loss: 3222.1928710938\n",
            "Epoch: 2 | iter: 1036 | train loss: 3378.2543945312\n",
            "Epoch: 2 | iter: 1037 | train loss: 2663.3789062500\n",
            "Epoch: 2 | iter: 1038 | train loss: 3259.8442382812\n",
            "Epoch: 2 | iter: 1039 | train loss: 2898.8356933594\n",
            "Epoch: 2 | iter: 1040 | train loss: 2936.6391601562\n",
            "Epoch: 2 | iter: 1041 | train loss: 3278.3806152344\n",
            "Epoch: 2 | iter: 1042 | train loss: 3292.6306152344\n",
            "Epoch: 2 | iter: 1043 | train loss: 3397.2580566406\n",
            "Epoch: 2 | iter: 1044 | train loss: 2322.4375000000\n",
            "Epoch: 2 | iter: 1045 | train loss: 3404.5991210938\n",
            "Epoch: 2 | iter: 1046 | train loss: 3177.6726074219\n",
            "Epoch: 2 | iter: 1047 | train loss: 3189.8830566406\n",
            "Epoch: 2 | iter: 1048 | train loss: 3574.6508789062\n",
            "Epoch: 2 | iter: 1049 | train loss: 2571.5739746094\n",
            "Epoch: 2 | iter: 1050 | train loss: 3207.9367675781\n",
            "Epoch: 2 | iter: 1051 | train loss: 3221.1655273438\n",
            "Epoch: 2 | iter: 1052 | train loss: 2789.0637207031\n",
            "Epoch: 2 | iter: 1053 | train loss: 3042.5568847656\n",
            "Epoch: 2 | iter: 1054 | train loss: 3353.3908691406\n",
            "Epoch: 2 | iter: 1055 | train loss: 3489.6967773438\n",
            "Epoch: 2 | iter: 1056 | train loss: 2777.3046875000\n",
            "Epoch: 2 | iter: 1057 | train loss: 3649.5051269531\n",
            "Epoch: 2 | iter: 1058 | train loss: 2888.2563476562\n",
            "Epoch: 2 | iter: 1059 | train loss: 3613.3632812500\n",
            "Epoch: 2 | iter: 1060 | train loss: 3561.1499023438\n",
            "Epoch: 2 | iter: 1061 | train loss: 3931.8044433594\n",
            "Epoch: 2 | iter: 1062 | train loss: 3263.2050781250\n",
            "Epoch: 2 | iter: 1063 | train loss: 2935.3798828125\n",
            "Epoch: 2 | iter: 1064 | train loss: 2436.7497558594\n",
            "Epoch: 2 | iter: 1065 | train loss: 3125.1022949219\n",
            "Epoch: 2 | iter: 1066 | train loss: 2699.5053710938\n",
            "Epoch: 2 | iter: 1067 | train loss: 2552.4580078125\n",
            "Epoch: 2 | iter: 1068 | train loss: 2597.5356445312\n",
            "Epoch: 2 | iter: 1069 | train loss: 1865.9455566406\n",
            "Epoch: 2 | iter: 1070 | train loss: 2729.9060058594\n",
            "Epoch: 2 | iter: 1071 | train loss: 1693.1619873047\n",
            "Epoch: 2 | iter: 1072 | train loss: 2074.6936035156\n",
            "Epoch: 2 | iter: 1073 | train loss: 2729.7956542969\n",
            "Epoch: 2 | iter: 1074 | train loss: 2424.6437988281\n",
            "Epoch: 2 | iter: 1075 | train loss: 2543.2817382812\n",
            "Epoch: 2 | iter: 1076 | train loss: 2108.1096191406\n",
            "Epoch: 2 | iter: 1077 | train loss: 2002.5808105469\n",
            "Epoch: 2 | iter: 1078 | train loss: 2137.4387207031\n",
            "Epoch: 2 | iter: 1079 | train loss: 2837.7719726562\n",
            "Epoch: 2 | iter: 1080 | train loss: 2079.1547851562\n",
            "Epoch: 2 | iter: 1081 | train loss: 2488.9885253906\n",
            "Epoch: 2 | iter: 1082 | train loss: 2829.9450683594\n",
            "Epoch: 2 | iter: 1083 | train loss: 2738.7875976562\n",
            "Epoch: 2 | iter: 1084 | train loss: 2542.5092773438\n",
            "Epoch: 2 | iter: 1085 | train loss: 2118.0788574219\n",
            "Epoch: 2 | iter: 1086 | train loss: 2739.6557617188\n",
            "Epoch: 2 | iter: 1087 | train loss: 2301.5627441406\n",
            "Epoch: 2 | iter: 1088 | train loss: 2316.1037597656\n",
            "Epoch: 2 | iter: 1089 | train loss: 1928.7072753906\n",
            "Epoch: 2 | iter: 1090 | train loss: 2714.6081542969\n",
            "Epoch: 2 | iter: 1091 | train loss: 2340.0219726562\n",
            "Epoch: 2 | iter: 1092 | train loss: 3158.2829589844\n",
            "Epoch: 2 | iter: 1093 | train loss: 2452.4711914062\n",
            "Epoch: 2 | iter: 1094 | train loss: 2427.6264648438\n",
            "Epoch: 2 | iter: 1095 | train loss: 2729.2197265625\n",
            "Epoch: 2 | iter: 1096 | train loss: 2858.3488769531\n",
            "Epoch: 2 | iter: 1097 | train loss: 2781.5678710938\n",
            "Epoch: 2 | iter: 1098 | train loss: 2770.3466796875\n",
            "Epoch: 2 | iter: 1099 | train loss: 2326.8071289062\n",
            "Epoch: 2 | iter: 1100 | train loss: 2785.4003906250\n",
            "Epoch: 2 | iter: 1101 | train loss: 2430.9580078125\n",
            "Epoch: 2 | iter: 1102 | train loss: 2882.0903320312\n",
            "Epoch: 2 | iter: 1103 | train loss: 2548.2263183594\n",
            "Epoch: 2 | iter: 1104 | train loss: 1983.0355224609\n",
            "Epoch: 2 | iter: 1105 | train loss: 2334.2309570312\n",
            "Epoch: 2 | iter: 1106 | train loss: 2441.5051269531\n",
            "Epoch: 2 | iter: 1107 | train loss: 2177.7797851562\n",
            "Epoch: 2 | iter: 1108 | train loss: 2450.4836425781\n",
            "Epoch: 2 | iter: 1109 | train loss: 2198.5473632812\n",
            "Epoch: 2 | iter: 1110 | train loss: 2142.4506835938\n",
            "Epoch: 2 | iter: 1111 | train loss: 2625.9118652344\n",
            "Epoch: 2 | iter: 1112 | train loss: 1831.3991699219\n",
            "Epoch: 2 | iter: 1113 | train loss: 2447.8745117188\n",
            "Epoch: 2 | iter: 1114 | train loss: 2196.4929199219\n",
            "Epoch: 2 | iter: 1115 | train loss: 2451.4311523438\n",
            "Epoch: 2 | iter: 1116 | train loss: 2872.6520996094\n",
            "Epoch: 2 | iter: 1117 | train loss: 2905.2661132812\n",
            "Epoch: 2 | iter: 1118 | train loss: 2146.8190917969\n",
            "Epoch: 2 | iter: 1119 | train loss: 3216.9562988281\n",
            "Epoch: 2 | iter: 1120 | train loss: 2489.8352050781\n",
            "Epoch: 2 | iter: 1121 | train loss: 2199.8364257812\n",
            "Epoch: 2 | iter: 1122 | train loss: 2523.4326171875\n",
            "Epoch: 2 | iter: 1123 | train loss: 2537.2668457031\n",
            "Epoch: 2 | iter: 1124 | train loss: 2207.8315429688\n",
            "Epoch: 2 | iter: 1125 | train loss: 2640.8112792969\n",
            "Epoch: 2 | iter: 1126 | train loss: 2925.2675781250\n",
            "Epoch: 2 | iter: 1127 | train loss: 2994.9638671875\n",
            "Epoch: 2 | iter: 1128 | train loss: 2355.8645019531\n",
            "Epoch: 2 | iter: 1129 | train loss: 2527.2111816406\n",
            "Epoch: 2 | iter: 1130 | train loss: 2170.8557128906\n",
            "Epoch: 2 | iter: 1131 | train loss: 2577.2829589844\n",
            "Epoch: 2 | iter: 1132 | train loss: 3045.3669433594\n",
            "Epoch: 2 | iter: 1133 | train loss: 2009.8192138672\n",
            "Epoch: 2 | iter: 1134 | train loss: 2068.1076660156\n",
            "Epoch: 2 | iter: 1135 | train loss: 2760.6179199219\n",
            "Epoch: 2 | iter: 1136 | train loss: 2019.5373535156\n",
            "Epoch: 2 | iter: 1137 | train loss: 2269.2624511719\n",
            "Epoch: 2 | iter: 1138 | train loss: 1878.9372558594\n",
            "Epoch: 2 | iter: 1139 | train loss: 2274.3540039062\n",
            "Epoch: 2 | iter: 1140 | train loss: 1666.2071533203\n",
            "Epoch: 2 | iter: 1141 | train loss: 1738.4345703125\n",
            "Epoch: 2 | iter: 1142 | train loss: 1740.9010009766\n",
            "Epoch: 2 | iter: 1143 | train loss: 1746.2358398438\n",
            "Epoch: 2 | iter: 1144 | train loss: 1861.5201416016\n",
            "Epoch: 2 | iter: 1145 | train loss: 2038.9677734375\n",
            "Epoch: 2 | iter: 1146 | train loss: 2339.6262207031\n",
            "Epoch: 2 | iter: 1147 | train loss: 2152.7607421875\n",
            "Epoch: 2 | iter: 1148 | train loss: 1597.9106445312\n",
            "Epoch: 2 | iter: 1149 | train loss: 1615.6114501953\n",
            "Epoch: 2 | iter: 1150 | train loss: 1652.8845214844\n",
            "Epoch: 2 | iter: 1151 | train loss: 2164.5900878906\n",
            "Epoch: 2 | iter: 1152 | train loss: 1751.4334716797\n",
            "Epoch: 2 | iter: 1153 | train loss: 2440.0031738281\n",
            "Epoch: 2 | iter: 1154 | train loss: 2094.6154785156\n",
            "Epoch: 2 | iter: 1155 | train loss: 2778.0539550781\n",
            "Epoch: 2 | iter: 1156 | train loss: 2805.7871093750\n",
            "Epoch: 2 | iter: 1157 | train loss: 1535.8800048828\n",
            "Epoch: 2 | iter: 1158 | train loss: 1678.3470458984\n",
            "Epoch: 2 | iter: 1159 | train loss: 2701.8757324219\n",
            "Epoch: 2 | iter: 1160 | train loss: 2526.9072265625\n",
            "Epoch: 2 | iter: 1161 | train loss: 2949.8842773438\n",
            "Epoch: 2 | iter: 1162 | train loss: 2128.4626464844\n",
            "Epoch: 2 | iter: 1163 | train loss: 1722.9886474609\n",
            "Epoch: 2 | iter: 1164 | train loss: 2261.1464843750\n",
            "Epoch: 2 | iter: 1165 | train loss: 2027.2894287109\n",
            "Epoch: 2 | iter: 1166 | train loss: 2464.1103515625\n",
            "Epoch: 2 | iter: 1167 | train loss: 1691.9869384766\n",
            "Epoch: 2 | iter: 1168 | train loss: 2859.9814453125\n",
            "Epoch: 2 | iter: 1169 | train loss: 2588.3073730469\n",
            "Epoch: 2 | iter: 1170 | train loss: 2563.5373535156\n",
            "Epoch: 2 | iter: 1171 | train loss: 2792.1220703125\n",
            "Epoch: 2 | iter: 1172 | train loss: 3211.1213378906\n",
            "Epoch: 2 | iter: 1173 | train loss: 2880.2912597656\n",
            "Epoch: 2 | iter: 1174 | train loss: 2544.7788085938\n",
            "Epoch: 2 | iter: 1175 | train loss: 1590.8463134766\n",
            "Epoch: 2 | iter: 1176 | train loss: 3292.0581054688\n",
            "Epoch: 2 | iter: 1177 | train loss: 1559.9970703125\n",
            "Epoch: 2 | iter: 1178 | train loss: 1609.3804931641\n",
            "Epoch: 2 | iter: 1179 | train loss: 1464.3299560547\n",
            "Epoch: 2 | iter: 1180 | train loss: 2340.7001953125\n",
            "Epoch: 2 | iter: 1181 | train loss: 1778.9543457031\n",
            "Epoch: 2 | iter: 1182 | train loss: 2396.5803222656\n",
            "Epoch: 2 | iter: 1183 | train loss: 2649.9631347656\n",
            "Epoch: 2 | iter: 1184 | train loss: 1555.8764648438\n",
            "Epoch: 2 | iter: 1185 | train loss: 2374.3217773438\n",
            "Epoch: 2 | iter: 1186 | train loss: 2487.5520019531\n",
            "Epoch: 2 | iter: 1187 | train loss: 2417.2011718750\n",
            "Epoch: 2 | iter: 1188 | train loss: 2215.9309082031\n",
            "Epoch: 2 | iter: 1189 | train loss: 1549.5573730469\n",
            "Epoch: 2 | iter: 1190 | train loss: 2510.2680664062\n",
            "Epoch: 2 | iter: 1191 | train loss: 2444.2268066406\n",
            "Epoch: 2 | iter: 1192 | train loss: 2071.9470214844\n",
            "Epoch: 2 | iter: 1193 | train loss: 2475.5322265625\n",
            "Epoch: 2 | iter: 1194 | train loss: 1725.7414550781\n",
            "Epoch: 2 | iter: 1195 | train loss: 1875.3566894531\n",
            "Epoch: 2 | iter: 1196 | train loss: 2086.6520996094\n",
            "Epoch: 2 | iter: 1197 | train loss: 2311.1447753906\n",
            "Epoch: 2 | iter: 1198 | train loss: 1433.3448486328\n",
            "Epoch: 2 | iter: 1199 | train loss: 1547.8795166016\n",
            "Epoch: 2 | iter: 1200 | train loss: 1697.0625000000\n",
            "Epoch: 2 | iter: 1201 | train loss: 2318.3850097656\n",
            "Epoch: 2 | iter: 1202 | train loss: 1624.0632324219\n",
            "Epoch: 2 | iter: 1203 | train loss: 2473.9885253906\n",
            "Epoch: 2 | iter: 1204 | train loss: 2192.0185546875\n",
            "Epoch: 2 | iter: 1205 | train loss: 2597.4345703125\n",
            "Epoch: 2 | iter: 1206 | train loss: 2082.0876464844\n",
            "Epoch: 2 | iter: 1207 | train loss: 1571.6511230469\n",
            "Epoch: 2 | iter: 1208 | train loss: 1863.8503417969\n",
            "Epoch: 2 | iter: 1209 | train loss: 2249.6464843750\n",
            "Epoch: 2 | iter: 1210 | train loss: 1130.0947265625\n",
            "Epoch: 2 | iter: 1211 | train loss: 1927.4042968750\n",
            "Epoch: 2 | iter: 1212 | train loss: 1981.8531494141\n",
            "Epoch: 2 | iter: 1213 | train loss: 1959.9095458984\n",
            "Epoch: 2 | iter: 1214 | train loss: 2036.4001464844\n",
            "Epoch: 2 | iter: 1215 | train loss: 1162.4736328125\n",
            "Epoch: 2 | iter: 1216 | train loss: 1304.2755126953\n",
            "Epoch: 2 | iter: 1217 | train loss: 2036.5935058594\n",
            "Epoch: 2 | iter: 1218 | train loss: 2552.8432617188\n",
            "Epoch: 2 | iter: 1219 | train loss: 2216.9011230469\n",
            "Epoch: 2 | iter: 1220 | train loss: 1506.8518066406\n",
            "Epoch: 2 | iter: 1221 | train loss: 1625.5427246094\n",
            "Epoch: 2 | iter: 1222 | train loss: 1874.0380859375\n",
            "Epoch: 2 | iter: 1223 | train loss: 1343.3178710938\n",
            "Epoch: 2 | iter: 1224 | train loss: 1875.3747558594\n",
            "Epoch: 2 | iter: 1225 | train loss: 1850.1778564453\n",
            "Epoch: 2 | iter: 1226 | train loss: 2324.2651367188\n",
            "Epoch: 2 | iter: 1227 | train loss: 1541.4997558594\n",
            "Epoch: 2 | iter: 1228 | train loss: 1788.9313964844\n",
            "Epoch: 2 | iter: 1229 | train loss: 2517.0407714844\n",
            "Epoch: 2 | iter: 1230 | train loss: 1382.7364501953\n",
            "Epoch: 2 | iter: 1231 | train loss: 1232.6826171875\n",
            "Epoch: 2 | iter: 1232 | train loss: 2157.9145507812\n",
            "Epoch: 2 | iter: 1233 | train loss: 1604.2084960938\n",
            "Epoch: 2 | iter: 1234 | train loss: 1614.6517333984\n",
            "Epoch: 2 | iter: 1235 | train loss: 2422.5651855469\n",
            "Epoch: 2 | iter: 1236 | train loss: 1231.4517822266\n",
            "Epoch: 2 | iter: 1237 | train loss: 1306.0915527344\n",
            "Epoch: 2 | iter: 1238 | train loss: 1620.7205810547\n",
            "Epoch: 2 | iter: 1239 | train loss: 1580.1018066406\n",
            "Epoch: 2 | iter: 1240 | train loss: 1621.3994140625\n",
            "Epoch: 2 | iter: 1241 | train loss: 1456.5705566406\n",
            "Epoch: 2 | iter: 1242 | train loss: 2794.5874023438\n",
            "Epoch: 2 | iter: 1243 | train loss: 2334.2241210938\n",
            "Epoch: 2 | iter: 1244 | train loss: 1160.6909179688\n",
            "Epoch: 2 | iter: 1245 | train loss: 1792.0849609375\n",
            "Epoch: 2 | iter: 1246 | train loss: 1011.1646728516\n",
            "Epoch: 2 | iter: 1247 | train loss: 2617.2414550781\n",
            "Epoch: 2 | iter: 1248 | train loss: 2188.4702148438\n",
            "Epoch: 2 | iter: 1249 | train loss: 1709.9215087891\n",
            "Epoch: 2 | iter: 1250 | train loss: 1421.8040771484\n",
            "Epoch: 2 | iter: 1251 | train loss: 2197.2590332031\n",
            "Epoch: 2 | iter: 1252 | train loss: 1615.1634521484\n",
            "Epoch: 2 | iter: 1253 | train loss: 1841.2641601562\n",
            "Epoch: 2 | iter: 1254 | train loss: 2173.2036132812\n",
            "Epoch: 2 | iter: 1255 | train loss: 2076.3620605469\n",
            "Epoch: 2 | iter: 1256 | train loss: 1821.6068115234\n",
            "Epoch: 2 | iter: 1257 | train loss: 2203.5925292969\n",
            "Epoch: 2 | iter: 1258 | train loss: 1797.6311035156\n",
            "Epoch: 2 | iter: 1259 | train loss: 2264.0971679688\n",
            "Epoch: 2 | iter: 1260 | train loss: 2463.4328613281\n",
            "Epoch: 2 | iter: 1261 | train loss: 1646.4925537109\n",
            "Epoch: 2 | iter: 1262 | train loss: 2110.0947265625\n",
            "Epoch: 2 | iter: 1263 | train loss: 1757.6883544922\n",
            "Epoch: 2 | iter: 1264 | train loss: 1624.2762451172\n",
            "Epoch: 2 | iter: 1265 | train loss: 1741.1455078125\n",
            "Epoch: 2 | iter: 1266 | train loss: 1635.2395019531\n",
            "Epoch: 2 | iter: 1267 | train loss: 1756.2446289062\n",
            "Epoch: 2 | iter: 1268 | train loss: 2331.0449218750\n",
            "Epoch: 2 | iter: 1269 | train loss: 2336.1352539062\n",
            "Epoch: 2 | iter: 1270 | train loss: 1220.5725097656\n",
            "Epoch: 2 | iter: 1271 | train loss: 1778.9642333984\n",
            "Epoch: 2 | iter: 1272 | train loss: 2274.6357421875\n",
            "Epoch: 2 | iter: 1273 | train loss: 2222.0690917969\n",
            "Epoch: 2 | iter: 1274 | train loss: 1913.6020507812\n",
            "Epoch: 2 | iter: 1275 | train loss: 1975.6437988281\n",
            "Epoch: 2 | iter: 1276 | train loss: 1991.5211181641\n",
            "Epoch: 2 | iter: 1277 | train loss: 2851.0295410156\n",
            "Epoch: 2 | iter: 1278 | train loss: 3014.1379394531\n",
            "Epoch: 2 | iter: 1279 | train loss: 2719.7731933594\n",
            "Epoch: 2 | iter: 1280 | train loss: 1979.9697265625\n",
            "Epoch: 2 | iter: 1281 | train loss: 1376.2100830078\n",
            "Epoch: 2 | iter: 1282 | train loss: 1627.6406250000\n",
            "Epoch: 2 | iter: 1283 | train loss: 2176.0417480469\n",
            "Epoch: 2 | iter: 1284 | train loss: 1523.1484375000\n",
            "Epoch: 2 | iter: 1285 | train loss: 1040.1010742188\n",
            "Epoch: 2 | iter: 1286 | train loss: 2212.6745605469\n",
            "Epoch: 2 | iter: 1287 | train loss: 2110.4318847656\n",
            "Epoch: 2 | iter: 1288 | train loss: 1107.8165283203\n",
            "Epoch: 2 | iter: 1289 | train loss: 1845.8645019531\n",
            "Epoch: 2 | iter: 1290 | train loss: 2025.1740722656\n",
            "Epoch: 2 | iter: 1291 | train loss: 1242.6643066406\n",
            "Epoch: 2 | iter: 1292 | train loss: 1868.1314697266\n",
            "Epoch: 2 | iter: 1293 | train loss: 2672.0998535156\n",
            "Epoch: 2 | iter: 1294 | train loss: 1891.1749267578\n",
            "Epoch: 2 | iter: 1295 | train loss: 2397.4177246094\n",
            "Epoch: 2 | iter: 1296 | train loss: 2330.7065429688\n",
            "Epoch: 2 | iter: 1297 | train loss: 2152.9821777344\n",
            "Epoch: 2 | iter: 1298 | train loss: 2540.7163085938\n",
            "Epoch: 2 | iter: 1299 | train loss: 1758.5505371094\n",
            "Epoch: 2 | iter: 1300 | train loss: 2458.9985351562\n",
            "Epoch: 2 | iter: 1301 | train loss: 1241.0501708984\n",
            "Epoch: 2 | iter: 1302 | train loss: 891.3636474609\n",
            "Epoch: 2 | iter: 1303 | train loss: 559.8190917969\n",
            "Epoch: 2 | iter: 1304 | train loss: 1650.0648193359\n",
            "Epoch: 2 | iter: 1305 | train loss: 1321.9774169922\n",
            "Epoch: 2 | iter: 1306 | train loss: 1887.3161621094\n",
            "Epoch: 2 | iter: 1307 | train loss: 2132.8308105469\n",
            "Epoch: 2 | iter: 1308 | train loss: 703.5989379883\n",
            "Epoch: 2 | iter: 1309 | train loss: 1218.5364990234\n",
            "Epoch: 2 | iter: 1310 | train loss: 2097.3474121094\n",
            "Epoch: 2 | iter: 1311 | train loss: 1273.4853515625\n",
            "Epoch: 2 | iter: 1312 | train loss: 1487.6623535156\n",
            "Epoch: 2 | iter: 1313 | train loss: 1942.2867431641\n",
            "Epoch: 2 | iter: 1314 | train loss: 764.2371215820\n",
            "Epoch: 2 | iter: 1315 | train loss: 2334.7819824219\n",
            "Epoch: 2 | iter: 1316 | train loss: 1417.6007080078\n",
            "Epoch: 2 | iter: 1317 | train loss: 694.2095336914\n",
            "Epoch: 2 | iter: 1318 | train loss: 2165.8872070312\n",
            "Epoch: 2 | iter: 1319 | train loss: 1974.9224853516\n",
            "Epoch: 2 | iter: 1320 | train loss: 1873.1787109375\n",
            "Epoch: 2 | iter: 1321 | train loss: 827.9909667969\n",
            "Epoch: 2 | iter: 1322 | train loss: 1175.5240478516\n",
            "Epoch: 2 | iter: 1323 | train loss: 1048.8734130859\n",
            "Epoch: 2 | iter: 1324 | train loss: 1482.5283203125\n",
            "Epoch: 2 | iter: 1325 | train loss: 1727.6779785156\n",
            "Epoch: 2 | iter: 1326 | train loss: 1315.2271728516\n",
            "Epoch: 2 | iter: 1327 | train loss: 1238.2955322266\n",
            "Epoch: 2 | iter: 1328 | train loss: 1726.7753906250\n",
            "Epoch: 2 | iter: 1329 | train loss: 1314.6884765625\n",
            "Epoch: 3 | iter: 0 | train loss: 1168.3754882812\n",
            "Epoch: 3 | iter: 1 | train loss: 903.8249511719\n",
            "Epoch: 3 | iter: 2 | train loss: 1387.8657226562\n",
            "Epoch: 3 | iter: 3 | train loss: 478.4303588867\n",
            "Epoch: 3 | iter: 4 | train loss: 973.0421142578\n",
            "Epoch: 3 | iter: 5 | train loss: 692.4647216797\n",
            "Epoch: 3 | iter: 6 | train loss: 554.0081787109\n",
            "Epoch: 3 | iter: 7 | train loss: 896.2705078125\n",
            "Epoch: 3 | iter: 8 | train loss: 597.6663818359\n",
            "Epoch: 3 | iter: 9 | train loss: 1177.1959228516\n",
            "Epoch: 3 | iter: 10 | train loss: 956.2965698242\n",
            "Epoch: 3 | iter: 11 | train loss: 439.1916503906\n",
            "Epoch: 3 | iter: 12 | train loss: 1106.9672851562\n",
            "Epoch: 3 | iter: 13 | train loss: 875.5478515625\n",
            "Epoch: 3 | iter: 14 | train loss: 574.5480346680\n",
            "Epoch: 3 | iter: 15 | train loss: 820.4717407227\n",
            "Epoch: 3 | iter: 16 | train loss: 545.6067504883\n",
            "Epoch: 3 | iter: 17 | train loss: 195.3511047363\n",
            "Epoch: 3 | iter: 18 | train loss: 794.6038818359\n",
            "Epoch: 3 | iter: 19 | train loss: 443.4153747559\n",
            "Epoch: 3 | iter: 20 | train loss: 1064.9224853516\n",
            "Epoch: 3 | iter: 21 | train loss: 463.2035522461\n",
            "Epoch: 3 | iter: 22 | train loss: 766.6098022461\n",
            "Epoch: 3 | iter: 23 | train loss: 663.9006347656\n",
            "Epoch: 3 | iter: 24 | train loss: 1009.4686889648\n",
            "Epoch: 3 | iter: 25 | train loss: 1425.1165771484\n",
            "Epoch: 3 | iter: 26 | train loss: 641.5554809570\n",
            "Epoch: 3 | iter: 27 | train loss: 313.0344543457\n",
            "Epoch: 3 | iter: 28 | train loss: 1204.7424316406\n",
            "Epoch: 3 | iter: 29 | train loss: 545.2573852539\n",
            "Epoch: 3 | iter: 30 | train loss: 1773.2082519531\n",
            "Epoch: 3 | iter: 31 | train loss: 449.2267150879\n",
            "Epoch: 3 | iter: 32 | train loss: 379.9912414551\n",
            "Epoch: 3 | iter: 33 | train loss: 867.8803100586\n",
            "Epoch: 3 | iter: 34 | train loss: 675.1692504883\n",
            "Epoch: 3 | iter: 35 | train loss: 925.5546875000\n",
            "Epoch: 3 | iter: 36 | train loss: 272.8908386230\n",
            "Epoch: 3 | iter: 37 | train loss: 754.3435058594\n",
            "Epoch: 3 | iter: 38 | train loss: 857.8075561523\n",
            "Epoch: 3 | iter: 39 | train loss: 699.6730957031\n",
            "Epoch: 3 | iter: 40 | train loss: 385.3826599121\n",
            "Epoch: 3 | iter: 41 | train loss: 703.7401123047\n",
            "Epoch: 3 | iter: 42 | train loss: 504.4715270996\n",
            "Epoch: 3 | iter: 43 | train loss: 1142.0971679688\n",
            "Epoch: 3 | iter: 44 | train loss: 747.1902465820\n",
            "Epoch: 3 | iter: 45 | train loss: 746.4327392578\n",
            "Epoch: 3 | iter: 46 | train loss: 444.9436340332\n",
            "Epoch: 3 | iter: 47 | train loss: 368.5107116699\n",
            "Epoch: 3 | iter: 48 | train loss: 1073.7133789062\n",
            "Epoch: 3 | iter: 49 | train loss: 484.6372680664\n",
            "Epoch: 3 | iter: 50 | train loss: 321.3399047852\n",
            "Epoch: 3 | iter: 51 | train loss: 1113.6872558594\n",
            "Epoch: 3 | iter: 52 | train loss: 859.8157348633\n",
            "Epoch: 3 | iter: 53 | train loss: 678.2828369141\n",
            "Epoch: 3 | iter: 54 | train loss: 871.7828979492\n",
            "Epoch: 3 | iter: 55 | train loss: 468.5144042969\n",
            "Epoch: 3 | iter: 56 | train loss: 743.2431030273\n",
            "Epoch: 3 | iter: 57 | train loss: 710.1417846680\n",
            "Epoch: 3 | iter: 58 | train loss: 884.4890136719\n",
            "Epoch: 3 | iter: 59 | train loss: 867.8988037109\n",
            "Epoch: 3 | iter: 60 | train loss: 496.0553588867\n",
            "Epoch: 3 | iter: 61 | train loss: 917.7402343750\n",
            "Epoch: 3 | iter: 62 | train loss: 844.2333984375\n",
            "Epoch: 3 | iter: 63 | train loss: 911.1491699219\n",
            "Epoch: 3 | iter: 64 | train loss: 1304.1589355469\n",
            "Epoch: 3 | iter: 65 | train loss: 1255.6481933594\n",
            "Epoch: 3 | iter: 66 | train loss: 1193.1102294922\n",
            "Epoch: 3 | iter: 67 | train loss: 945.3245239258\n",
            "Epoch: 3 | iter: 68 | train loss: 755.3578491211\n",
            "Epoch: 3 | iter: 69 | train loss: 589.8483276367\n",
            "Epoch: 3 | iter: 70 | train loss: 206.6426086426\n",
            "Epoch: 3 | iter: 71 | train loss: 1430.1325683594\n",
            "Epoch: 3 | iter: 72 | train loss: 862.5407104492\n",
            "Epoch: 3 | iter: 73 | train loss: 936.1498413086\n",
            "Epoch: 3 | iter: 74 | train loss: 552.8599243164\n",
            "Epoch: 3 | iter: 75 | train loss: 682.1762695312\n",
            "Epoch: 3 | iter: 76 | train loss: 365.9319763184\n",
            "Epoch: 3 | iter: 77 | train loss: 246.3704071045\n",
            "Epoch: 3 | iter: 78 | train loss: 1134.5096435547\n",
            "Epoch: 3 | iter: 79 | train loss: 549.4941406250\n",
            "Epoch: 3 | iter: 80 | train loss: 1690.8535156250\n",
            "Epoch: 3 | iter: 81 | train loss: 497.2602233887\n",
            "Epoch: 3 | iter: 82 | train loss: 495.7467346191\n",
            "Epoch: 3 | iter: 83 | train loss: 256.8261413574\n",
            "Epoch: 3 | iter: 84 | train loss: 746.8748168945\n",
            "Epoch: 3 | iter: 85 | train loss: 631.4257202148\n",
            "Epoch: 3 | iter: 86 | train loss: 893.9678955078\n",
            "Epoch: 3 | iter: 87 | train loss: 625.6063842773\n",
            "Epoch: 3 | iter: 88 | train loss: 1033.1071777344\n",
            "Epoch: 3 | iter: 89 | train loss: 512.2813720703\n",
            "Epoch: 3 | iter: 90 | train loss: 466.2707519531\n",
            "Epoch: 3 | iter: 91 | train loss: 706.2973632812\n",
            "Epoch: 3 | iter: 92 | train loss: 1059.8695068359\n",
            "Epoch: 3 | iter: 93 | train loss: 612.1313476562\n",
            "Epoch: 3 | iter: 94 | train loss: 565.0740356445\n",
            "Epoch: 3 | iter: 95 | train loss: 320.1534729004\n",
            "Epoch: 3 | iter: 96 | train loss: 137.6938934326\n",
            "Epoch: 3 | iter: 97 | train loss: 740.0142211914\n",
            "Epoch: 3 | iter: 98 | train loss: 503.0488891602\n",
            "Epoch: 3 | iter: 99 | train loss: 740.5237426758\n",
            "Epoch: 3 | iter: 100 | train loss: 462.6357727051\n",
            "Epoch: 3 | iter: 101 | train loss: 213.5695800781\n",
            "Epoch: 3 | iter: 102 | train loss: 584.0294799805\n",
            "Epoch: 3 | iter: 103 | train loss: 388.5748901367\n",
            "Epoch: 3 | iter: 104 | train loss: 323.5376586914\n",
            "Epoch: 3 | iter: 105 | train loss: 373.0677795410\n",
            "Epoch: 3 | iter: 106 | train loss: 627.1824340820\n",
            "Epoch: 3 | iter: 107 | train loss: 551.0475463867\n",
            "Epoch: 3 | iter: 108 | train loss: 1165.9096679688\n",
            "Epoch: 3 | iter: 109 | train loss: 379.7828063965\n",
            "Epoch: 3 | iter: 110 | train loss: 449.8639526367\n",
            "Epoch: 3 | iter: 111 | train loss: 675.2612304688\n",
            "Epoch: 3 | iter: 112 | train loss: 1009.6304931641\n",
            "Epoch: 3 | iter: 113 | train loss: 578.4114379883\n",
            "Epoch: 3 | iter: 114 | train loss: 851.9617919922\n",
            "Epoch: 3 | iter: 115 | train loss: 897.2225341797\n",
            "Epoch: 3 | iter: 116 | train loss: 2711.6708984375\n",
            "Epoch: 3 | iter: 117 | train loss: 1398.2725830078\n",
            "Epoch: 3 | iter: 118 | train loss: 2418.6892089844\n",
            "Epoch: 3 | iter: 119 | train loss: 2272.0607910156\n",
            "Epoch: 3 | iter: 120 | train loss: 1260.4014892578\n",
            "Epoch: 3 | iter: 121 | train loss: 1906.9255371094\n",
            "Epoch: 3 | iter: 122 | train loss: 1184.5325927734\n",
            "Epoch: 3 | iter: 123 | train loss: 868.9151000977\n",
            "Epoch: 3 | iter: 124 | train loss: 491.8102722168\n",
            "Epoch: 3 | iter: 125 | train loss: 2616.6674804688\n",
            "Epoch: 3 | iter: 126 | train loss: 669.8463745117\n",
            "Epoch: 3 | iter: 127 | train loss: 1560.7031250000\n",
            "Epoch: 3 | iter: 128 | train loss: 699.4825439453\n",
            "Epoch: 3 | iter: 129 | train loss: 1356.0611572266\n",
            "Epoch: 3 | iter: 130 | train loss: 954.3502807617\n",
            "Epoch: 3 | iter: 131 | train loss: 374.5740356445\n",
            "Epoch: 3 | iter: 132 | train loss: 656.5664672852\n",
            "Epoch: 3 | iter: 133 | train loss: 1452.0288085938\n",
            "Epoch: 3 | iter: 134 | train loss: 2555.9931640625\n",
            "Epoch: 3 | iter: 135 | train loss: 1528.1276855469\n",
            "Epoch: 3 | iter: 136 | train loss: 1636.6903076172\n",
            "Epoch: 3 | iter: 137 | train loss: 1036.1114501953\n",
            "Epoch: 3 | iter: 138 | train loss: 1873.3560791016\n",
            "Epoch: 3 | iter: 139 | train loss: 1990.7318115234\n",
            "Epoch: 3 | iter: 140 | train loss: 2450.6965332031\n",
            "Epoch: 3 | iter: 141 | train loss: 2614.1257324219\n",
            "Epoch: 3 | iter: 142 | train loss: 3139.7822265625\n",
            "Epoch: 3 | iter: 143 | train loss: 3422.2209472656\n",
            "Epoch: 3 | iter: 144 | train loss: 2986.0908203125\n",
            "Epoch: 3 | iter: 145 | train loss: 3281.7077636719\n",
            "Epoch: 3 | iter: 146 | train loss: 3214.0795898438\n",
            "Epoch: 3 | iter: 147 | train loss: 2326.8969726562\n",
            "Epoch: 3 | iter: 148 | train loss: 2804.0917968750\n",
            "Epoch: 3 | iter: 149 | train loss: 2191.2668457031\n",
            "Epoch: 3 | iter: 150 | train loss: 3031.4714355469\n",
            "Epoch: 3 | iter: 151 | train loss: 1857.8950195312\n",
            "Epoch: 3 | iter: 152 | train loss: 1259.4277343750\n",
            "Epoch: 3 | iter: 153 | train loss: 1418.9503173828\n",
            "Epoch: 3 | iter: 154 | train loss: 2423.8674316406\n",
            "Epoch: 3 | iter: 155 | train loss: 1120.6695556641\n",
            "Epoch: 3 | iter: 156 | train loss: 1154.0208740234\n",
            "Epoch: 3 | iter: 157 | train loss: 1269.6016845703\n",
            "Epoch: 3 | iter: 158 | train loss: 1271.0498046875\n",
            "Epoch: 3 | iter: 159 | train loss: 951.6202392578\n",
            "Epoch: 3 | iter: 160 | train loss: 1637.8332519531\n",
            "Epoch: 3 | iter: 161 | train loss: 918.1592407227\n",
            "Epoch: 3 | iter: 162 | train loss: 653.5642700195\n",
            "Epoch: 3 | iter: 163 | train loss: 1133.1560058594\n",
            "Epoch: 3 | iter: 164 | train loss: 599.0429077148\n",
            "Epoch: 3 | iter: 165 | train loss: 1427.4633789062\n",
            "Epoch: 3 | iter: 166 | train loss: 850.5177612305\n",
            "Epoch: 3 | iter: 167 | train loss: 337.5562438965\n",
            "Epoch: 3 | iter: 168 | train loss: 723.9773559570\n",
            "Epoch: 3 | iter: 169 | train loss: 964.1432495117\n",
            "Epoch: 3 | iter: 170 | train loss: 742.4249267578\n",
            "Epoch: 3 | iter: 171 | train loss: 500.9869384766\n",
            "Epoch: 3 | iter: 172 | train loss: 460.7892456055\n",
            "Epoch: 3 | iter: 173 | train loss: 629.2696533203\n",
            "Epoch: 3 | iter: 174 | train loss: 541.4938354492\n",
            "Epoch: 3 | iter: 175 | train loss: 295.1920776367\n",
            "Epoch: 3 | iter: 176 | train loss: 274.3591308594\n",
            "Epoch: 3 | iter: 177 | train loss: 412.9099426270\n",
            "Epoch: 3 | iter: 178 | train loss: 316.0433349609\n",
            "Epoch: 3 | iter: 179 | train loss: 884.1621093750\n",
            "Epoch: 3 | iter: 180 | train loss: 544.6267089844\n",
            "Epoch: 3 | iter: 181 | train loss: 1525.6354980469\n",
            "Epoch: 3 | iter: 182 | train loss: 488.4263000488\n",
            "Epoch: 3 | iter: 183 | train loss: 455.9841613770\n",
            "Epoch: 3 | iter: 184 | train loss: 304.3602905273\n",
            "Epoch: 3 | iter: 185 | train loss: 174.2856750488\n",
            "Epoch: 3 | iter: 186 | train loss: 727.7163085938\n",
            "Epoch: 3 | iter: 187 | train loss: 360.6085510254\n",
            "Epoch: 3 | iter: 188 | train loss: 588.7208251953\n",
            "Epoch: 3 | iter: 189 | train loss: 602.5364379883\n",
            "Epoch: 3 | iter: 190 | train loss: 407.6482543945\n",
            "Epoch: 3 | iter: 191 | train loss: 267.4768371582\n",
            "Epoch: 3 | iter: 192 | train loss: 159.5692443848\n",
            "Epoch: 3 | iter: 193 | train loss: 224.4706268311\n",
            "Epoch: 3 | iter: 194 | train loss: 304.2051391602\n",
            "Epoch: 3 | iter: 195 | train loss: 669.2575073242\n",
            "Epoch: 3 | iter: 196 | train loss: 1144.4061279297\n",
            "Epoch: 3 | iter: 197 | train loss: 343.7657165527\n",
            "Epoch: 3 | iter: 198 | train loss: 253.9725341797\n",
            "Epoch: 3 | iter: 199 | train loss: 169.4779968262\n",
            "Epoch: 3 | iter: 200 | train loss: 625.2647094727\n",
            "Epoch: 3 | iter: 201 | train loss: 136.8777465820\n",
            "Epoch: 3 | iter: 202 | train loss: 715.7768554688\n",
            "Epoch: 3 | iter: 203 | train loss: 343.2066650391\n",
            "Epoch: 3 | iter: 204 | train loss: 1151.2091064453\n",
            "Epoch: 3 | iter: 205 | train loss: 592.6342163086\n",
            "Epoch: 3 | iter: 206 | train loss: 173.3515319824\n",
            "Epoch: 3 | iter: 207 | train loss: 235.3270416260\n",
            "Epoch: 3 | iter: 208 | train loss: 607.4345703125\n",
            "Epoch: 3 | iter: 209 | train loss: 456.6446838379\n",
            "Epoch: 3 | iter: 210 | train loss: 630.9802246094\n",
            "Epoch: 3 | iter: 211 | train loss: 162.0458831787\n",
            "Epoch: 3 | iter: 212 | train loss: 552.2206420898\n",
            "Epoch: 3 | iter: 213 | train loss: 679.5390625000\n",
            "Epoch: 3 | iter: 214 | train loss: 440.8327026367\n",
            "Epoch: 3 | iter: 215 | train loss: 492.8332214355\n",
            "Epoch: 3 | iter: 216 | train loss: 446.0224609375\n",
            "Epoch: 3 | iter: 217 | train loss: 376.7083435059\n",
            "Epoch: 3 | iter: 218 | train loss: 512.4244384766\n",
            "Epoch: 3 | iter: 219 | train loss: 548.9030151367\n",
            "Epoch: 3 | iter: 220 | train loss: 462.8943786621\n",
            "Epoch: 3 | iter: 221 | train loss: 197.1114654541\n",
            "Epoch: 3 | iter: 222 | train loss: 634.3125610352\n",
            "Epoch: 3 | iter: 223 | train loss: 244.3545684814\n",
            "Epoch: 3 | iter: 224 | train loss: 241.4061889648\n",
            "Epoch: 3 | iter: 225 | train loss: 569.5621948242\n",
            "Epoch: 3 | iter: 226 | train loss: 304.8363952637\n",
            "Epoch: 3 | iter: 227 | train loss: 1109.8338623047\n",
            "Epoch: 3 | iter: 228 | train loss: 264.4792480469\n",
            "Epoch: 3 | iter: 229 | train loss: 356.3070678711\n",
            "Epoch: 3 | iter: 230 | train loss: 173.8248596191\n",
            "Epoch: 3 | iter: 231 | train loss: 304.1683044434\n",
            "Epoch: 3 | iter: 232 | train loss: 687.1427001953\n",
            "Epoch: 3 | iter: 233 | train loss: 246.7767639160\n",
            "Epoch: 3 | iter: 234 | train loss: 1030.7844238281\n",
            "Epoch: 3 | iter: 235 | train loss: 642.4152221680\n",
            "Epoch: 3 | iter: 236 | train loss: 297.3770141602\n",
            "Epoch: 3 | iter: 237 | train loss: 394.8518981934\n",
            "Epoch: 3 | iter: 238 | train loss: 521.2529907227\n",
            "Epoch: 3 | iter: 239 | train loss: 177.4369201660\n",
            "Epoch: 3 | iter: 240 | train loss: 551.1660766602\n",
            "Epoch: 3 | iter: 241 | train loss: 361.3819885254\n",
            "Epoch: 3 | iter: 242 | train loss: 627.3046875000\n",
            "Epoch: 3 | iter: 243 | train loss: 964.9494628906\n",
            "Epoch: 3 | iter: 244 | train loss: 472.1561889648\n",
            "Epoch: 3 | iter: 245 | train loss: 458.0377197266\n",
            "Epoch: 3 | iter: 246 | train loss: 930.6048583984\n",
            "Epoch: 3 | iter: 247 | train loss: 482.8130493164\n",
            "Epoch: 3 | iter: 248 | train loss: 500.7026672363\n",
            "Epoch: 3 | iter: 249 | train loss: 252.1589355469\n",
            "Epoch: 3 | iter: 250 | train loss: 352.1903991699\n",
            "Epoch: 3 | iter: 251 | train loss: 144.4068298340\n",
            "Epoch: 3 | iter: 252 | train loss: 196.1393127441\n",
            "Epoch: 3 | iter: 253 | train loss: 211.5233917236\n",
            "Epoch: 3 | iter: 254 | train loss: 327.6396484375\n",
            "Epoch: 3 | iter: 255 | train loss: 131.6630401611\n",
            "Epoch: 3 | iter: 256 | train loss: 349.8405151367\n",
            "Epoch: 3 | iter: 257 | train loss: 137.1970520020\n",
            "Epoch: 3 | iter: 258 | train loss: 609.1481933594\n",
            "Epoch: 3 | iter: 259 | train loss: 286.5439758301\n",
            "Epoch: 3 | iter: 260 | train loss: 142.4175415039\n",
            "Epoch: 3 | iter: 261 | train loss: 319.7079467773\n",
            "Epoch: 3 | iter: 262 | train loss: 547.9973144531\n",
            "Epoch: 3 | iter: 263 | train loss: 238.8926849365\n",
            "Epoch: 3 | iter: 264 | train loss: 517.0356445312\n",
            "Epoch: 3 | iter: 265 | train loss: 443.8293762207\n",
            "Epoch: 3 | iter: 266 | train loss: 227.2530975342\n",
            "Epoch: 3 | iter: 267 | train loss: 240.4653472900\n",
            "Epoch: 3 | iter: 268 | train loss: 286.8634338379\n",
            "Epoch: 3 | iter: 269 | train loss: 452.2032165527\n",
            "Epoch: 3 | iter: 270 | train loss: 290.5347290039\n",
            "Epoch: 3 | iter: 271 | train loss: 343.3632202148\n",
            "Epoch: 3 | iter: 272 | train loss: 348.2027587891\n",
            "Epoch: 3 | iter: 273 | train loss: 681.9583740234\n",
            "Epoch: 3 | iter: 274 | train loss: 236.6053009033\n",
            "Epoch: 3 | iter: 275 | train loss: 244.2348022461\n",
            "Epoch: 3 | iter: 276 | train loss: 557.8908081055\n",
            "Epoch: 3 | iter: 277 | train loss: 379.5101623535\n",
            "Epoch: 3 | iter: 278 | train loss: 416.6075744629\n",
            "Epoch: 3 | iter: 279 | train loss: 250.6869812012\n",
            "Epoch: 3 | iter: 280 | train loss: 340.0953063965\n",
            "Epoch: 3 | iter: 281 | train loss: 235.8879394531\n",
            "Epoch: 3 | iter: 282 | train loss: 419.3964233398\n",
            "Epoch: 3 | iter: 283 | train loss: 446.1852722168\n",
            "Epoch: 3 | iter: 284 | train loss: 276.7959594727\n",
            "Epoch: 3 | iter: 285 | train loss: 423.8878784180\n",
            "Epoch: 3 | iter: 286 | train loss: 259.9521789551\n",
            "Epoch: 3 | iter: 287 | train loss: 460.2249450684\n",
            "Epoch: 3 | iter: 288 | train loss: 658.3901367188\n",
            "Epoch: 3 | iter: 289 | train loss: 182.1262817383\n",
            "Epoch: 3 | iter: 290 | train loss: 204.5552368164\n",
            "Epoch: 3 | iter: 291 | train loss: 252.1884460449\n",
            "Epoch: 3 | iter: 292 | train loss: 393.5449523926\n",
            "Epoch: 3 | iter: 293 | train loss: 148.0089416504\n",
            "Epoch: 3 | iter: 294 | train loss: 446.2712097168\n",
            "Epoch: 3 | iter: 295 | train loss: 327.9554748535\n",
            "Epoch: 3 | iter: 296 | train loss: 572.3457031250\n",
            "Epoch: 3 | iter: 297 | train loss: 372.1878662109\n",
            "Epoch: 3 | iter: 298 | train loss: 241.5599365234\n",
            "Epoch: 3 | iter: 299 | train loss: 307.8726806641\n",
            "Epoch: 3 | iter: 300 | train loss: 356.6727600098\n",
            "Epoch: 3 | iter: 301 | train loss: 526.7739257812\n",
            "Epoch: 3 | iter: 302 | train loss: 767.2924804688\n",
            "Epoch: 3 | iter: 303 | train loss: 596.1594848633\n",
            "Epoch: 3 | iter: 304 | train loss: 866.8657226562\n",
            "Epoch: 3 | iter: 305 | train loss: 553.8084106445\n",
            "Epoch: 3 | iter: 306 | train loss: 623.8377685547\n",
            "Epoch: 3 | iter: 307 | train loss: 364.1758728027\n",
            "Epoch: 3 | iter: 308 | train loss: 290.1811828613\n",
            "Epoch: 3 | iter: 309 | train loss: 372.0935668945\n",
            "Epoch: 3 | iter: 310 | train loss: 503.6958007812\n",
            "Epoch: 3 | iter: 311 | train loss: 355.2286071777\n",
            "Epoch: 3 | iter: 312 | train loss: 338.2688598633\n",
            "Epoch: 3 | iter: 313 | train loss: 379.1534423828\n",
            "Epoch: 3 | iter: 314 | train loss: 290.7423095703\n",
            "Epoch: 3 | iter: 315 | train loss: 186.2634582520\n",
            "Epoch: 3 | iter: 316 | train loss: 353.0382995605\n",
            "Epoch: 3 | iter: 317 | train loss: 587.6145629883\n",
            "Epoch: 3 | iter: 318 | train loss: 467.7604064941\n",
            "Epoch: 3 | iter: 319 | train loss: 285.0791320801\n",
            "Epoch: 3 | iter: 320 | train loss: 193.8832092285\n",
            "Epoch: 3 | iter: 321 | train loss: 242.4017639160\n",
            "Epoch: 3 | iter: 322 | train loss: 323.4633483887\n",
            "Epoch: 3 | iter: 323 | train loss: 366.5304260254\n",
            "Epoch: 3 | iter: 324 | train loss: 339.3925781250\n",
            "Epoch: 3 | iter: 325 | train loss: 284.6264953613\n",
            "Epoch: 3 | iter: 326 | train loss: 564.2592773438\n",
            "Epoch: 3 | iter: 327 | train loss: 692.8400268555\n",
            "Epoch: 3 | iter: 328 | train loss: 489.5428466797\n",
            "Epoch: 3 | iter: 329 | train loss: 749.1042480469\n",
            "Epoch: 3 | iter: 330 | train loss: 476.3641357422\n",
            "Epoch: 3 | iter: 331 | train loss: 354.7584533691\n",
            "Epoch: 3 | iter: 332 | train loss: 522.8517456055\n",
            "Epoch: 3 | iter: 333 | train loss: 546.9591064453\n",
            "Epoch: 3 | iter: 334 | train loss: 464.7571105957\n",
            "Epoch: 3 | iter: 335 | train loss: 473.1102905273\n",
            "Epoch: 3 | iter: 336 | train loss: 557.0007324219\n",
            "Epoch: 3 | iter: 337 | train loss: 594.9619140625\n",
            "Epoch: 3 | iter: 338 | train loss: 767.1145019531\n",
            "Epoch: 3 | iter: 339 | train loss: 816.4987792969\n",
            "Epoch: 3 | iter: 340 | train loss: 779.0042724609\n",
            "Epoch: 3 | iter: 341 | train loss: 722.7100830078\n",
            "Epoch: 3 | iter: 342 | train loss: 881.2212524414\n",
            "Epoch: 3 | iter: 343 | train loss: 746.3682250977\n",
            "Epoch: 3 | iter: 344 | train loss: 617.8394775391\n",
            "Epoch: 3 | iter: 345 | train loss: 905.6526489258\n",
            "Epoch: 3 | iter: 346 | train loss: 697.9140014648\n",
            "Epoch: 3 | iter: 347 | train loss: 693.3672485352\n",
            "Epoch: 3 | iter: 348 | train loss: 828.8345336914\n",
            "Epoch: 3 | iter: 349 | train loss: 770.9937133789\n",
            "Epoch: 3 | iter: 350 | train loss: 489.0752868652\n",
            "Epoch: 3 | iter: 351 | train loss: 883.1748046875\n",
            "Epoch: 3 | iter: 352 | train loss: 603.9888916016\n",
            "Epoch: 3 | iter: 353 | train loss: 747.6706542969\n",
            "Epoch: 3 | iter: 354 | train loss: 846.7843627930\n",
            "Epoch: 3 | iter: 355 | train loss: 767.4673461914\n",
            "Epoch: 3 | iter: 356 | train loss: 898.4938964844\n",
            "Epoch: 3 | iter: 357 | train loss: 642.8732299805\n",
            "Epoch: 3 | iter: 358 | train loss: 811.7050170898\n",
            "Epoch: 3 | iter: 359 | train loss: 848.6583862305\n",
            "Epoch: 3 | iter: 360 | train loss: 760.4829711914\n",
            "Epoch: 3 | iter: 361 | train loss: 873.9935913086\n",
            "Epoch: 3 | iter: 362 | train loss: 676.7388305664\n",
            "Epoch: 3 | iter: 363 | train loss: 937.1118774414\n",
            "Epoch: 3 | iter: 364 | train loss: 750.8134155273\n",
            "Epoch: 3 | iter: 365 | train loss: 360.4163818359\n",
            "Epoch: 3 | iter: 366 | train loss: 693.2847290039\n",
            "Epoch: 3 | iter: 367 | train loss: 818.4118041992\n",
            "Epoch: 3 | iter: 368 | train loss: 300.1018676758\n",
            "Epoch: 3 | iter: 369 | train loss: 802.0544433594\n",
            "Epoch: 3 | iter: 370 | train loss: 560.5230712891\n",
            "Epoch: 3 | iter: 371 | train loss: 722.2059936523\n",
            "Epoch: 3 | iter: 372 | train loss: 823.1176757812\n",
            "Epoch: 3 | iter: 373 | train loss: 619.1939086914\n",
            "Epoch: 3 | iter: 374 | train loss: 721.1202392578\n",
            "Epoch: 3 | iter: 375 | train loss: 521.9059448242\n",
            "Epoch: 3 | iter: 376 | train loss: 489.7711486816\n",
            "Epoch: 3 | iter: 377 | train loss: 637.9081420898\n",
            "Epoch: 3 | iter: 378 | train loss: 863.8636474609\n",
            "Epoch: 3 | iter: 379 | train loss: 657.2445068359\n",
            "Epoch: 3 | iter: 380 | train loss: 1034.3015136719\n",
            "Epoch: 3 | iter: 381 | train loss: 373.5563659668\n",
            "Epoch: 3 | iter: 382 | train loss: 522.4044799805\n",
            "Epoch: 3 | iter: 383 | train loss: 769.3720092773\n",
            "Epoch: 3 | iter: 384 | train loss: 608.4309082031\n",
            "Epoch: 3 | iter: 385 | train loss: 1037.9407958984\n",
            "Epoch: 3 | iter: 386 | train loss: 627.7543945312\n",
            "Epoch: 3 | iter: 387 | train loss: 746.1008300781\n",
            "Epoch: 3 | iter: 388 | train loss: 661.5221557617\n",
            "Epoch: 3 | iter: 389 | train loss: 704.6434326172\n",
            "Epoch: 3 | iter: 390 | train loss: 627.5636596680\n",
            "Epoch: 3 | iter: 391 | train loss: 475.7795715332\n",
            "Epoch: 3 | iter: 392 | train loss: 428.5473632812\n",
            "Epoch: 3 | iter: 393 | train loss: 556.4597167969\n",
            "Epoch: 3 | iter: 394 | train loss: 651.7224731445\n",
            "Epoch: 3 | iter: 395 | train loss: 583.4038085938\n",
            "Epoch: 3 | iter: 396 | train loss: 570.8770751953\n",
            "Epoch: 3 | iter: 397 | train loss: 911.4307861328\n",
            "Epoch: 3 | iter: 398 | train loss: 440.3543395996\n",
            "Epoch: 3 | iter: 399 | train loss: 566.9611816406\n",
            "Epoch: 3 | iter: 400 | train loss: 972.9362792969\n",
            "Epoch: 3 | iter: 401 | train loss: 731.5383911133\n",
            "Epoch: 3 | iter: 402 | train loss: 589.8433837891\n",
            "Epoch: 3 | iter: 403 | train loss: 514.4105834961\n",
            "Epoch: 3 | iter: 404 | train loss: 455.7339782715\n",
            "Epoch: 3 | iter: 405 | train loss: 881.9645385742\n",
            "Epoch: 3 | iter: 406 | train loss: 641.3314208984\n",
            "Epoch: 3 | iter: 407 | train loss: 737.1682128906\n",
            "Epoch: 3 | iter: 408 | train loss: 636.5852050781\n",
            "Epoch: 3 | iter: 409 | train loss: 788.4447631836\n",
            "Epoch: 3 | iter: 410 | train loss: 567.1917724609\n",
            "Epoch: 3 | iter: 411 | train loss: 668.0714721680\n",
            "Epoch: 3 | iter: 412 | train loss: 549.7561645508\n",
            "Epoch: 3 | iter: 413 | train loss: 640.7845458984\n",
            "Epoch: 3 | iter: 414 | train loss: 572.5576782227\n",
            "Epoch: 3 | iter: 415 | train loss: 961.5819091797\n",
            "Epoch: 3 | iter: 416 | train loss: 533.7054443359\n",
            "Epoch: 3 | iter: 417 | train loss: 588.2940063477\n",
            "Epoch: 3 | iter: 418 | train loss: 877.7916259766\n",
            "Epoch: 3 | iter: 419 | train loss: 642.1029663086\n",
            "Epoch: 3 | iter: 420 | train loss: 1035.8089599609\n",
            "Epoch: 3 | iter: 421 | train loss: 511.0038146973\n",
            "Epoch: 3 | iter: 422 | train loss: 933.9369506836\n",
            "Epoch: 3 | iter: 423 | train loss: 596.9253540039\n",
            "Epoch: 3 | iter: 424 | train loss: 633.0944213867\n",
            "Epoch: 3 | iter: 425 | train loss: 514.6014404297\n",
            "Epoch: 3 | iter: 426 | train loss: 533.7498168945\n",
            "Epoch: 3 | iter: 427 | train loss: 699.2965698242\n",
            "Epoch: 3 | iter: 428 | train loss: 852.9329833984\n",
            "Epoch: 3 | iter: 429 | train loss: 321.2660217285\n",
            "Epoch: 3 | iter: 430 | train loss: 696.8522338867\n",
            "Epoch: 3 | iter: 431 | train loss: 590.5617675781\n",
            "Epoch: 3 | iter: 432 | train loss: 704.2103271484\n",
            "Epoch: 3 | iter: 433 | train loss: 531.2765502930\n",
            "Epoch: 3 | iter: 434 | train loss: 420.4170532227\n",
            "Epoch: 3 | iter: 435 | train loss: 829.6821899414\n",
            "Epoch: 3 | iter: 436 | train loss: 620.0977172852\n",
            "Epoch: 3 | iter: 437 | train loss: 314.2103576660\n",
            "Epoch: 3 | iter: 438 | train loss: 727.7379150391\n",
            "Epoch: 3 | iter: 439 | train loss: 263.5352172852\n",
            "Epoch: 3 | iter: 440 | train loss: 696.9366455078\n",
            "Epoch: 3 | iter: 441 | train loss: 626.9716796875\n",
            "Epoch: 3 | iter: 442 | train loss: 779.4122924805\n",
            "Epoch: 3 | iter: 443 | train loss: 1070.5379638672\n",
            "Epoch: 3 | iter: 444 | train loss: 762.2496337891\n",
            "Epoch: 3 | iter: 445 | train loss: 572.3305053711\n",
            "Epoch: 3 | iter: 446 | train loss: 708.7029418945\n",
            "Epoch: 3 | iter: 447 | train loss: 728.6946411133\n",
            "Epoch: 3 | iter: 448 | train loss: 680.5729980469\n",
            "Epoch: 3 | iter: 449 | train loss: 573.6020507812\n",
            "Epoch: 3 | iter: 450 | train loss: 621.6546020508\n",
            "Epoch: 3 | iter: 451 | train loss: 652.7055053711\n",
            "Epoch: 3 | iter: 452 | train loss: 756.0944824219\n",
            "Epoch: 3 | iter: 453 | train loss: 743.8148803711\n",
            "Epoch: 3 | iter: 454 | train loss: 720.2185668945\n",
            "Epoch: 3 | iter: 455 | train loss: 538.3470458984\n",
            "Epoch: 3 | iter: 456 | train loss: 792.2280273438\n",
            "Epoch: 3 | iter: 457 | train loss: 763.0982666016\n",
            "Epoch: 3 | iter: 458 | train loss: 914.8998413086\n",
            "Epoch: 3 | iter: 459 | train loss: 824.1832885742\n",
            "Epoch: 3 | iter: 460 | train loss: 845.3571166992\n",
            "Epoch: 3 | iter: 461 | train loss: 623.4744262695\n",
            "Epoch: 3 | iter: 462 | train loss: 584.2490234375\n",
            "Epoch: 3 | iter: 463 | train loss: 732.2656860352\n",
            "Epoch: 3 | iter: 464 | train loss: 711.0297851562\n",
            "Epoch: 3 | iter: 465 | train loss: 628.2912597656\n",
            "Epoch: 3 | iter: 466 | train loss: 644.9913330078\n",
            "Epoch: 3 | iter: 467 | train loss: 618.9430541992\n",
            "Epoch: 3 | iter: 468 | train loss: 432.9425659180\n",
            "Epoch: 3 | iter: 469 | train loss: 355.5226135254\n",
            "Epoch: 3 | iter: 470 | train loss: 810.7603149414\n",
            "Epoch: 3 | iter: 471 | train loss: 675.2795410156\n",
            "Epoch: 3 | iter: 472 | train loss: 560.8182983398\n",
            "Epoch: 3 | iter: 473 | train loss: 688.7921142578\n",
            "Epoch: 3 | iter: 474 | train loss: 1021.7849121094\n",
            "Epoch: 3 | iter: 475 | train loss: 826.0119018555\n",
            "Epoch: 3 | iter: 476 | train loss: 827.1362304688\n",
            "Epoch: 3 | iter: 477 | train loss: 384.5171203613\n",
            "Epoch: 3 | iter: 478 | train loss: 466.0198974609\n",
            "Epoch: 3 | iter: 479 | train loss: 788.1723632812\n",
            "Epoch: 3 | iter: 480 | train loss: 870.8402099609\n",
            "Epoch: 3 | iter: 481 | train loss: 1040.3624267578\n",
            "Epoch: 3 | iter: 482 | train loss: 1024.9698486328\n",
            "Epoch: 3 | iter: 483 | train loss: 660.2534179688\n",
            "Epoch: 3 | iter: 484 | train loss: 581.1027221680\n",
            "Epoch: 3 | iter: 485 | train loss: 482.2722167969\n",
            "Epoch: 3 | iter: 486 | train loss: 598.3657836914\n",
            "Epoch: 3 | iter: 487 | train loss: 1001.1671752930\n",
            "Epoch: 3 | iter: 488 | train loss: 969.1491088867\n",
            "Epoch: 3 | iter: 489 | train loss: 456.6480407715\n",
            "Epoch: 3 | iter: 490 | train loss: 653.1039428711\n",
            "Epoch: 3 | iter: 491 | train loss: 538.2156372070\n",
            "Epoch: 3 | iter: 492 | train loss: 764.0790405273\n",
            "Epoch: 3 | iter: 493 | train loss: 858.4747314453\n",
            "Epoch: 3 | iter: 494 | train loss: 685.7807617188\n",
            "Epoch: 3 | iter: 495 | train loss: 890.3115844727\n",
            "Epoch: 3 | iter: 496 | train loss: 735.3616333008\n",
            "Epoch: 3 | iter: 497 | train loss: 909.6437377930\n",
            "Epoch: 3 | iter: 498 | train loss: 855.1381835938\n",
            "Epoch: 3 | iter: 499 | train loss: 591.1185913086\n",
            "Epoch: 3 | iter: 500 | train loss: 554.7393188477\n",
            "Epoch: 3 | iter: 501 | train loss: 744.5374145508\n",
            "Epoch: 3 | iter: 502 | train loss: 690.4069824219\n",
            "Epoch: 3 | iter: 503 | train loss: 683.1591186523\n",
            "Epoch: 3 | iter: 504 | train loss: 1018.2741088867\n",
            "Epoch: 3 | iter: 505 | train loss: 805.4013061523\n",
            "Epoch: 3 | iter: 506 | train loss: 1070.0882568359\n",
            "Epoch: 3 | iter: 507 | train loss: 768.5408935547\n",
            "Epoch: 3 | iter: 508 | train loss: 866.5109863281\n",
            "Epoch: 3 | iter: 509 | train loss: 469.2623901367\n",
            "Epoch: 3 | iter: 510 | train loss: 699.5791625977\n",
            "Epoch: 3 | iter: 511 | train loss: 861.3464355469\n",
            "Epoch: 3 | iter: 512 | train loss: 672.9809570312\n",
            "Epoch: 3 | iter: 513 | train loss: 743.7328491211\n",
            "Epoch: 3 | iter: 514 | train loss: 993.8166503906\n",
            "Epoch: 3 | iter: 515 | train loss: 598.5175170898\n",
            "Epoch: 3 | iter: 516 | train loss: 863.5968627930\n",
            "Epoch: 3 | iter: 517 | train loss: 658.0328979492\n",
            "Epoch: 3 | iter: 518 | train loss: 494.9733886719\n",
            "Epoch: 3 | iter: 519 | train loss: 695.9075927734\n",
            "Epoch: 3 | iter: 520 | train loss: 854.6387329102\n",
            "Epoch: 3 | iter: 521 | train loss: 467.7373657227\n",
            "Epoch: 3 | iter: 522 | train loss: 799.5160522461\n",
            "Epoch: 3 | iter: 523 | train loss: 671.1717529297\n",
            "Epoch: 3 | iter: 524 | train loss: 980.5462036133\n",
            "Epoch: 3 | iter: 525 | train loss: 846.7279663086\n",
            "Epoch: 3 | iter: 526 | train loss: 819.6528930664\n",
            "Epoch: 3 | iter: 527 | train loss: 309.6477050781\n",
            "Epoch: 3 | iter: 528 | train loss: 1338.5109863281\n",
            "Epoch: 3 | iter: 529 | train loss: 338.8754882812\n",
            "Epoch: 3 | iter: 530 | train loss: 791.1206665039\n",
            "Epoch: 3 | iter: 531 | train loss: 804.0286254883\n",
            "Epoch: 3 | iter: 532 | train loss: 939.8426513672\n",
            "Epoch: 3 | iter: 533 | train loss: 762.2206420898\n",
            "Epoch: 3 | iter: 534 | train loss: 1045.8438720703\n",
            "Epoch: 3 | iter: 535 | train loss: 518.1693115234\n",
            "Epoch: 3 | iter: 536 | train loss: 830.1185302734\n",
            "Epoch: 3 | iter: 537 | train loss: 739.2292480469\n",
            "Epoch: 3 | iter: 538 | train loss: 760.4667968750\n",
            "Epoch: 3 | iter: 539 | train loss: 921.1962890625\n",
            "Epoch: 3 | iter: 540 | train loss: 971.8798217773\n",
            "Epoch: 3 | iter: 541 | train loss: 619.5784912109\n",
            "Epoch: 3 | iter: 542 | train loss: 893.5169677734\n",
            "Epoch: 3 | iter: 543 | train loss: 717.8157958984\n",
            "Epoch: 3 | iter: 544 | train loss: 936.5328369141\n",
            "Epoch: 3 | iter: 545 | train loss: 1036.3815917969\n",
            "Epoch: 3 | iter: 546 | train loss: 926.5476074219\n",
            "Epoch: 3 | iter: 547 | train loss: 626.7009887695\n",
            "Epoch: 3 | iter: 548 | train loss: 398.7032470703\n",
            "Epoch: 3 | iter: 549 | train loss: 598.1186523438\n",
            "Epoch: 3 | iter: 550 | train loss: 750.5133056641\n",
            "Epoch: 3 | iter: 551 | train loss: 722.8219604492\n",
            "Epoch: 3 | iter: 552 | train loss: 1484.5363769531\n",
            "Epoch: 3 | iter: 553 | train loss: 863.8333129883\n",
            "Epoch: 3 | iter: 554 | train loss: 886.9779663086\n",
            "Epoch: 3 | iter: 555 | train loss: 402.5557556152\n",
            "Epoch: 3 | iter: 556 | train loss: 1472.6754150391\n",
            "Epoch: 3 | iter: 557 | train loss: 1603.9890136719\n",
            "Epoch: 3 | iter: 558 | train loss: 743.9512329102\n",
            "Epoch: 3 | iter: 559 | train loss: 966.8708496094\n",
            "Epoch: 3 | iter: 560 | train loss: 798.8534545898\n",
            "Epoch: 3 | iter: 561 | train loss: 608.6678466797\n",
            "Epoch: 3 | iter: 562 | train loss: 828.8858032227\n",
            "Epoch: 3 | iter: 563 | train loss: 548.8140258789\n",
            "Epoch: 3 | iter: 564 | train loss: 807.5192871094\n",
            "Epoch: 3 | iter: 565 | train loss: 876.2703247070\n",
            "Epoch: 3 | iter: 566 | train loss: 961.1693115234\n",
            "Epoch: 3 | iter: 567 | train loss: 482.9254760742\n",
            "Epoch: 3 | iter: 568 | train loss: 876.1629028320\n",
            "Epoch: 3 | iter: 569 | train loss: 377.4427795410\n",
            "Epoch: 3 | iter: 570 | train loss: 673.3548583984\n",
            "Epoch: 3 | iter: 571 | train loss: 1194.2095947266\n",
            "Epoch: 3 | iter: 572 | train loss: 1331.0452880859\n",
            "Epoch: 3 | iter: 573 | train loss: 1245.5067138672\n",
            "Epoch: 3 | iter: 574 | train loss: 734.9924926758\n",
            "Epoch: 3 | iter: 575 | train loss: 1321.7648925781\n",
            "Epoch: 3 | iter: 576 | train loss: 900.6243286133\n",
            "Epoch: 3 | iter: 577 | train loss: 892.7658081055\n",
            "Epoch: 3 | iter: 578 | train loss: 1429.7159423828\n",
            "Epoch: 3 | iter: 579 | train loss: 760.5339355469\n",
            "Epoch: 3 | iter: 580 | train loss: 1069.8482666016\n",
            "Epoch: 3 | iter: 581 | train loss: 493.1503906250\n",
            "Epoch: 3 | iter: 582 | train loss: 1247.3347167969\n",
            "Epoch: 3 | iter: 583 | train loss: 809.9116210938\n",
            "Epoch: 3 | iter: 584 | train loss: 762.0868530273\n",
            "Epoch: 3 | iter: 585 | train loss: 753.8031616211\n",
            "Epoch: 3 | iter: 586 | train loss: 759.5220336914\n",
            "Epoch: 3 | iter: 587 | train loss: 496.9122314453\n",
            "Epoch: 3 | iter: 588 | train loss: 1058.1088867188\n",
            "Epoch: 3 | iter: 589 | train loss: 542.3813476562\n",
            "Epoch: 3 | iter: 590 | train loss: 1011.1865844727\n",
            "Epoch: 3 | iter: 591 | train loss: 1021.5174560547\n",
            "Epoch: 3 | iter: 592 | train loss: 1439.8608398438\n",
            "Epoch: 3 | iter: 593 | train loss: 1076.6768798828\n",
            "Epoch: 3 | iter: 594 | train loss: 1166.6535644531\n",
            "Epoch: 3 | iter: 595 | train loss: 1348.2329101562\n",
            "Epoch: 3 | iter: 596 | train loss: 1245.1740722656\n",
            "Epoch: 3 | iter: 597 | train loss: 1279.8684082031\n",
            "Epoch: 3 | iter: 598 | train loss: 779.6388549805\n",
            "Epoch: 3 | iter: 599 | train loss: 1110.0909423828\n",
            "Epoch: 3 | iter: 600 | train loss: 908.6306152344\n",
            "Epoch: 3 | iter: 601 | train loss: 812.2033691406\n",
            "Epoch: 3 | iter: 602 | train loss: 848.5852050781\n",
            "Epoch: 3 | iter: 603 | train loss: 770.8134765625\n",
            "Epoch: 3 | iter: 604 | train loss: 622.4949340820\n",
            "Epoch: 3 | iter: 605 | train loss: 884.3328857422\n",
            "Epoch: 3 | iter: 606 | train loss: 426.2614746094\n",
            "Epoch: 3 | iter: 607 | train loss: 1240.7799072266\n",
            "Epoch: 3 | iter: 608 | train loss: 1087.0344238281\n",
            "Epoch: 3 | iter: 609 | train loss: 1116.6743164062\n",
            "Epoch: 3 | iter: 610 | train loss: 929.4588012695\n",
            "Epoch: 3 | iter: 611 | train loss: 1151.7163085938\n",
            "Epoch: 3 | iter: 612 | train loss: 1117.7298583984\n",
            "Epoch: 3 | iter: 613 | train loss: 1915.9343261719\n",
            "Epoch: 3 | iter: 614 | train loss: 863.4353637695\n",
            "Epoch: 3 | iter: 615 | train loss: 1185.0646972656\n",
            "Epoch: 3 | iter: 616 | train loss: 930.0273437500\n",
            "Epoch: 3 | iter: 617 | train loss: 816.8095092773\n",
            "Epoch: 3 | iter: 618 | train loss: 889.9321289062\n",
            "Epoch: 3 | iter: 619 | train loss: 400.1394042969\n",
            "Epoch: 3 | iter: 620 | train loss: 385.1972045898\n",
            "Epoch: 3 | iter: 621 | train loss: 328.6413574219\n",
            "Epoch: 3 | iter: 622 | train loss: 788.7668457031\n",
            "Epoch: 3 | iter: 623 | train loss: 361.5292968750\n",
            "Epoch: 3 | iter: 624 | train loss: 957.1425781250\n",
            "Epoch: 3 | iter: 625 | train loss: 623.1793823242\n",
            "Epoch: 3 | iter: 626 | train loss: 1153.5870361328\n",
            "Epoch: 3 | iter: 627 | train loss: 391.5971679688\n",
            "Epoch: 3 | iter: 628 | train loss: 1095.4702148438\n",
            "Epoch: 3 | iter: 629 | train loss: 1026.8898925781\n",
            "Epoch: 3 | iter: 630 | train loss: 1164.8135986328\n",
            "Epoch: 3 | iter: 631 | train loss: 1898.3640136719\n",
            "Epoch: 3 | iter: 632 | train loss: 1813.3601074219\n",
            "Epoch: 3 | iter: 633 | train loss: 1344.8032226562\n",
            "Epoch: 3 | iter: 634 | train loss: 963.6944580078\n",
            "Epoch: 3 | iter: 635 | train loss: 949.0068359375\n",
            "Epoch: 3 | iter: 636 | train loss: 441.2679748535\n",
            "Epoch: 3 | iter: 637 | train loss: 965.4471435547\n",
            "Epoch: 3 | iter: 638 | train loss: 1291.1143798828\n",
            "Epoch: 3 | iter: 639 | train loss: 1198.4064941406\n",
            "Epoch: 3 | iter: 640 | train loss: 1459.9200439453\n",
            "Epoch: 3 | iter: 641 | train loss: 680.5810546875\n",
            "Epoch: 3 | iter: 642 | train loss: 965.3209838867\n",
            "Epoch: 3 | iter: 643 | train loss: 957.7290039062\n",
            "Epoch: 3 | iter: 644 | train loss: 432.5347595215\n",
            "Epoch: 3 | iter: 645 | train loss: 478.2009582520\n",
            "Epoch: 3 | iter: 646 | train loss: 1365.7757568359\n",
            "Epoch: 3 | iter: 647 | train loss: 702.0575561523\n",
            "Epoch: 3 | iter: 648 | train loss: 1396.1081542969\n",
            "Epoch: 3 | iter: 649 | train loss: 418.7254333496\n",
            "Epoch: 3 | iter: 650 | train loss: 1125.7923583984\n",
            "Epoch: 3 | iter: 651 | train loss: 1462.8559570312\n",
            "Epoch: 3 | iter: 652 | train loss: 975.6461181641\n",
            "Epoch: 3 | iter: 653 | train loss: 1107.5877685547\n",
            "Epoch: 3 | iter: 654 | train loss: 1352.9050292969\n",
            "Epoch: 3 | iter: 655 | train loss: 693.3344116211\n",
            "Epoch: 3 | iter: 656 | train loss: 1166.3717041016\n",
            "Epoch: 3 | iter: 657 | train loss: 1152.3984375000\n",
            "Epoch: 3 | iter: 658 | train loss: 920.8585815430\n",
            "Epoch: 3 | iter: 659 | train loss: 1551.6667480469\n",
            "Epoch: 3 | iter: 660 | train loss: 1144.0747070312\n",
            "Epoch: 3 | iter: 661 | train loss: 2264.5427246094\n",
            "Epoch: 3 | iter: 662 | train loss: 1070.9154052734\n",
            "Epoch: 3 | iter: 663 | train loss: 355.5304260254\n",
            "Epoch: 3 | iter: 664 | train loss: 881.0573120117\n",
            "Epoch: 3 | iter: 665 | train loss: 999.1147460938\n",
            "Epoch: 3 | iter: 666 | train loss: 1173.8444824219\n",
            "Epoch: 3 | iter: 667 | train loss: 920.0266113281\n",
            "Epoch: 3 | iter: 668 | train loss: 768.3734741211\n",
            "Epoch: 3 | iter: 669 | train loss: 1865.8989257812\n",
            "Epoch: 3 | iter: 670 | train loss: 1459.4783935547\n",
            "Epoch: 3 | iter: 671 | train loss: 1308.0960693359\n",
            "Epoch: 3 | iter: 672 | train loss: 268.6972045898\n",
            "Epoch: 3 | iter: 673 | train loss: 327.7247619629\n",
            "Epoch: 3 | iter: 674 | train loss: 573.9322509766\n",
            "Epoch: 3 | iter: 675 | train loss: 778.5647583008\n",
            "Epoch: 3 | iter: 676 | train loss: 484.0439147949\n",
            "Epoch: 3 | iter: 677 | train loss: 923.2794799805\n",
            "Epoch: 3 | iter: 678 | train loss: 199.0867767334\n",
            "Epoch: 3 | iter: 679 | train loss: 283.2853088379\n",
            "Epoch: 3 | iter: 680 | train loss: 556.5572509766\n",
            "Epoch: 3 | iter: 681 | train loss: 218.3934173584\n",
            "Epoch: 3 | iter: 682 | train loss: 1096.8104248047\n",
            "Epoch: 3 | iter: 683 | train loss: 488.6957397461\n",
            "Epoch: 3 | iter: 684 | train loss: 456.4297180176\n",
            "Epoch: 3 | iter: 685 | train loss: 158.9300537109\n",
            "Epoch: 3 | iter: 686 | train loss: 472.7209167480\n",
            "Epoch: 3 | iter: 687 | train loss: 1665.4188232422\n",
            "Epoch: 3 | iter: 688 | train loss: 1055.3325195312\n",
            "Epoch: 3 | iter: 689 | train loss: 820.6547851562\n",
            "Epoch: 3 | iter: 690 | train loss: 418.1946716309\n",
            "Epoch: 3 | iter: 691 | train loss: 846.6190795898\n",
            "Epoch: 3 | iter: 692 | train loss: 1103.7966308594\n",
            "Epoch: 3 | iter: 693 | train loss: 250.0564270020\n",
            "Epoch: 3 | iter: 694 | train loss: 897.4298706055\n",
            "Epoch: 3 | iter: 695 | train loss: 266.3620910645\n",
            "Epoch: 3 | iter: 696 | train loss: 376.7583923340\n",
            "Epoch: 3 | iter: 697 | train loss: 362.4234924316\n",
            "Epoch: 3 | iter: 698 | train loss: 545.0667724609\n",
            "Epoch: 3 | iter: 699 | train loss: 1557.7010498047\n",
            "Epoch: 3 | iter: 700 | train loss: 2209.7299804688\n",
            "Epoch: 3 | iter: 701 | train loss: 1221.2535400391\n",
            "Epoch: 3 | iter: 702 | train loss: 1070.8518066406\n",
            "Epoch: 3 | iter: 703 | train loss: 1961.3450927734\n",
            "Epoch: 3 | iter: 704 | train loss: 651.7678222656\n",
            "Epoch: 3 | iter: 705 | train loss: 912.3400878906\n",
            "Epoch: 3 | iter: 706 | train loss: 415.6697387695\n",
            "Epoch: 3 | iter: 707 | train loss: 1436.2413330078\n",
            "Epoch: 3 | iter: 708 | train loss: 917.8923950195\n",
            "Epoch: 3 | iter: 709 | train loss: 2226.9094238281\n",
            "Epoch: 3 | iter: 710 | train loss: 1241.4637451172\n",
            "Epoch: 3 | iter: 711 | train loss: 656.8880615234\n",
            "Epoch: 3 | iter: 712 | train loss: 2261.0891113281\n",
            "Epoch: 3 | iter: 713 | train loss: 1595.1804199219\n",
            "Epoch: 3 | iter: 714 | train loss: 775.9072265625\n",
            "Epoch: 3 | iter: 715 | train loss: 2300.5932617188\n",
            "Epoch: 3 | iter: 716 | train loss: 1038.2960205078\n",
            "Epoch: 3 | iter: 717 | train loss: 1098.1650390625\n",
            "Epoch: 3 | iter: 718 | train loss: 365.5718383789\n",
            "Epoch: 3 | iter: 719 | train loss: 1803.1691894531\n",
            "Epoch: 3 | iter: 720 | train loss: 773.6612548828\n",
            "Epoch: 3 | iter: 721 | train loss: 1501.6784667969\n",
            "Epoch: 3 | iter: 722 | train loss: 1483.7584228516\n",
            "Epoch: 3 | iter: 723 | train loss: 375.6608886719\n",
            "Epoch: 3 | iter: 724 | train loss: 1447.7229003906\n",
            "Epoch: 3 | iter: 725 | train loss: 2396.9799804688\n",
            "Epoch: 3 | iter: 726 | train loss: 787.2995605469\n",
            "Epoch: 3 | iter: 727 | train loss: 1964.2473144531\n",
            "Epoch: 3 | iter: 728 | train loss: 896.5272827148\n",
            "Epoch: 3 | iter: 729 | train loss: 1316.2447509766\n",
            "Epoch: 3 | iter: 730 | train loss: 1121.4804687500\n",
            "Epoch: 3 | iter: 731 | train loss: 510.1111450195\n",
            "Epoch: 3 | iter: 732 | train loss: 818.3453979492\n",
            "Epoch: 3 | iter: 733 | train loss: 1303.1573486328\n",
            "Epoch: 3 | iter: 734 | train loss: 2151.4987792969\n",
            "Epoch: 3 | iter: 735 | train loss: 691.5958862305\n",
            "Epoch: 3 | iter: 736 | train loss: 1638.1524658203\n",
            "Epoch: 3 | iter: 737 | train loss: 984.6876220703\n",
            "Epoch: 3 | iter: 738 | train loss: 1457.7088623047\n",
            "Epoch: 3 | iter: 739 | train loss: 1307.2419433594\n",
            "Epoch: 3 | iter: 740 | train loss: 538.0139770508\n",
            "Epoch: 3 | iter: 741 | train loss: 1145.2489013672\n",
            "Epoch: 3 | iter: 742 | train loss: 1626.4558105469\n",
            "Epoch: 3 | iter: 743 | train loss: 1224.7899169922\n",
            "Epoch: 3 | iter: 744 | train loss: 1034.8963623047\n",
            "Epoch: 3 | iter: 745 | train loss: 805.5269165039\n",
            "Epoch: 3 | iter: 746 | train loss: 656.2707519531\n",
            "Epoch: 3 | iter: 747 | train loss: 2274.3530273438\n",
            "Epoch: 3 | iter: 748 | train loss: 1078.8798828125\n",
            "Epoch: 3 | iter: 749 | train loss: 1216.4797363281\n",
            "Epoch: 3 | iter: 750 | train loss: 1125.5379638672\n",
            "Epoch: 3 | iter: 751 | train loss: 446.7123413086\n",
            "Epoch: 3 | iter: 752 | train loss: 2297.5202636719\n",
            "Epoch: 3 | iter: 753 | train loss: 759.9888916016\n",
            "Epoch: 3 | iter: 754 | train loss: 1181.0682373047\n",
            "Epoch: 3 | iter: 755 | train loss: 876.9962768555\n",
            "Epoch: 3 | iter: 756 | train loss: 1017.7872314453\n",
            "Epoch: 3 | iter: 757 | train loss: 589.8153686523\n",
            "Epoch: 3 | iter: 758 | train loss: 802.1092529297\n",
            "Epoch: 3 | iter: 759 | train loss: 947.4946289062\n",
            "Epoch: 3 | iter: 760 | train loss: 968.9198608398\n",
            "Epoch: 3 | iter: 761 | train loss: 719.5053100586\n",
            "Epoch: 3 | iter: 762 | train loss: 1169.6854248047\n",
            "Epoch: 3 | iter: 763 | train loss: 855.5296020508\n",
            "Epoch: 3 | iter: 764 | train loss: 1057.8874511719\n",
            "Epoch: 3 | iter: 765 | train loss: 2123.4538574219\n",
            "Epoch: 3 | iter: 766 | train loss: 757.5479125977\n",
            "Epoch: 3 | iter: 767 | train loss: 844.5687866211\n",
            "Epoch: 3 | iter: 768 | train loss: 971.5471801758\n",
            "Epoch: 3 | iter: 769 | train loss: 1076.1484375000\n",
            "Epoch: 3 | iter: 770 | train loss: 941.7349853516\n",
            "Epoch: 3 | iter: 771 | train loss: 875.1952514648\n",
            "Epoch: 3 | iter: 772 | train loss: 232.2501373291\n",
            "Epoch: 3 | iter: 773 | train loss: 2221.9206542969\n",
            "Epoch: 3 | iter: 774 | train loss: 1295.7850341797\n",
            "Epoch: 3 | iter: 775 | train loss: 417.0111999512\n",
            "Epoch: 3 | iter: 776 | train loss: 1250.2357177734\n",
            "Epoch: 3 | iter: 777 | train loss: 2030.5806884766\n",
            "Epoch: 3 | iter: 778 | train loss: 1638.5349121094\n",
            "Epoch: 3 | iter: 779 | train loss: 407.0856018066\n",
            "Epoch: 3 | iter: 780 | train loss: 906.3544921875\n",
            "Epoch: 3 | iter: 781 | train loss: 1036.4586181641\n",
            "Epoch: 3 | iter: 782 | train loss: 1119.1075439453\n",
            "Epoch: 3 | iter: 783 | train loss: 733.3489990234\n",
            "Epoch: 3 | iter: 784 | train loss: 801.5665893555\n",
            "Epoch: 3 | iter: 785 | train loss: 812.6449584961\n",
            "Epoch: 3 | iter: 786 | train loss: 720.7857055664\n",
            "Epoch: 3 | iter: 787 | train loss: 1839.2584228516\n",
            "Epoch: 3 | iter: 788 | train loss: 644.6121826172\n",
            "Epoch: 3 | iter: 789 | train loss: 1174.6906738281\n",
            "Epoch: 3 | iter: 790 | train loss: 1268.9305419922\n",
            "Epoch: 3 | iter: 791 | train loss: 712.7775878906\n",
            "Epoch: 3 | iter: 792 | train loss: 1200.5728759766\n",
            "Epoch: 3 | iter: 793 | train loss: 894.6041259766\n",
            "Epoch: 3 | iter: 794 | train loss: 757.0173950195\n",
            "Epoch: 3 | iter: 795 | train loss: 899.9481811523\n",
            "Epoch: 3 | iter: 796 | train loss: 842.7575073242\n",
            "Epoch: 3 | iter: 797 | train loss: 689.1992187500\n",
            "Epoch: 3 | iter: 798 | train loss: 1989.5502929688\n",
            "Epoch: 3 | iter: 799 | train loss: 2114.5791015625\n",
            "Epoch: 3 | iter: 800 | train loss: 1188.5181884766\n",
            "Epoch: 3 | iter: 801 | train loss: 922.3403320312\n",
            "Epoch: 3 | iter: 802 | train loss: 867.8411254883\n",
            "Epoch: 3 | iter: 803 | train loss: 660.9166259766\n",
            "Epoch: 3 | iter: 804 | train loss: 710.4109497070\n",
            "Epoch: 3 | iter: 805 | train loss: 1046.1235351562\n",
            "Epoch: 3 | iter: 806 | train loss: 515.0088500977\n",
            "Epoch: 3 | iter: 807 | train loss: 412.5683288574\n",
            "Epoch: 3 | iter: 808 | train loss: 498.5606079102\n",
            "Epoch: 3 | iter: 809 | train loss: 663.8936157227\n",
            "Epoch: 3 | iter: 810 | train loss: 786.8615112305\n",
            "Epoch: 3 | iter: 811 | train loss: 1021.6801147461\n",
            "Epoch: 3 | iter: 812 | train loss: 523.8358764648\n",
            "Epoch: 3 | iter: 813 | train loss: 992.3975219727\n",
            "Epoch: 3 | iter: 814 | train loss: 1720.5262451172\n",
            "Epoch: 3 | iter: 815 | train loss: 849.3574218750\n",
            "Epoch: 3 | iter: 816 | train loss: 1167.8031005859\n",
            "Epoch: 3 | iter: 817 | train loss: 814.9950561523\n",
            "Epoch: 3 | iter: 818 | train loss: 1217.5253906250\n",
            "Epoch: 3 | iter: 819 | train loss: 779.2307128906\n",
            "Epoch: 3 | iter: 820 | train loss: 2145.0815429688\n",
            "Epoch: 3 | iter: 821 | train loss: 1094.6137695312\n",
            "Epoch: 3 | iter: 822 | train loss: 1856.6329345703\n",
            "Epoch: 3 | iter: 823 | train loss: 677.9793701172\n",
            "Epoch: 3 | iter: 824 | train loss: 648.8101806641\n",
            "Epoch: 3 | iter: 825 | train loss: 1372.3493652344\n",
            "Epoch: 3 | iter: 826 | train loss: 840.6395263672\n",
            "Epoch: 3 | iter: 827 | train loss: 1224.1815185547\n",
            "Epoch: 3 | iter: 828 | train loss: 1111.9918212891\n",
            "Epoch: 3 | iter: 829 | train loss: 1079.6822509766\n",
            "Epoch: 3 | iter: 830 | train loss: 470.4310913086\n",
            "Epoch: 3 | iter: 831 | train loss: 1237.3828125000\n",
            "Epoch: 3 | iter: 832 | train loss: 1323.2978515625\n",
            "Epoch: 3 | iter: 833 | train loss: 801.2684936523\n",
            "Epoch: 3 | iter: 834 | train loss: 1222.2635498047\n",
            "Epoch: 3 | iter: 835 | train loss: 763.5025634766\n",
            "Epoch: 3 | iter: 836 | train loss: 1331.3026123047\n",
            "Epoch: 3 | iter: 837 | train loss: 1019.3002929688\n",
            "Epoch: 3 | iter: 838 | train loss: 781.2395629883\n",
            "Epoch: 3 | iter: 839 | train loss: 1433.6811523438\n",
            "Epoch: 3 | iter: 840 | train loss: 650.7268066406\n",
            "Epoch: 3 | iter: 841 | train loss: 1116.0642089844\n",
            "Epoch: 3 | iter: 842 | train loss: 833.8101196289\n",
            "Epoch: 3 | iter: 843 | train loss: 1304.8148193359\n",
            "Epoch: 3 | iter: 844 | train loss: 1009.1928710938\n",
            "Epoch: 3 | iter: 845 | train loss: 353.8275756836\n",
            "Epoch: 3 | iter: 846 | train loss: 1317.0415039062\n",
            "Epoch: 3 | iter: 847 | train loss: 759.6207275391\n",
            "Epoch: 3 | iter: 848 | train loss: 478.1216430664\n",
            "Epoch: 3 | iter: 849 | train loss: 1109.5826416016\n",
            "Epoch: 3 | iter: 850 | train loss: 407.1290283203\n",
            "Epoch: 3 | iter: 851 | train loss: 1035.0452880859\n",
            "Epoch: 3 | iter: 852 | train loss: 528.8477783203\n",
            "Epoch: 3 | iter: 853 | train loss: 578.3587036133\n",
            "Epoch: 3 | iter: 854 | train loss: 832.4183959961\n",
            "Epoch: 3 | iter: 855 | train loss: 750.9032592773\n",
            "Epoch: 3 | iter: 856 | train loss: 779.1571044922\n",
            "Epoch: 3 | iter: 857 | train loss: 923.4716186523\n",
            "Epoch: 3 | iter: 858 | train loss: 1125.3020019531\n",
            "Epoch: 3 | iter: 859 | train loss: 1016.6663818359\n",
            "Epoch: 3 | iter: 860 | train loss: 826.0509033203\n",
            "Epoch: 3 | iter: 861 | train loss: 284.6256408691\n",
            "Epoch: 3 | iter: 862 | train loss: 560.8117675781\n",
            "Epoch: 3 | iter: 863 | train loss: 528.3255615234\n",
            "Epoch: 3 | iter: 864 | train loss: 1031.2175292969\n",
            "Epoch: 3 | iter: 865 | train loss: 626.0416870117\n",
            "Epoch: 3 | iter: 866 | train loss: 1061.5169677734\n",
            "Epoch: 3 | iter: 867 | train loss: 1374.3460693359\n",
            "Epoch: 3 | iter: 868 | train loss: 1221.5184326172\n",
            "Epoch: 3 | iter: 869 | train loss: 1416.0875244141\n",
            "Epoch: 3 | iter: 870 | train loss: 180.5557403564\n",
            "Epoch: 3 | iter: 871 | train loss: 476.9488830566\n",
            "Epoch: 3 | iter: 872 | train loss: 296.8154602051\n",
            "Epoch: 3 | iter: 873 | train loss: 892.5723876953\n",
            "Epoch: 3 | iter: 874 | train loss: 655.3265380859\n",
            "Epoch: 3 | iter: 875 | train loss: 1830.0002441406\n",
            "Epoch: 3 | iter: 876 | train loss: 999.2363891602\n",
            "Epoch: 3 | iter: 877 | train loss: 1043.5894775391\n",
            "Epoch: 3 | iter: 878 | train loss: 478.1524658203\n",
            "Epoch: 3 | iter: 879 | train loss: 608.1553955078\n",
            "Epoch: 3 | iter: 880 | train loss: 845.8557128906\n",
            "Epoch: 3 | iter: 881 | train loss: 353.4531250000\n",
            "Epoch: 3 | iter: 882 | train loss: 835.6562500000\n",
            "Epoch: 3 | iter: 883 | train loss: 1607.0064697266\n",
            "Epoch: 3 | iter: 884 | train loss: 720.9928588867\n",
            "Epoch: 3 | iter: 885 | train loss: 740.6669921875\n",
            "Epoch: 3 | iter: 886 | train loss: 1120.5272216797\n",
            "Epoch: 3 | iter: 887 | train loss: 822.0856323242\n",
            "Epoch: 3 | iter: 888 | train loss: 1218.7874755859\n",
            "Epoch: 3 | iter: 889 | train loss: 703.8623657227\n",
            "Epoch: 3 | iter: 890 | train loss: 2122.3789062500\n",
            "Epoch: 3 | iter: 891 | train loss: 1182.4450683594\n",
            "Epoch: 3 | iter: 892 | train loss: 2072.7333984375\n",
            "Epoch: 3 | iter: 893 | train loss: 952.8895263672\n",
            "Epoch: 3 | iter: 894 | train loss: 1770.9898681641\n",
            "Epoch: 3 | iter: 895 | train loss: 304.7983703613\n",
            "Epoch: 3 | iter: 896 | train loss: 295.5458374023\n",
            "Epoch: 3 | iter: 897 | train loss: 847.3877563477\n",
            "Epoch: 3 | iter: 898 | train loss: 1008.5996093750\n",
            "Epoch: 3 | iter: 899 | train loss: 506.6781616211\n",
            "Epoch: 3 | iter: 900 | train loss: 952.3647460938\n",
            "Epoch: 3 | iter: 901 | train loss: 1627.4224853516\n",
            "Epoch: 3 | iter: 902 | train loss: 421.3404235840\n",
            "Epoch: 3 | iter: 903 | train loss: 817.5405273438\n",
            "Epoch: 3 | iter: 904 | train loss: 900.6195068359\n",
            "Epoch: 3 | iter: 905 | train loss: 1118.4586181641\n",
            "Epoch: 3 | iter: 906 | train loss: 966.5402221680\n",
            "Epoch: 3 | iter: 907 | train loss: 1405.5924072266\n",
            "Epoch: 3 | iter: 908 | train loss: 258.4079284668\n",
            "Epoch: 3 | iter: 909 | train loss: 544.9456787109\n",
            "Epoch: 3 | iter: 910 | train loss: 1066.2935791016\n",
            "Epoch: 3 | iter: 911 | train loss: 1626.4753417969\n",
            "Epoch: 3 | iter: 912 | train loss: 541.7987060547\n",
            "Epoch: 3 | iter: 913 | train loss: 417.1230468750\n",
            "Epoch: 3 | iter: 914 | train loss: 693.9680175781\n",
            "Epoch: 3 | iter: 915 | train loss: 654.5180053711\n",
            "Epoch: 3 | iter: 916 | train loss: 503.4458618164\n",
            "Epoch: 3 | iter: 917 | train loss: 869.7176513672\n",
            "Epoch: 3 | iter: 918 | train loss: 390.4704589844\n",
            "Epoch: 3 | iter: 919 | train loss: 441.3442077637\n",
            "Epoch: 3 | iter: 920 | train loss: 580.5042114258\n",
            "Epoch: 3 | iter: 921 | train loss: 910.3295898438\n",
            "Epoch: 3 | iter: 922 | train loss: 352.8662719727\n",
            "Epoch: 3 | iter: 923 | train loss: 1439.1843261719\n",
            "Epoch: 3 | iter: 924 | train loss: 1129.4346923828\n",
            "Epoch: 3 | iter: 925 | train loss: 684.6759033203\n",
            "Epoch: 3 | iter: 926 | train loss: 1493.4006347656\n",
            "Epoch: 3 | iter: 927 | train loss: 1653.2758789062\n",
            "Epoch: 3 | iter: 928 | train loss: 1508.2106933594\n",
            "Epoch: 3 | iter: 929 | train loss: 950.5399169922\n",
            "Epoch: 3 | iter: 930 | train loss: 429.0536499023\n",
            "Epoch: 3 | iter: 931 | train loss: 785.1264038086\n",
            "Epoch: 3 | iter: 932 | train loss: 1487.9400634766\n",
            "Epoch: 3 | iter: 933 | train loss: 764.0419311523\n",
            "Epoch: 3 | iter: 934 | train loss: 1088.2258300781\n",
            "Epoch: 3 | iter: 935 | train loss: 1250.1810302734\n",
            "Epoch: 3 | iter: 936 | train loss: 540.6594848633\n",
            "Epoch: 3 | iter: 937 | train loss: 699.8008422852\n",
            "Epoch: 3 | iter: 938 | train loss: 493.1996154785\n",
            "Epoch: 3 | iter: 939 | train loss: 1734.1717529297\n",
            "Epoch: 3 | iter: 940 | train loss: 892.8621826172\n",
            "Epoch: 3 | iter: 941 | train loss: 309.7270507812\n",
            "Epoch: 3 | iter: 942 | train loss: 692.2368774414\n",
            "Epoch: 3 | iter: 943 | train loss: 1641.4747314453\n",
            "Epoch: 3 | iter: 944 | train loss: 193.7572174072\n",
            "Epoch: 3 | iter: 945 | train loss: 854.6368408203\n",
            "Epoch: 3 | iter: 946 | train loss: 1853.1777343750\n",
            "Epoch: 3 | iter: 947 | train loss: 465.8219299316\n",
            "Epoch: 3 | iter: 948 | train loss: 868.4953002930\n",
            "Epoch: 3 | iter: 949 | train loss: 672.0753173828\n",
            "Epoch: 3 | iter: 950 | train loss: 1532.7890625000\n",
            "Epoch: 3 | iter: 951 | train loss: 756.2104492188\n",
            "Epoch: 3 | iter: 952 | train loss: 817.5296020508\n",
            "Epoch: 3 | iter: 953 | train loss: 852.8006591797\n",
            "Epoch: 3 | iter: 954 | train loss: 550.6220092773\n",
            "Epoch: 3 | iter: 955 | train loss: 1656.9091796875\n",
            "Epoch: 3 | iter: 956 | train loss: 695.8984985352\n",
            "Epoch: 3 | iter: 957 | train loss: 689.9111938477\n",
            "Epoch: 3 | iter: 958 | train loss: 1122.0349121094\n",
            "Epoch: 3 | iter: 959 | train loss: 859.6459350586\n",
            "Epoch: 3 | iter: 960 | train loss: 579.3425903320\n",
            "Epoch: 3 | iter: 961 | train loss: 639.1065063477\n",
            "Epoch: 3 | iter: 962 | train loss: 551.6239013672\n",
            "Epoch: 3 | iter: 963 | train loss: 933.8975830078\n",
            "Epoch: 3 | iter: 964 | train loss: 724.8616943359\n",
            "Epoch: 3 | iter: 965 | train loss: 1185.9615478516\n",
            "Epoch: 3 | iter: 966 | train loss: 896.0169677734\n",
            "Epoch: 3 | iter: 967 | train loss: 928.4074096680\n",
            "Epoch: 3 | iter: 968 | train loss: 806.4135742188\n",
            "Epoch: 3 | iter: 969 | train loss: 645.7192993164\n",
            "Epoch: 3 | iter: 970 | train loss: 539.1442260742\n",
            "Epoch: 3 | iter: 971 | train loss: 132.4921112061\n",
            "Epoch: 3 | iter: 972 | train loss: 1290.7086181641\n",
            "Epoch: 3 | iter: 973 | train loss: 810.3197021484\n",
            "Epoch: 3 | iter: 974 | train loss: 687.3399047852\n",
            "Epoch: 3 | iter: 975 | train loss: 203.8822937012\n",
            "Epoch: 3 | iter: 976 | train loss: 451.5759887695\n",
            "Epoch: 3 | iter: 977 | train loss: 461.9382019043\n",
            "Epoch: 3 | iter: 978 | train loss: 903.4032592773\n",
            "Epoch: 3 | iter: 979 | train loss: 790.0449829102\n",
            "Epoch: 3 | iter: 980 | train loss: 480.5528869629\n",
            "Epoch: 3 | iter: 981 | train loss: 1152.1676025391\n",
            "Epoch: 3 | iter: 982 | train loss: 1174.5888671875\n",
            "Epoch: 3 | iter: 983 | train loss: 347.6858215332\n",
            "Epoch: 3 | iter: 984 | train loss: 849.9024047852\n",
            "Epoch: 3 | iter: 985 | train loss: 1115.2524414062\n",
            "Epoch: 3 | iter: 986 | train loss: 714.8761596680\n",
            "Epoch: 3 | iter: 987 | train loss: 1429.8505859375\n",
            "Epoch: 3 | iter: 988 | train loss: 933.8214721680\n",
            "Epoch: 3 | iter: 989 | train loss: 852.8923950195\n",
            "Epoch: 3 | iter: 990 | train loss: 950.6217041016\n",
            "Epoch: 3 | iter: 991 | train loss: 563.9547729492\n",
            "Epoch: 3 | iter: 992 | train loss: 1594.0428466797\n",
            "Epoch: 3 | iter: 993 | train loss: 614.9553833008\n",
            "Epoch: 3 | iter: 994 | train loss: 607.5180664062\n",
            "Epoch: 3 | iter: 995 | train loss: 961.3372192383\n",
            "Epoch: 3 | iter: 996 | train loss: 496.5467834473\n",
            "Epoch: 3 | iter: 997 | train loss: 923.7568359375\n",
            "Epoch: 3 | iter: 998 | train loss: 400.2032470703\n",
            "Epoch: 3 | iter: 999 | train loss: 458.8305358887\n",
            "Epoch: 3 | iter: 1000 | train loss: 225.9878845215\n",
            "Epoch: 3 | iter: 1001 | train loss: 1367.9025878906\n",
            "Epoch: 3 | iter: 1002 | train loss: 940.9578857422\n",
            "Epoch: 3 | iter: 1003 | train loss: 895.0430908203\n",
            "Epoch: 3 | iter: 1004 | train loss: 430.0529479980\n",
            "Epoch: 3 | iter: 1005 | train loss: 390.2733764648\n",
            "Epoch: 3 | iter: 1006 | train loss: 615.9689331055\n",
            "Epoch: 3 | iter: 1007 | train loss: 643.0330810547\n",
            "Epoch: 3 | iter: 1008 | train loss: 1163.5538330078\n",
            "Epoch: 3 | iter: 1009 | train loss: 809.1105957031\n",
            "Epoch: 3 | iter: 1010 | train loss: 1572.5374755859\n",
            "Epoch: 3 | iter: 1011 | train loss: 929.5816650391\n",
            "Epoch: 3 | iter: 1012 | train loss: 642.9181518555\n",
            "Epoch: 3 | iter: 1013 | train loss: 760.7734375000\n",
            "Epoch: 3 | iter: 1014 | train loss: 472.6839294434\n",
            "Epoch: 3 | iter: 1015 | train loss: 591.1481323242\n",
            "Epoch: 3 | iter: 1016 | train loss: 513.7488403320\n",
            "Epoch: 3 | iter: 1017 | train loss: 1138.3793945312\n",
            "Epoch: 3 | iter: 1018 | train loss: 889.5820312500\n",
            "Epoch: 3 | iter: 1019 | train loss: 416.7653198242\n",
            "Epoch: 3 | iter: 1020 | train loss: 790.2410278320\n",
            "Epoch: 3 | iter: 1021 | train loss: 779.4412841797\n",
            "Epoch: 3 | iter: 1022 | train loss: 874.1634521484\n",
            "Epoch: 3 | iter: 1023 | train loss: 472.2590942383\n",
            "Epoch: 3 | iter: 1024 | train loss: 832.1862792969\n",
            "Epoch: 3 | iter: 1025 | train loss: 578.5645751953\n",
            "Epoch: 3 | iter: 1026 | train loss: 983.4086303711\n",
            "Epoch: 3 | iter: 1027 | train loss: 561.2973632812\n",
            "Epoch: 3 | iter: 1028 | train loss: 900.8080444336\n",
            "Epoch: 3 | iter: 1029 | train loss: 743.7434692383\n",
            "Epoch: 3 | iter: 1030 | train loss: 1572.8154296875\n",
            "Epoch: 3 | iter: 1031 | train loss: 1150.6518554688\n",
            "Epoch: 3 | iter: 1032 | train loss: 656.4849243164\n",
            "Epoch: 3 | iter: 1033 | train loss: 1054.4086914062\n",
            "Epoch: 3 | iter: 1034 | train loss: 668.5115356445\n",
            "Epoch: 3 | iter: 1035 | train loss: 904.3828125000\n",
            "Epoch: 3 | iter: 1036 | train loss: 1560.3598632812\n",
            "Epoch: 3 | iter: 1037 | train loss: 647.2314453125\n",
            "Epoch: 3 | iter: 1038 | train loss: 451.4414978027\n",
            "Epoch: 3 | iter: 1039 | train loss: 554.1796875000\n",
            "Epoch: 3 | iter: 1040 | train loss: 317.7250061035\n",
            "Epoch: 3 | iter: 1041 | train loss: 516.6273193359\n",
            "Epoch: 3 | iter: 1042 | train loss: 373.1358947754\n",
            "Epoch: 3 | iter: 1043 | train loss: 806.4756469727\n",
            "Epoch: 3 | iter: 1044 | train loss: 733.6083984375\n",
            "Epoch: 3 | iter: 1045 | train loss: 696.3663940430\n",
            "Epoch: 3 | iter: 1046 | train loss: 438.7514953613\n",
            "Epoch: 3 | iter: 1047 | train loss: 271.1846618652\n",
            "Epoch: 3 | iter: 1048 | train loss: 678.2939453125\n",
            "Epoch: 3 | iter: 1049 | train loss: 686.4965209961\n",
            "Epoch: 3 | iter: 1050 | train loss: 1686.5518798828\n",
            "Epoch: 3 | iter: 1051 | train loss: 1025.6350097656\n",
            "Epoch: 3 | iter: 1052 | train loss: 736.4219970703\n",
            "Epoch: 3 | iter: 1053 | train loss: 289.6925964355\n",
            "Epoch: 3 | iter: 1054 | train loss: 391.1570129395\n",
            "Epoch: 3 | iter: 1055 | train loss: 561.2414550781\n",
            "Epoch: 3 | iter: 1056 | train loss: 534.3128662109\n",
            "Epoch: 3 | iter: 1057 | train loss: 492.7734985352\n",
            "Epoch: 3 | iter: 1058 | train loss: 330.2179565430\n",
            "Epoch: 3 | iter: 1059 | train loss: 385.4111938477\n",
            "Epoch: 3 | iter: 1060 | train loss: 528.7181396484\n",
            "Epoch: 3 | iter: 1061 | train loss: 1092.8524169922\n",
            "Epoch: 3 | iter: 1062 | train loss: 949.3643798828\n",
            "Epoch: 3 | iter: 1063 | train loss: 360.4596862793\n",
            "Epoch: 3 | iter: 1064 | train loss: 1097.1539306641\n",
            "Epoch: 3 | iter: 1065 | train loss: 762.9273681641\n",
            "Epoch: 3 | iter: 1066 | train loss: 320.3720397949\n",
            "Epoch: 3 | iter: 1067 | train loss: 1316.2697753906\n",
            "Epoch: 3 | iter: 1068 | train loss: 1113.1512451172\n",
            "Epoch: 3 | iter: 1069 | train loss: 833.7306518555\n",
            "Epoch: 3 | iter: 1070 | train loss: 627.9255371094\n",
            "Epoch: 3 | iter: 1071 | train loss: 855.3136596680\n",
            "Epoch: 3 | iter: 1072 | train loss: 578.6121215820\n",
            "Epoch: 3 | iter: 1073 | train loss: 752.5304565430\n",
            "Epoch: 3 | iter: 1074 | train loss: 354.0088500977\n",
            "Epoch: 3 | iter: 1075 | train loss: 804.4426269531\n",
            "Epoch: 3 | iter: 1076 | train loss: 476.4172668457\n",
            "Epoch: 3 | iter: 1077 | train loss: 1009.2631835938\n",
            "Epoch: 3 | iter: 1078 | train loss: 899.6213989258\n",
            "Epoch: 3 | iter: 1079 | train loss: 987.7267456055\n",
            "Epoch: 3 | iter: 1080 | train loss: 1096.7120361328\n",
            "Epoch: 3 | iter: 1081 | train loss: 1911.7818603516\n",
            "Epoch: 3 | iter: 1082 | train loss: 1332.7539062500\n",
            "Epoch: 3 | iter: 1083 | train loss: 739.4941406250\n",
            "Epoch: 3 | iter: 1084 | train loss: 885.7980346680\n",
            "Epoch: 3 | iter: 1085 | train loss: 1508.5433349609\n",
            "Epoch: 3 | iter: 1086 | train loss: 771.6077880859\n",
            "Epoch: 3 | iter: 1087 | train loss: 1501.9827880859\n",
            "Epoch: 3 | iter: 1088 | train loss: 644.2980957031\n",
            "Epoch: 3 | iter: 1089 | train loss: 1329.9580078125\n",
            "Epoch: 3 | iter: 1090 | train loss: 1395.8959960938\n",
            "Epoch: 3 | iter: 1091 | train loss: 388.7066650391\n",
            "Epoch: 3 | iter: 1092 | train loss: 986.9948120117\n",
            "Epoch: 3 | iter: 1093 | train loss: 385.4079895020\n",
            "Epoch: 3 | iter: 1094 | train loss: 1689.4342041016\n",
            "Epoch: 3 | iter: 1095 | train loss: 1051.1080322266\n",
            "Epoch: 3 | iter: 1096 | train loss: 677.3259277344\n",
            "Epoch: 3 | iter: 1097 | train loss: 571.1499023438\n",
            "Epoch: 3 | iter: 1098 | train loss: 1064.2230224609\n",
            "Epoch: 3 | iter: 1099 | train loss: 867.9879150391\n",
            "Epoch: 3 | iter: 1100 | train loss: 1126.2098388672\n",
            "Epoch: 3 | iter: 1101 | train loss: 347.0329284668\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}